[
  {
    "id": "2510.27304v1",
    "title": "Binary Anomaly Detection in Streaming IoT Traffic under Concept Drift",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG",
      "cs.CR",
      "CR"
    ],
    "authors": [
      "Rodrigo Matos Carnier",
      "Laura Lahesoo",
      "Kensuke Fukuda"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27304v1",
    "abstract": "With the growing volume of Internet of Things (IoT) network traffic, machine learning (ML)-based anomaly detection is more relevant than ever. Traditional batch learning models face challenges such as high maintenance and poor adaptability to rapid anomaly changes, known as concept drift. In contrast, streaming learning integrates online and incremental learning, enabling seamless updates and concept drift detection to improve robustness. This study investigates anomaly detection in streaming IoT traffic as binary classification, comparing batch and streaming learning approaches while assessing the limitations of current IoT traffic datasets. We simulated heterogeneous network data streams by carefully mixing existing datasets and streaming the samples one by one. Our results highlight the failure of batch models to handle concept drift, but also reveal persisting limitations of current datasets to expose model limitations due to low traffic heterogeneity. We also investigated the competitiveness of tree-based ML algorithms, well-known in batch anomaly detection, and compared it to non-tree-based ones, confirming the advantages of the former. Adaptive Random Forest achieved F1-score of 0.990 $\\pm$ 0.006 at one-third the computational cost of its batch counterpart. Hoeffding Adaptive Tree reached F1-score of 0.910 $\\pm$ 0.007, reducing computational cost by four times, making it a viable choice for online applications despite a slight trade-off in stability.",
    "fetched_at": "2025-11-05T02:19:05.055273Z"
  },
  {
    "id": "2510.27313v1",
    "title": "Un-Attributability: Computing Novelty From Retrieval & Semantic   Similarity",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI",
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Philipp Davydov",
      "Ameya Prabhu",
      "Matthias Bethge",
      "Elisa Nguyen",
      "Seong Joon Oh"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27313v1",
    "abstract": "Understanding how language-model outputs relate to the pretraining corpus is central to studying model behavior. Most training data attribution (TDA) methods ask which training examples causally influence a given output, often using leave-one-out tests. We invert the question: which outputs cannot be attributed to any pretraining example? We introduce un-attributability as an operational measure of semantic novelty: an output is novel if the pretraining corpus contains no semantically similar context. We approximate this with a simple two-stage retrieval pipeline: index the corpus with lightweight GIST embeddings, retrieve the top-n candidates, then rerank with ColBERTv2. If the nearest corpus item is less attributable than a human-generated text reference, we consider the output of the model as novel. We evaluate on SmolLM and SmolLM2 and report three findings: (1) models draw on pretraining data across much longer spans than previously reported; (2) some domains systematically promote or suppress novelty; and (3) instruction tuning not only alters style but also increases novelty. Reframing novelty assessment around un-attributability enables efficient analysis at pretraining scale. We release ~20 TB of corpus chunks and index artifacts to support replication and large-scale extension of our analysis at https://huggingface.co/datasets/stai-tuebingen/faiss-smollm",
    "fetched_at": "2025-11-05T02:19:05.055227Z"
  },
  {
    "id": "2510.27315v1",
    "title": "CASR-Net: An Image Processing-focused Deep Learning-based Coronary   Artery Segmentation and Refinement Network for X-ray Coronary Angiogram",
    "date": "2025-10-31",
    "tags": [
      "cs.CV",
      "CV",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Alvee Hassan",
      "Rusab Sarmun",
      "Muhammad E. H. Chowdhury",
      "M. Murugappan",
      "Md. Sakib Abrar Hossain",
      "Sakib Mahmud",
      "Abdulrahman Alqahtani",
      "Sohaib Bassam Zoghoul",
      "Amith Khandakar",
      "Susu M. Zughaier",
      "Somaya Al-Maadeed",
      "Anwarul Hasan"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27315v1",
    "abstract": "Early detection of coronary artery disease (CAD) is critical for reducing mortality and improving patient treatment planning. While angiographic image analysis from X-rays is a common and cost-effective method for identifying cardiac abnormalities, including stenotic coronary arteries, poor image quality can significantly impede clinical diagnosis. We present the Coronary Artery Segmentation and Refinement Network (CASR-Net), a three-stage pipeline comprising image preprocessing, segmentation, and refinement. A novel multichannel preprocessing strategy combining CLAHE and an improved Ben Graham method provides incremental gains, increasing Dice Score Coefficient (DSC) by 0.31-0.89% and Intersection over Union (IoU) by 0.40-1.16% compared with using the techniques individually. The core innovation is a segmentation network built on a UNet with a DenseNet121 encoder and a Self-organized Operational Neural Network (Self-ONN) based decoder, which preserves the continuity of narrow and stenotic vessel branches. A final contour refinement module further suppresses false positives. Evaluated with 5-fold cross-validation on a combination of two public datasets that contain both healthy and stenotic arteries, CASR-Net outperformed several state-of-the-art models, achieving an IoU of 61.43%, a DSC of 76.10%, and clDice of 79.36%. These results highlight a robust approach to automated coronary artery segmentation, offering a valuable tool to support clinicians in diagnosis and treatment planning.",
    "fetched_at": "2025-11-05T02:19:05.055173Z"
  },
  {
    "id": "2510.27321v1",
    "title": "MedM2T: A MultiModal Framework for Time-Aware Modeling with Electronic   Health Record and Electrocardiogram Data",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Yu-Chen Kuo",
      "Yi-Ju Tseng"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27321v1",
    "abstract": "The inherent multimodality and heterogeneous temporal structures of medical data pose significant challenges for modeling. We propose MedM2T, a time-aware multimodal framework designed to address these complexities. MedM2T integrates: (i) Sparse Time Series Encoder to flexibly handle irregular and sparse time series, (ii) Hierarchical Time-Aware Fusion to capture both micro- and macro-temporal patterns from multiple dense time series, such as ECGs, and (iii) Bi-Modal Attention to extract cross-modal interactions, which can be extended to any number of modalities. To mitigate granularity gaps between modalities, MedM2T uses modality-specific pre-trained encoders and aligns resulting features within a shared encoder. We evaluated MedM2T on MIMIC-IV and MIMIC-IV-ECG datasets for three tasks that encompass chronic and acute disease dynamics: 90-day cardiovascular disease (CVD) prediction, in-hospital mortality prediction, and ICU length-of-stay (LOS) regression. MedM2T outperformed state-of-the-art multimodal learning frameworks and existing time series models, achieving an AUROC of 0.947 and an AUPRC of 0.706 for CVD prediction; an AUROC of 0.901 and an AUPRC of 0.558 for mortality prediction; and Mean Absolute Error (MAE) of 2.31 for LOS regression. These results highlight the robustness and broad applicability of MedM2T, positioning it as a promising tool in clinical prediction. We provide the implementation of MedM2T at https://github.com/DHLab-TSENG/MedM2T.",
    "fetched_at": "2025-11-05T02:19:05.055093Z"
  },
  {
    "id": "2510.27324v1",
    "title": "Generative Semantic Coding for Ultra-Low Bitrate Visual Communication   and Analysis",
    "date": "2025-10-31",
    "tags": [
      "cs.CV",
      "CV",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Weiming Chen",
      "Yijia Wang",
      "Zhihan Zhu",
      "Zhihai He"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27324v1",
    "abstract": "We consider the problem of ultra-low bit rate visual communication for remote vision analysis, human interactions and control in challenging scenarios with very low communication bandwidth, such as deep space exploration, battlefield intelligence, and robot navigation in complex environments. In this paper, we ask the following important question: can we accurately reconstruct the visual scene using only a very small portion of the bit rate in existing coding methods while not sacrificing the accuracy of vision analysis and performance of human interactions? Existing text-to-image generation models offer a new approach for ultra-low bitrate image description. However, they can only achieve a semantic-level approximation of the visual scene, which is far insufficient for the purpose of visual communication and remote vision analysis and human interactions. To address this important issue, we propose to seamlessly integrate image generation with deep image compression, using joint text and coding latent to guide the rectified flow models for precise generation of the visual scene. The semantic text description and coding latent are both encoded and transmitted to the decoder at a very small bit rate. Experimental results demonstrate that our method can achieve the same image reconstruction quality and vision analysis accuracy as existing methods while using much less bandwidth. The code will be released upon paper acceptance.",
    "fetched_at": "2025-11-05T02:19:05.055046Z"
  },
  {
    "id": "2510.27328v2",
    "title": "A Unified Representation Underlying the Judgment of Large Language   Models",
    "date": "2025-10-31",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Yi-Long Lu",
      "Jiajun Song",
      "Wei Wang"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27328v2",
    "abstract": "A central architectural question for both biological and artificial intelligence is whether judgment relies on specialized modules or a unified, domain-general resource. While the discovery of decodable neural representations for distinct concepts in Large Language Models (LLMs) has suggested a modular architecture, whether these representations are truly independent systems remains an open question. Here we provide evidence for a convergent architecture for evaluative judgment. Across a range of LLMs, we find that diverse evaluative judgments are computed along a dominant dimension, which we term the Valence-Assent Axis (VAA). This axis jointly encodes subjective valence (\"what is good\") and the model's assent to factual claims (\"what is true\"). Through direct interventions, we demonstrate this axis drives a critical mechanism, which is identified as the subordination of reasoning: the VAA functions as a control signal that steers the generative process to construct a rationale consistent with its evaluative state, even at the cost of factual accuracy. Our discovery offers a mechanistic account for response bias and hallucination, revealing how an architecture that promotes coherent judgment can systematically undermine faithful reasoning.",
    "fetched_at": "2025-11-05T02:19:05.054958Z"
  },
  {
    "id": "2510.27337v1",
    "title": "TransAlign: Machine Translation Encoders are Strong Word Aligners, Too",
    "date": "2025-10-31",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Benedikt Ebing",
      "Christian Goldschmied",
      "Goran Glavaš"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27337v1",
    "abstract": "In the absence of sizable training data for most world languages and NLP tasks, translation-based strategies such as translate-test -- evaluating on noisy source language data translated from the target language -- and translate-train -- training on noisy target language data translated from the source language -- have been established as competitive approaches for cross-lingual transfer (XLT). For token classification tasks, these strategies require label projection: mapping the labels from each token in the original sentence to its counterpart(s) in the translation. To this end, it is common to leverage multilingual word aligners (WAs) derived from encoder language models such as mBERT or LaBSE. Despite obvious associations between machine translation (MT) and WA, research on extracting alignments with MT models is largely limited to exploiting cross-attention in encoder-decoder architectures, yielding poor WA results. In this work, in contrast, we propose TransAlign, a novel word aligner that utilizes the encoder of a massively multilingual MT model. We show that TransAlign not only achieves strong WA performance but substantially outperforms popular WA and state-of-the-art non-WA-based label projection methods in MT-based XLT for token classification.",
    "fetched_at": "2025-11-05T02:19:05.054818Z"
  },
  {
    "id": "2510.27338v1",
    "title": "Reasoning Models Sometimes Output Illegible Chains of Thought",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Arun Jose"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27338v1",
    "abstract": "Language models trained via outcome-based reinforcement learning (RL) to reason using chain-of-thought (CoT) have shown remarkable performance. Monitoring such a model's CoT may allow us to understand its intentions and detect potential malicious behavior. However, to be effective, this requires that CoTs are legible and faithful. We study CoT legibility across 14 reasoning models, finding that RL often causes reasoning to become illegible to both humans and AI monitors, with reasoning models (except Claude) generating illegible CoTs while returning to perfectly readable final answers. We show that models use illegible reasoning to reach correct answers (accuracy dropping by 53\\% when forced to use only legible portions), yet find no correlation between legibility and performance when resampling - suggesting the relationship is more nuanced. We also find that legibility degrades on harder questions. We discuss potential hypotheses for these results, including steganography, training artifacts, and vestigial tokens. These results suggest that without explicit optimization for legibility, outcome-based RL naturally produces models with increasingly opaque reasoning processes, potentially undermining monitoring approaches.",
    "fetched_at": "2025-11-05T02:19:05.054775Z"
  },
  {
    "id": "2510.27342v1",
    "title": "Pairwise and Attribute-Aware Decision Tree-Based Preference Elicitation   for Cold-Start Recommendation",
    "date": "2025-10-31",
    "tags": [
      "cs.IR",
      "IR",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Alireza Gharahighehi",
      "Felipe Kenji Nakano",
      "Xuehua Yang",
      "Wenhan Cu",
      "Celine Vens"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27342v1",
    "abstract": "Recommender systems (RSs) are intelligent filtering methods that suggest items to users based on their inferred preferences, derived from their interaction history on the platform. Collaborative filtering-based RSs rely on users past interactions to generate recommendations. However, when a user is new to the platform, referred to as a cold-start user, there is no historical data available, making it difficult to provide personalized recommendations. To address this, rating elicitation techniques can be used to gather initial ratings or preferences on selected items, helping to build an early understanding of the user's tastes. Rating elicitation approaches are generally categorized into two types: non-personalized and personalized. Decision tree-based rating elicitation is a personalized method that queries users about their preferences at each node of the tree until sufficient information is gathered. In this paper, we propose an extension to the decision tree approach for rating elicitation in the context of music recommendation. Our method: (i) elicits not only item ratings but also preferences on attributes such as genres to better cluster users, and (ii) uses item pairs instead of single items at each node to more effectively learn user preferences. Experimental results demonstrate that both proposed enhancements lead to improved performance, particularly with a reduced number of queries.",
    "fetched_at": "2025-11-05T02:19:05.054739Z"
  },
  {
    "id": "2510.27343v1",
    "title": "Discriminative Rule Learning for Outcome-Guided Process Model Discovery",
    "date": "2025-10-31",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Ali Norouzifar",
      "Wil van der Aalst"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27343v1",
    "abstract": "Event logs extracted from information systems offer a rich foundation for understanding and improving business processes. In many real-world applications, it is possible to distinguish between desirable and undesirable process executions, where desirable traces reflect efficient or compliant behavior, and undesirable ones may involve inefficiencies, rule violations, delays, or resource waste. This distinction presents an opportunity to guide process discovery in a more outcome-aware manner. Discovering a single process model without considering outcomes can yield representations poorly suited for conformance checking and performance analysis, as they fail to capture critical behavioral differences. Moreover, prioritizing one behavior over the other may obscure structural distinctions vital for understanding process outcomes. By learning interpretable discriminative rules over control-flow features, we group traces with similar desirability profiles and apply process discovery separately within each group. This results in focused and interpretable models that reveal the drivers of both desirable and undesirable executions. The approach is implemented as a publicly available tool and it is evaluated on multiple real-life event logs, demonstrating its effectiveness in isolating and visualizing critical process patterns.",
    "fetched_at": "2025-11-05T02:19:05.054687Z"
  },
  {
    "id": "2510.27353v1",
    "title": "An In-depth Study of LLM Contributions to the Bin Packing Problem",
    "date": "2025-10-31",
    "tags": [
      "cs.AI",
      "AI",
      "I.2.8; F.2.2",
      "2"
    ],
    "authors": [
      "Julien Herrmann",
      "Guillaume Pallez"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27353v1",
    "abstract": "Recent studies have suggested that Large Language Models (LLMs) could provide interesting ideas contributing to mathematical discovery. This claim was motivated by reports that LLM-based genetic algorithms produced heuristics offering new insights into the online bin packing problem under uniform and Weibull distributions. In this work, we reassess this claim through a detailed analysis of the heuristics produced by LLMs, examining both their behavior and interpretability. Despite being human-readable, these heuristics remain largely opaque even to domain experts. Building on this analysis, we propose a new class of algorithms tailored to these specific bin packing instances. The derived algorithms are significantly simpler, more efficient, more interpretable, and more generalizable, suggesting that the considered instances are themselves relatively simple. We then discuss the limitations of the claim regarding LLMs' contribution to this problem, which appears to rest on the mistaken assumption that the instances had previously been studied. Our findings instead emphasize the need for rigorous validation and contextualization when assessing the scientific value of LLM-generated outputs.",
    "fetched_at": "2025-11-05T02:19:05.054646Z"
  },
  {
    "id": "2510.27355v1",
    "title": "ThoughtProbe: Classifier-Guided LLM Thought Space Exploration via   Probing Representations",
    "date": "2025-10-31",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Zijian Wang",
      "Chang Xu"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27355v1",
    "abstract": "This paper introduces ThoughtProbe, a novel inference time framework that leverages the hidden reasoning features of Large Language Models (LLMs) to improve their reasoning performance. Unlike previous works that manipulate the hidden representations to steer LLM generation, we harness them as discriminative signals to guide the tree structured response space exploration. In each node expansion, a classifier serves as a scoring and ranking mechanism that efficiently allocates computational resources by prioritizing higher score candidates for continuation. After completing the tree expansion, we collect answers from all branches to form a candidate answer pool. We then propose a branch aggregation method that marginalizes over all supporting branches by aggregating their CoT scores, thereby identifying the optimal answer from the pool. Experimental results show that our framework's comprehensive exploration not only covers valid reasoning chains but also effectively identifies them, achieving significant improvements across multiple arithmetic reasoning benchmarks.",
    "fetched_at": "2025-11-05T02:19:05.054607Z"
  },
  {
    "id": "2510.27359v1",
    "title": "FPS: Feedforward-based Parameter Selection For Efficient Fine-Tuning",
    "date": "2025-10-31",
    "tags": [
      "cs.CV",
      "CV",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Kenneth Yang",
      "Wen-Li Wei",
      "Jen-Chun Lin"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27359v1",
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) has emerged as a key strategy for adapting large-scale pre-trained models to downstream tasks, but existing approaches face notable limitations. Addition-based methods, such as Adapters [1], introduce inference latency and engineering complexity, while selection-based methods like Gradient-based Parameter Selection (GPS) [2] require a full backward pass, which results in the same peak memory usage as full fine-tuning. To address this dilemma, we propose Feedforward-based Parameter Selection (FPS), a gradient-free method that identifies an optimal parameter subset in a single forward pass. FPS ranks parameters by the product of their magnitudes and corresponding input activations, leveraging both pre-trained knowledge and downstream data. Evaluated on $24$ visual tasks from FGVC and VTAB-1k, FPS achieves performance comparable to state-of-the-art methods while reducing peak memory usage by nearly $9 \\times$ and accelerating parameter selection by about $2 \\times$, offering a genuinely memory-efficient and practical solution for fine-tuning large-scale pre-trained models.",
    "fetched_at": "2025-11-05T02:19:05.054567Z"
  },
  {
    "id": "2510.27364v1",
    "title": "Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A   Small-Data Pipeline with LoRA and Wan2.1 I2V",
    "date": "2025-10-31",
    "tags": [
      "cs.CV",
      "CV",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Meftun Akarsu",
      "Kerem Catay",
      "Sedat Bin Vedat",
      "Enes Kutay Yarkan",
      "Ilke Senturk",
      "Arda Sar",
      "Dafne Eksioglu"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27364v1",
    "abstract": "We present a practical pipeline for fine-tuning open-source video diffusion transformers to synthesize cinematic scenes for television and film production from small datasets. The proposed two-stage process decouples visual style learning from motion generation. In the first stage, Low-Rank Adaptation (LoRA) modules are integrated into the cross-attention layers of the Wan2.1 I2V-14B model to adapt its visual representations using a compact dataset of short clips from Ay Yapim's historical television film El Turco. This enables efficient domain transfer within hours on a single GPU. In the second stage, the fine-tuned model produces stylistically consistent keyframes that preserve costume, lighting, and color grading, which are then temporally expanded into coherent 720p sequences through the model's video decoder. We further apply lightweight parallelization and sequence partitioning strategies to accelerate inference without quality degradation. Quantitative and qualitative evaluations using FVD, CLIP-SIM, and LPIPS metrics, supported by a small expert user study, demonstrate measurable improvements in cinematic fidelity and temporal stability over the base model. The complete training and inference pipeline is released to support reproducibility and adaptation across cinematic domains.",
    "fetched_at": "2025-11-05T02:19:05.054479Z"
  },
  {
    "id": "2510.27369v1",
    "title": "From the Rock Floor to the Cloud: A Systematic Survey of   State-of-the-Art NLP in Battery Life Cycle",
    "date": "2025-10-31",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Tosin Adewumi",
      "Martin Karlsson",
      "Marcus Liwicki",
      "Mikael Sjödahl",
      "Lama Alkhaled",
      "Rihab Gargouri",
      "Nudrat Habib",
      "Franz Hennie"
    ],
    "institution": "Google, Meta",
    "link": "http://arxiv.org/pdf/2510.27369v1",
    "abstract": "We present a comprehensive systematic survey of the application of natural language processing (NLP) along the entire battery life cycle, instead of one stage or method, and introduce a novel technical language processing (TLP) framework for the EU's proposed digital battery passport (DBP) and other general battery predictions. We follow the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) method and employ three reputable databases or search engines, including Google Scholar, Institute of Electrical and Electronics Engineers Xplore (IEEE Xplore), and Scopus. Consequently, we assessed 274 scientific papers before the critical review of the final 66 relevant papers. We publicly provide artifacts of the review for validation and reproducibility. The findings show that new NLP tasks are emerging in the battery domain, which facilitate materials discovery and other stages of the life cycle. Notwithstanding, challenges remain, such as the lack of standard benchmarks. Our proposed TLP framework, which incorporates agentic AI and optimized prompts, will be apt for tackling some of the challenges.",
    "fetched_at": "2025-11-05T02:19:05.054421Z"
  },
  {
    "id": "2510.27378v1",
    "title": "Measuring Chain-of-Thought Monitorability Through Faithfulness and   Verbosity",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI",
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Austin Meek",
      "Eitan Sprejer",
      "Iván Arcuschin",
      "Austin J. Brockmeier",
      "Steven Basart"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27378v1",
    "abstract": "Chain-of-thought (CoT) outputs let us read a model's step-by-step reasoning. Since any long, serial reasoning process must pass through this textual trace, the quality of the CoT is a direct window into what the model is thinking. This visibility could help us spot unsafe or misaligned behavior (monitorability), but only if the CoT is transparent about its internal reasoning (faithfulness). Fully measuring faithfulness is difficult, so researchers often focus on examining the CoT in cases where the model changes its answer after adding a cue to the input. This proxy finds some instances of unfaithfulness but loses information when the model maintains its answer, and does not investigate aspects of reasoning not tied to the cue. We extend these results to a more holistic sense of monitorability by introducing verbosity: whether the CoT lists every factor needed to solve the task. We combine faithfulness and verbosity into a single monitorability score that shows how well the CoT serves as the model's external `working memory', a property that many safety schemes based on CoT monitoring depend on. We evaluate instruction-tuned and reasoning models on BBH, GPQA, and MMLU. Our results show that models can appear faithful yet remain hard to monitor when they leave out key factors, and that monitorability differs sharply across model families. We release our evaluation code using the Inspect library to support reproducible future work.",
    "fetched_at": "2025-11-05T02:19:05.054361Z"
  },
  {
    "id": "2510.27379v1",
    "title": "Spiking Neural Networks: The Future of Brain-Inspired Computing",
    "date": "2025-10-31",
    "tags": [
      "cs.NE",
      "NE",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Sales G. Aribe Jr"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27379v1",
    "abstract": "Spiking Neural Networks (SNNs) represent the latest generation of neural computation, offering a brain-inspired alternative to conventional Artificial Neural Networks (ANNs). Unlike ANNs, which depend on continuous-valued signals, SNNs operate using distinct spike events, making them inherently more energy-efficient and temporally dynamic. This study presents a comprehensive analysis of SNN design models, training algorithms, and multi-dimensional performance metrics, including accuracy, energy consumption, latency, spike count, and convergence behavior. Key neuron models such as the Leaky Integrate-and-Fire (LIF) and training strategies, including surrogate gradient descent, ANN-to-SNN conversion, and Spike-Timing Dependent Plasticity (STDP), are examined in depth. Results show that surrogate gradient-trained SNNs closely approximate ANN accuracy (within 1-2%), with faster convergence by the 20th epoch and latency as low as 10 milliseconds. Converted SNNs also achieve competitive performance but require higher spike counts and longer simulation windows. STDP-based SNNs, though slower to converge, exhibit the lowest spike counts and energy consumption (as low as 5 millijoules per inference), making them optimal for unsupervised and low-power tasks. These findings reinforce the suitability of SNNs for energy-constrained, latency-sensitive, and adaptive applications such as robotics, neuromorphic vision, and edge AI systems. While promising, challenges persist in hardware standardization and scalable training. This study concludes that SNNs, with further refinement, are poised to propel the next phase of neuromorphic computing.",
    "fetched_at": "2025-11-05T02:19:05.054307Z"
  },
  {
    "id": "2510.27385v1",
    "title": "On the Equivalence of Optimal Transport Problem and Action Matching with   Optimal Vector Fields",
    "date": "2025-10-31",
    "tags": [
      "stat.ML",
      "ML",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Nikita Kornilov",
      "Alexander Korotin"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27385v1",
    "abstract": "Flow Matching (FM) method in generative modeling maps arbitrary probability distributions by constructing an interpolation between them and then learning the vector field that defines ODE for this interpolation. Recently, it was shown that FM can be modified to map distributions optimally in terms of the quadratic cost function for any initial interpolation. To achieve this, only specific optimal vector fields, which are typical for solutions of Optimal Transport (OT) problems, need to be considered during FM loss minimization. In this note, we show that considering only optimal vector fields can lead to OT in another approach: Action Matching (AM). Unlike FM, which learns a vector field for a manually chosen interpolation between given distributions, AM learns the vector field that defines ODE for an entire given sequence of distributions.",
    "fetched_at": "2025-11-05T02:19:05.054215Z"
  },
  {
    "id": "2510.27391v1",
    "title": "Modality Alignment across Trees on Heterogeneous Hyperbolic Manifolds",
    "date": "2025-10-31",
    "tags": [
      "cs.CV",
      "CV",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Wu Wei",
      "Xiaomeng Fan",
      "Yuwei Wu",
      "Zhi Gao",
      "Pengxiang Li",
      "Yunde Jia",
      "Mehrtash Harandi"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27391v1",
    "abstract": "Modality alignment is critical for vision-language models (VLMs) to effectively integrate information across modalities. However, existing methods extract hierarchical features from text while representing each image with a single feature, leading to asymmetric and suboptimal alignment. To address this, we propose Alignment across Trees, a method that constructs and aligns tree-like hierarchical features for both image and text modalities. Specifically, we introduce a semantic-aware visual feature extraction framework that applies a cross-attention mechanism to visual class tokens from intermediate Transformer layers, guided by textual cues to extract visual features with coarse-to-fine semantics. We then embed the feature trees of the two modalities into hyperbolic manifolds with distinct curvatures to effectively model their hierarchical structures. To align across the heterogeneous hyperbolic manifolds with different curvatures, we formulate a KL distance measure between distributions on heterogeneous manifolds, and learn an intermediate manifold for manifold alignment by minimizing the distance. We prove the existence and uniqueness of the optimal intermediate manifold. Experiments on taxonomic open-set classification tasks across multiple image datasets demonstrate that our method consistently outperforms strong baselines under few-shot and cross-domain settings.",
    "fetched_at": "2025-11-05T02:19:05.054177Z"
  },
  {
    "id": "2510.27397v1",
    "title": "Interpretable Model-Aware Counterfactual Explanations for Random Forest",
    "date": "2025-10-31",
    "tags": [
      "stat.ML",
      "ML",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Joshua S. Harvey",
      "Guanchao Feng",
      "Sai Anusha Meesala",
      "Tina Zhao",
      "Dhagash Mehta"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27397v1",
    "abstract": "Despite their enormous predictive power, machine learning models are often unsuitable for applications in regulated industries such as finance, due to their limited capacity to provide explanations. While model-agnostic frameworks such as Shapley values have proved to be convenient and popular, they rarely align with the kinds of causal explanations that are typically sought after. Counterfactual case-based explanations, where an individual is informed of which circumstances would need to be different to cause a change in outcome, may be more intuitive and actionable. However, finding appropriate counterfactual cases is an open challenge, as is interpreting which features are most critical for the change in outcome. Here, we pose the question of counterfactual search and interpretation in terms of similarity learning, exploiting the representation learned by the random forest predictive model itself. Once a counterfactual is found, the feature importance of the explanation is computed as a function of which random forest partitions are crossed in order to reach it from the original instance. We demonstrate this method on both the MNIST hand-drawn digit dataset and the German credit dataset, finding that it generates explanations that are sparser and more useful than Shapley values.",
    "fetched_at": "2025-11-05T02:19:05.054117Z"
  },
  {
    "id": "2510.27400v1",
    "title": "Balancing Knowledge Updates: Toward Unified Modular Editing in LLMs",
    "date": "2025-10-31",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Jiahao Liu",
      "Zijian Wang",
      "Kuo Zhao",
      "Dong Hu"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27400v1",
    "abstract": "Knowledge editing has emerged as an efficient approach for updating factual knowledge in large language models (LLMs). It typically locates knowledge storage modules and then modifies their parameters. However, most existing methods focus on the weights of multilayer perceptron (MLP) modules, which are often identified as the main repositories of factual information. Other components, such as attention (Attn) modules, are often ignored during editing. This imbalance can leave residual outdated knowledge and limit editing effectiveness. We perform comprehensive knowledge localization experiments on advanced LLMs and find that Attn modules play a substantial role in factual knowledge storage and retrieval, especially in earlier layers. Based on these insights, we propose IntAttn-Edit, a method that extends the associative memory paradigm to jointly update both MLP and Attn modules. Our approach uses a knowledge balancing strategy that allocates update magnitudes in proportion to each module's measured contribution to knowledge storage. Experiments on standard benchmarks show that IntAttn-Edit achieves higher edit success, better generalization, and stronger knowledge preservation than prior methods. Further analysis shows that the balancing strategy keeps editing performance within an optimal range across diverse settings.",
    "fetched_at": "2025-11-05T02:19:05.054066Z"
  },
  {
    "id": "2510.27403v1",
    "title": "FedMuon: Accelerating Federated Learning with Matrix Orthogonalization",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Junkang Liu",
      "Fanhua Shang",
      "Junchao Zhou",
      "Hongying Liu",
      "Yuanyuan Liu",
      "Jin Liu"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27403v1",
    "abstract": "The core bottleneck of Federated Learning (FL) lies in the communication rounds. That is, how to achieve more effective local updates is crucial for reducing communication rounds. Existing FL methods still primarily use element-wise local optimizers (Adam/SGD), neglecting the geometric structure of the weight matrices. This often leads to the amplification of pathological directions in the weights during local updates, leading deterioration in the condition number and slow convergence. Therefore, we introduce the Muon optimizer in local, which has matrix orthogonalization to optimize matrix-structured parameters. Experimental results show that, in IID setting, Local Muon significantly accelerates the convergence of FL and reduces communication rounds compared to Local SGD and Local AdamW. However, in non-IID setting, independent matrix orthogonalization based on the local distributions of each client induces strong client drift. Applying Muon in non-IID FL poses significant challenges: (1) client preconditioner leading to client drift; (2) moment reinitialization. To address these challenges, we propose a novel Federated Muon optimizer (FedMuon), which incorporates two key techniques: (1) momentum aggregation, where clients use the aggregated momentum for local initialization; (2) local-global alignment, where the local gradients are aligned with the global update direction to significantly reduce client drift. Theoretically, we prove that \\texttt{FedMuon} achieves a linear speedup convergence rate without the heterogeneity assumption, where $S$ is the number of participating clients per round, $K$ is the number of local iterations, and $R$ is the total number of communication rounds. Empirically, we validate the effectiveness of FedMuon on language and vision models. Compared to several baselines, FedMuon significantly reduces communication rounds and improves test accuracy.",
    "fetched_at": "2025-11-05T02:19:05.054017Z"
  },
  {
    "id": "2510.27407v1",
    "title": "Awal -- Community-Powered Language Technology for Tamazight",
    "date": "2025-10-31",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Alp Öktem",
      "Farida Boudichat"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27407v1",
    "abstract": "This paper presents Awal, a community-powered initiative for developing language technology resources for Tamazight. We provide a comprehensive review of the NLP landscape for Tamazight, examining recent progress in computational resources, and the emergence of community-driven approaches to address persistent data scarcity. Launched in 2024, awaldigital.org platform addresses the underrepresentation of Tamazight in digital spaces through a collaborative platform enabling speakers to contribute translation and voice data. We analyze 18 months of community engagement, revealing significant barriers to participation including limited confidence in written Tamazight and ongoing standardization challenges. Despite widespread positive reception, actual data contribution remained concentrated among linguists and activists. The modest scale of community contributions -- 6,421 translation pairs and 3 hours of speech data -- highlights the limitations of applying standard crowdsourcing approaches to languages with complex sociolinguistic contexts. We are working on improved open-source MT models using the collected data.",
    "fetched_at": "2025-11-05T02:19:05.053943Z"
  },
  {
    "id": "2510.27408v3",
    "title": "Estimation of aboveground biomass in a tropical dry forest: An   intercomparison of airborne, unmanned, and space laser scanning",
    "date": "2025-10-31",
    "tags": [
      "eess.SP",
      "SP",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Nelson Mattié",
      "Arturo Sanchez-Azofeifa",
      "Pablo Crespo-Peremarch",
      "Juan-Ygnacio López-Hernández"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27408v3",
    "abstract": "According to the Paris Climate Change Agreement, all nations are required to submit reports on their greenhouse gas emissions and absorption every two years by 2024. Consequently, forests play a crucial role in reducing carbon emissions, which is essential for meeting these obligations. Recognizing the significance of forest conservation in the global battle against climate change, Article 5 of the Paris Agreement emphasizes the need for high-quality forest data. This study focuses on enhancing methods for mapping aboveground biomass in tropical dry forests. Tropical dry forests are considered one of the least understood tropical forest environments; therefore, there is a need for accurate approaches to estimate carbon pools. We employ a comparative analysis of AGB estimates, utilizing different discrete and full-waveform laser scanning datasets in conjunction with Ordinary Least Squares and Bayesian approaches SVM. Airborne Laser Scanning, Unmanned Laser Scanning, and Space Laser Scanning were used as independent variables for extracting forest metrics. Variable selection, SVM regression tuning, and cross-validation via a machine-learning approach were applied to account for overfitting and underfitting. The results indicate that six key variables primarily related to tree height: Elev\\.minimum, Elev\\.L3, lev\\.MAD\\.mode, Elev\\.mode, Elev\\.MAD\\.median, and Elev\\.skewness, are important for AGB estimation using ALSD and ULSD, while Leaf Area Index, canopy coverage and height, terrain elevation, and full-waveform signal energy emerged as the most vital variables. AGB values estimated from ten permanent tropical dry forest plots in Costa Rica Guanacaste province ranged from 26.02 Mg/ha to 175.43 Mg/ha. The SVM regressions demonstrated a 17.89 error across all laser scanning systems, with SLSF W exhibiting the lowest error 17.07 in estimating total biomass per plot.",
    "fetched_at": "2025-11-05T02:19:05.053904Z"
  },
  {
    "id": "2510.27413v1",
    "title": "Atlas-Alignment: Making Interpretability Transferable Across Language   Models",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI",
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Bruno Puri",
      "Jim Berend",
      "Sebastian Lapuschkin",
      "Wojciech Samek"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27413v1",
    "abstract": "Interpretability is crucial for building safe, reliable, and controllable language models, yet existing interpretability pipelines remain costly and difficult to scale. Interpreting a new model typically requires costly training of model-specific sparse autoencoders, manual or semi-automated labeling of SAE components, and their subsequent validation. We introduce Atlas-Alignment, a framework for transferring interpretability across language models by aligning unknown latent spaces to a Concept Atlas - a labeled, human-interpretable latent space - using only shared inputs and lightweight representational alignment techniques. Once aligned, this enables two key capabilities in previously opaque models: (1) semantic feature search and retrieval, and (2) steering generation along human-interpretable atlas concepts. Through quantitative and qualitative evaluations, we show that simple representational alignment methods enable robust semantic retrieval and steerable generation without the need for labeled concept data. Atlas-Alignment thus amortizes the cost of explainable AI and mechanistic interpretability: by investing in one high-quality Concept Atlas, we can make many new models transparent and controllable at minimal marginal cost.",
    "fetched_at": "2025-11-05T02:19:05.053781Z"
  },
  {
    "id": "2510.27419v1",
    "title": "DeepCompress: A Dual Reward Strategy for Dynamically Exploring and   Compressing Reasoning Chains",
    "date": "2025-10-31",
    "tags": [
      "cs.AI",
      "AI",
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Tian Liang",
      "Wenxiang Jiao",
      "Zhiwei He",
      "Jiahao Xu",
      "Haitao Mi",
      "Dong Yu"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27419v1",
    "abstract": "Large Reasoning Models (LRMs) have demonstrated impressive capabilities but suffer from cognitive inefficiencies like ``overthinking'' simple problems and ``underthinking'' complex ones. While existing methods that use supervised fine-tuning~(SFT) or reinforcement learning~(RL) with token-length rewards can improve efficiency, they often do so at the cost of accuracy. This paper introduces \\textbf{DeepCompress}, a novel framework that simultaneously enhances both the accuracy and efficiency of LRMs. We challenge the prevailing approach of consistently favoring shorter reasoning paths, showing that longer responses can contain a broader range of correct solutions for difficult problems. DeepCompress employs an adaptive length reward mechanism that dynamically classifies problems as ``Simple'' or ``Hard'' in real-time based on the model's evolving capability. It encourages shorter, more efficient reasoning for ``Simple'' problems while promoting longer, more exploratory thought chains for ``Hard'' problems. This dual-reward strategy enables the model to autonomously adjust its Chain-of-Thought (CoT) length, compressing reasoning for well-mastered problems and extending it for those it finds challenging. Experimental results on challenging mathematical benchmarks show that DeepCompress consistently outperforms baseline methods, achieving superior accuracy while significantly improving token efficiency.",
    "fetched_at": "2025-11-05T02:19:05.053692Z"
  },
  {
    "id": "2510.27421v1",
    "title": "Who Does Your Algorithm Fail? Investigating Age and Ethnic Bias in the   MAMA-MIA Dataset",
    "date": "2025-10-31",
    "tags": [
      "cs.CV",
      "CV",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Aditya Parikh",
      "Sneha Das",
      "Aasa Feragen"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27421v1",
    "abstract": "Deep learning models aim to improve diagnostic workflows, but fairness evaluation remains underexplored beyond classification, e.g., in image segmentation. Unaddressed segmentation bias can lead to disparities in the quality of care for certain populations, potentially compounded across clinical decision points and amplified through iterative model development. Here, we audit the fairness of the automated segmentation labels provided in the breast cancer tumor segmentation dataset MAMA-MIA. We evaluate automated segmentation quality across age, ethnicity, and data source. Our analysis reveals an intrinsic age-related bias against younger patients that continues to persist even after controlling for confounding factors, such as data source. We hypothesize that this bias may be linked to physiological factors, a known challenge for both radiologists and automated systems. Finally, we show how aggregating data from multiple data sources influences site-specific ethnic biases, underscoring the necessity of investigating data at a granular level.",
    "fetched_at": "2025-11-05T02:19:05.053634Z"
  },
  {
    "id": "2510.27428v1",
    "title": "Learning Soft Robotic Dynamics with Active Exploration",
    "date": "2025-10-31",
    "tags": [
      "cs.RO",
      "RO",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Hehui Zheng",
      "Bhavya Sukhija",
      "Chenhao Li",
      "Klemens Iten",
      "Andreas Krause",
      "Robert K. Katzschmann"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27428v1",
    "abstract": "Soft robots offer unmatched adaptability and safety in unstructured environments, yet their compliant, high-dimensional, and nonlinear dynamics make modeling for control notoriously difficult. Existing data-driven approaches often fail to generalize, constrained by narrowly focused task demonstrations or inefficient random exploration. We introduce SoftAE, an uncertainty-aware active exploration framework that autonomously learns task-agnostic and generalizable dynamics models of soft robotic systems. SoftAE employs probabilistic ensemble models to estimate epistemic uncertainty and actively guides exploration toward underrepresented regions of the state-action space, achieving efficient coverage of diverse behaviors without task-specific supervision. We evaluate SoftAE on three simulated soft robotic platforms -- a continuum arm, an articulated fish in fluid, and a musculoskeletal leg with hybrid actuation -- and on a pneumatically actuated continuum soft arm in the real world. Compared with random exploration and task-specific model-based reinforcement learning, SoftAE produces more accurate dynamics models, enables superior zero-shot control on unseen tasks, and maintains robustness under sensing noise, actuation delays, and nonlinear material effects. These results demonstrate that uncertainty-driven active exploration can yield scalable, reusable dynamics models across diverse soft robotic morphologies, representing a step toward more autonomous, adaptable, and data-efficient control in compliant robots.",
    "fetched_at": "2025-11-05T02:19:05.053573Z"
  },
  {
    "id": "2510.27432v1",
    "title": "Mitigating Semantic Collapse in Partially Relevant Video Retrieval",
    "date": "2025-10-31",
    "tags": [
      "cs.CV",
      "CV",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "WonJun Moon",
      "MinSeok Jung",
      "Gilhan Park",
      "Tae-Young Kim",
      "Cheol-Ho Cho",
      "Woojin Jun",
      "Jae-Pil Heo"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27432v1",
    "abstract": "Partially Relevant Video Retrieval (PRVR) seeks videos where only part of the content matches a text query. Existing methods treat every annotated text-video pair as a positive and all others as negatives, ignoring the rich semantic variation both within a single video and across different videos. Consequently, embeddings of both queries and their corresponding video-clip segments for distinct events within the same video collapse together, while embeddings of semantically similar queries and segments from different videos are driven apart. This limits retrieval performance when videos contain multiple, diverse events. This paper addresses the aforementioned problems, termed as semantic collapse, in both the text and video embedding spaces. We first introduce Text Correlation Preservation Learning, which preserves the semantic relationships encoded by the foundation model across text queries. To address collapse in video embeddings, we propose Cross-Branch Video Alignment (CBVA), a contrastive alignment method that disentangles hierarchical video representations across temporal scales. Subsequently, we introduce order-preserving token merging and adaptive CBVA to enhance alignment by producing video segments that are internally coherent yet mutually distinctive. Extensive experiments on PRVR benchmarks demonstrate that our framework effectively prevents semantic collapse and substantially improves retrieval accuracy.",
    "fetched_at": "2025-11-05T02:19:05.053511Z"
  },
  {
    "id": "2510.27442v1",
    "title": "CoMViT: An Efficient Vision Backbone for Supervised Classification in   Medical Imaging",
    "date": "2025-10-31",
    "tags": [
      "cs.CV",
      "CV",
      "cs.AI",
      "AI",
      "I.2.10",
      "10"
    ],
    "authors": [
      "Aon Safdar",
      "Mohamed Saadeldin"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27442v1",
    "abstract": "Vision Transformers (ViTs) have demonstrated strong potential in medical imaging; however, their high computational demands and tendency to overfit on small datasets limit their applicability in real-world clinical scenarios. In this paper, we present CoMViT, a compact and generalizable Vision Transformer architecture optimized for resource-constrained medical image analysis. CoMViT integrates a convolutional tokenizer, diagonal masking, dynamic temperature scaling, and pooling-based sequence aggregation to improve performance and generalization. Through systematic architectural optimization, CoMViT achieves robust performance across twelve MedMNIST datasets while maintaining a lightweight design with only ~4.5M parameters. It matches or outperforms deeper CNN and ViT variants, offering up to 5-20x parameter reduction without sacrificing accuracy. Qualitative Grad-CAM analyses show that CoMViT consistently attends to clinically relevant regions despite its compact size. These results highlight the potential of principled ViT redesign for developing efficient and interpretable models in low-resource medical imaging settings.",
    "fetched_at": "2025-11-05T02:19:05.053453Z"
  },
  {
    "id": "2510.27443v1",
    "title": "MVeLMA: Multimodal Vegetation Loss Modeling Architecture for Predicting   Post-fire Vegetation Loss",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Meenu Ravi",
      "Shailik Sarkar",
      "Yanshen Sun",
      "Vaishnavi Singh",
      "Chang-Tien Lu"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27443v1",
    "abstract": "Understanding post-wildfire vegetation loss is critical for developing effective ecological recovery strategies and is often challenging due to the extended time and effort required to capture the evolving ecosystem features. Recent works in this area have not fully explored all the contributing factors, their modalities, and interactions with each other. Furthermore, most research in this domain is limited by a lack of interpretability in predictive modeling, making it less useful in real-world settings. In this work, we propose a novel end-to-end ML pipeline called MVeLMA (\\textbf{M}ultimodal \\textbf{Ve}getation \\textbf{L}oss \\textbf{M}odeling \\textbf{A}rchitecture) to predict county-wise vegetation loss from fire events. MVeLMA uses a multimodal feature integration pipeline and a stacked ensemble-based architecture to capture different modalities while also incorporating uncertainty estimation through probabilistic modeling. Through comprehensive experiments, we show that our model outperforms several state-of-the-art (SOTA) and baseline models in predicting post-wildfire vegetation loss. Furthermore, we generate vegetation loss confidence maps to identify high-risk counties, thereby helping targeted recovery efforts. The findings of this work have the potential to inform future disaster relief planning, ecological policy development, and wildlife recovery management.",
    "fetched_at": "2025-11-05T02:19:05.053412Z"
  },
  {
    "id": "2510.27448v1",
    "title": "GeoFM: Enhancing Geometric Reasoning of MLLMs via Synthetic Data   Generation through Formal Language",
    "date": "2025-10-31",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Yuhao Zhang",
      "Dingxin Hu",
      "Tinghao Yu",
      "Hao Liu",
      "Yiting Liu"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27448v1",
    "abstract": "Multi-modal Large Language Models (MLLMs) have gained significant attention in both academia and industry for their capabilities in handling multi-modal tasks. However, these models face challenges in mathematical geometric reasoning due to the scarcity of high-quality geometric data. To address this issue, synthetic geometric data has become an essential strategy. Current methods for generating synthetic geometric data involve rephrasing or expanding existing problems and utilizing predefined rules and templates to create geometric images and problems. However, these approaches often produce data that lacks diversity or is prone to noise. Additionally, the geometric images synthesized by existing methods tend to exhibit limited variation and deviate significantly from authentic geometric diagrams. To overcome these limitations, we propose GeoFM, a novel method for synthesizing geometric data. GeoFM uses formal languages to explore combinations of conditions within metric space, generating high-fidelity geometric problems that differ from the originals while ensuring correctness through a symbolic engine. Experimental results show that our synthetic data significantly outperforms existing methods. The model trained with our data surpass the proprietary GPT-4o model by 18.7\\% on geometry problem-solving tasks in MathVista and by 16.5\\% on GeoQA. Additionally, it exceeds the performance of a leading open-source model by 5.7\\% on MathVista and by 2.7\\% on GeoQA.",
    "fetched_at": "2025-11-05T02:19:05.053356Z"
  },
  {
    "id": "2510.27462v1",
    "title": "VCORE: Variance-Controlled Optimization-based Reweighting for   Chain-of-Thought Supervision",
    "date": "2025-10-31",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Xuan Gong",
      "Senmiao Wang",
      "Hanbo Huang",
      "Ruoyu Sun",
      "Shiyu Liang"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27462v1",
    "abstract": "Supervised fine-tuning (SFT) on long chain-of-thought (CoT) trajectories has emerged as a crucial technique for enhancing the reasoning abilities of large language models (LLMs). However, the standard cross-entropy loss treats all tokens equally, ignoring their heterogeneous contributions across a reasoning trajectory. This uniform treatment leads to misallocated supervision and weak generalization, especially in complex, long-form reasoning tasks. To address this, we introduce \\textbf{V}ariance-\\textbf{C}ontrolled \\textbf{O}ptimization-based \\textbf{RE}weighting (VCORE), a principled framework that reformulates CoT supervision as a constrained optimization problem. By adopting an optimization-theoretic perspective, VCORE enables a principled and adaptive allocation of supervision across tokens, thereby aligning the training objective more closely with the goal of robust reasoning generalization. Empirical evaluations demonstrate that VCORE consistently outperforms existing token reweighting methods. Across both in-domain and out-of-domain settings, VCORE achieves substantial performance gains on mathematical and coding benchmarks, using models from the Qwen3 series (4B, 8B, 32B) and LLaMA-3.1-8B-Instruct. Moreover, we show that VCORE serves as a more effective initialization for subsequent reinforcement learning, establishing a stronger foundation for advancing the reasoning capabilities of LLMs. The Code will be released at https://github.com/coder-gx/VCORE.",
    "fetched_at": "2025-11-05T02:19:05.053302Z"
  },
  {
    "id": "2510.27469v1",
    "title": "Diffuse Thinking: Exploring Diffusion Language Models as Efficient   Thought Proposers for Reasoning",
    "date": "2025-10-31",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Chenyang Shao",
      "Sijian Ren",
      "Fengli Xu",
      "Yong Li"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27469v1",
    "abstract": "In recent years, large language models (LLMs) have witnessed remarkable advancements, with the test-time scaling law consistently enhancing the reasoning capabilities. Through systematic evaluation and exploration of a diverse spectrum of intermediate thoughts, LLMs demonstrate the potential to generate deliberate reasoning steps, thereby substantially enhancing reasoning accuracy. However, LLMs' autoregressive generation paradigm results in reasoning performance scaling sub-optimally with test-time computation, often requiring excessive computational overhead to propose thoughts while yielding only marginal performance gains. In contrast, diffusion language models (DLMs) can efficiently produce diverse samples through parallel denoising in a single forward pass, inspiring us to leverage them for proposing intermediate thoughts, thereby alleviating the computational burden associated with autoregressive generation while maintaining quality. In this work, we propose an efficient collaborative reasoning framework, leveraging DLMs to generate candidate thoughts and LLMs to evaluate their quality. Experiments across diverse benchmarks demonstrate that our framework achieves strong performance in complex reasoning tasks, offering a promising direction for future research. Our code is open-source at https://anonymous.4open.science/r/Diffuse-Thinking-EC60.",
    "fetched_at": "2025-11-05T02:19:05.053248Z"
  },
  {
    "id": "2510.27474v1",
    "title": "Spectral Neural Graph Sparsification",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Angelica Liguori",
      "Ettore Ritacco",
      "Pietro Sabatino",
      "Annalisa Socievole"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27474v1",
    "abstract": "Graphs are central to modeling complex systems in domains such as social networks, molecular chemistry, and neuroscience. While Graph Neural Networks, particularly Graph Convolutional Networks, have become standard tools for graph learning, they remain constrained by reliance on fixed structures and susceptibility to over-smoothing. We propose the Spectral Preservation Network, a new framework for graph representation learning that generates reduced graphs serving as faithful proxies of the original, enabling downstream tasks such as community detection, influence propagation, and information diffusion at a reduced computational cost. The Spectral Preservation Network introduces two key components: the Joint Graph Evolution layer and the Spectral Concordance loss. The former jointly transforms both the graph topology and the node feature matrix, allowing the structure and attributes to evolve adaptively across layers and overcoming the rigidity of static neighborhood aggregation. The latter regularizes these transformations by enforcing consistency in both the spectral properties of the graph and the feature vectors of the nodes. We evaluate the effectiveness of Spectral Preservation Network on node-level sparsification by analyzing well-established metrics and benchmarking against state-of-the-art methods. The experimental results demonstrate the superior performance and clear advantages of our approach.",
    "fetched_at": "2025-11-05T02:19:05.053200Z"
  },
  {
    "id": "2510.27477v1",
    "title": "The aftermath of compounds: Investigating Compounds and their Semantic   Representations",
    "date": "2025-10-31",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Swarang Joshi"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27477v1",
    "abstract": "This study investigates how well computational embeddings align with human semantic judgments in the processing of English compound words. We compare static word vectors (GloVe) and contextualized embeddings (BERT) against human ratings of lexeme meaning dominance (LMD) and semantic transparency (ST) drawn from a psycholinguistic dataset. Using measures of association strength (Edinburgh Associative Thesaurus), frequency (BNC), and predictability (LaDEC), we compute embedding-derived LMD and ST metrics and assess their relationships with human judgments via Spearmans correlation and regression analyses. Our results show that BERT embeddings better capture compositional semantics than GloVe, and that predictability ratings are strong predictors of semantic transparency in both human and model data. These findings advance computational psycholinguistics by clarifying the factors that drive compound word processing and offering insights into embedding-based semantic modeling.",
    "fetched_at": "2025-11-05T02:19:05.053152Z"
  },
  {
    "id": "2510.27480v1",
    "title": "Simplex-to-Euclidean Bijections for Categorical Flow Matching",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Bernardo Williams",
      "Victor M. Yeom-Song",
      "Marcelo Hartmann",
      "Arto Klami"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27480v1",
    "abstract": "We propose a method for learning and sampling from probability distributions supported on the simplex. Our approach maps the open simplex to Euclidean space via smooth bijections, leveraging the Aitchison geometry to define the mappings, and supports modeling categorical data by a Dirichlet interpolation that dequantizes discrete observations into continuous ones. This enables density modeling in Euclidean space through the bijection while still allowing exact recovery of the original discrete distribution. Compared to previous methods that operate on the simplex using Riemannian geometry or custom noise processes, our approach works in Euclidean space while respecting the Aitchison geometry, and achieves competitive performance on both synthetic and real-world data sets.",
    "fetched_at": "2025-11-05T02:19:05.053117Z"
  },
  {
    "id": "2510.27486v1",
    "title": "FedAdamW: A Communication-Efficient Optimizer with Convergence and   Generalization Guarantees for Federated Large Models",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Junkang Liu",
      "Fanhua Shang",
      "Kewen Zhu",
      "Hongying Liu",
      "Yuanyuan Liu",
      "Jin Liu"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27486v1",
    "abstract": "AdamW has become one of the most effective optimizers for training large-scale models. We have also observed its effectiveness in the context of federated learning (FL). However, directly applying AdamW in federated learning settings poses significant challenges: (1) due to data heterogeneity, AdamW often yields high variance in the second-moment estimate $\\boldsymbol{v}$; (2) the local overfitting of AdamW may cause client drift; and (3) Reinitializing moment estimates ($\\boldsymbol{v}$, $\\boldsymbol{m}$) at each round slows down convergence. To address these challenges, we propose the first \\underline{Fed}erated \\underline{AdamW} algorithm, called \\texttt{FedAdamW}, for training and fine-tuning various large models. \\texttt{FedAdamW} aligns local updates with the global update using both a \\textbf{local correction mechanism} and decoupled weight decay to mitigate local overfitting. \\texttt{FedAdamW} efficiently aggregates the \\texttt{mean} of the second-moment estimates to reduce their variance and reinitialize them. Theoretically, we prove that \\texttt{FedAdamW} achieves a linear speedup convergence rate of $\\mathcal{O}(\\sqrt{(L \\Delta \\sigma_l^2)/(S K R \\epsilon^2)}+(L \\Delta)/R)$ without \\textbf{heterogeneity assumption}, where $S$ is the number of participating clients per round, $K$ is the number of local iterations, and $R$ is the total number of communication rounds. We also employ PAC-Bayesian generalization analysis to explain the effectiveness of decoupled weight decay in local training. Empirically, we validate the effectiveness of \\texttt{FedAdamW} on language and vision Transformer models. Compared to several baselines, \\texttt{FedAdamW} significantly reduces communication rounds and improves test accuracy. The code is available in https://github.com/junkangLiu0/FedAdamW.",
    "fetched_at": "2025-11-05T02:19:05.053019Z"
  },
  {
    "id": "2510.27497v1",
    "title": "InertialAR: Autoregressive 3D Molecule Generation with Inertial Frames",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Haorui Li",
      "Weitao Du",
      "Yuqiang Li",
      "Hongyu Guo",
      "Shengchao Liu"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27497v1",
    "abstract": "Transformer-based autoregressive models have emerged as a unifying paradigm across modalities such as text and images, but their extension to 3D molecule generation remains underexplored. The gap stems from two fundamental challenges: (1) tokenizing molecules into a canonical 1D sequence of tokens that is invariant to both SE(3) transformations and atom index permutations, and (2) designing an architecture capable of modeling hybrid atom-based tokens that couple discrete atom types with continuous 3D coordinates. To address these challenges, we introduce InertialAR. InertialAR devises a canonical tokenization that aligns molecules to their inertial frames and reorders atoms to ensure SE(3) and permutation invariance. Moreover, InertialAR equips the attention mechanism with geometric awareness via geometric rotary positional encoding (GeoRoPE). In addition, it utilizes a hierarchical autoregressive paradigm to predict the next atom-based token, predicting the atom type first and then its 3D coordinates via Diffusion loss. Experimentally, InertialAR achieves state-of-the-art performance on 7 of the 10 evaluation metrics for unconditional molecule generation across QM9, GEOM-Drugs, and B3LYP. Moreover, it significantly outperforms strong baselines in controllable generation for targeted chemical functionality, attaining state-of-the-art results across all 5 metrics.",
    "fetched_at": "2025-11-05T02:19:05.052946Z"
  },
  {
    "id": "2510.27498v1",
    "title": "Minimax-Optimal Two-Sample Test with Sliced Wasserstein",
    "date": "2025-10-31",
    "tags": [
      "stat.ML",
      "ML",
      "cs.LG",
      "LG",
      "math.ST",
      "ST",
      "stat.ME",
      "ME",
      "stat.TH",
      "TH"
    ],
    "authors": [
      "Binh Thuan Tran",
      "Nicolas Schreuder"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27498v1",
    "abstract": "We study the problem of nonparametric two-sample testing using the sliced Wasserstein (SW) distance. While prior theoretical and empirical work indicates that the SW distance offers a promising balance between strong statistical guarantees and computational efficiency, its theoretical foundations for hypothesis testing remain limited. We address this gap by proposing a permutation-based SW test and analyzing its performance. The test inherits finite-sample Type I error control from the permutation principle. Moreover, we establish non-asymptotic power bounds and show that the procedure achieves the minimax separation rate $n^{-1/2}$ over multinomial and bounded-support alternatives, matching the optimal guarantees of kernel-based tests while building on the geometric foundations of Wasserstein distances. Our analysis further quantifies the trade-off between the number of projections and statistical power. Finally, numerical experiments demonstrate that the test combines finite-sample validity with competitive power and scalability, and -- unlike kernel-based tests, which require careful kernel tuning -- it performs consistently well across all scenarios we consider.",
    "fetched_at": "2025-11-05T02:19:05.052893Z"
  },
  {
    "id": "2510.27503v1",
    "title": "pDANSE: Particle-based Data-driven Nonlinear State Estimation from   Nonlinear Measurements",
    "date": "2025-10-31",
    "tags": [
      "eess.SP",
      "SP",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Anubhab Ghosh",
      "Yonina C. Eldar",
      "Saikat Chatterjee"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27503v1",
    "abstract": "We consider the problem of designing a data-driven nonlinear state estimation (DANSE) method that uses (noisy) nonlinear measurements of a process whose underlying state transition model (STM) is unknown. Such a process is referred to as a model-free process. A recurrent neural network (RNN) provides parameters of a Gaussian prior that characterize the state of the model-free process, using all previous measurements at a given time point. In the case of DANSE, the measurement system was linear, leading to a closed-form solution for the state posterior. However, the presence of a nonlinear measurement system renders a closed-form solution infeasible. Instead, the second-order statistics of the state posterior are computed using the nonlinear measurements observed at the time point. We address the nonlinear measurements using a reparameterization trick-based particle sampling approach, and estimate the second-order statistics of the state posterior. The proposed method is referred to as particle-based DANSE (pDANSE). The RNN of pDANSE uses sequential measurements efficiently and avoids the use of computationally intensive sequential Monte-Carlo (SMC) and/or ancestral sampling. We describe the semi-supervised learning method for pDANSE, which transitions to unsupervised learning in the absence of labeled data. Using a stochastic Lorenz-$63$ system as a benchmark process, we experimentally demonstrate the state estimation performance for four nonlinear measurement systems. We explore cubic nonlinearity and a camera-model nonlinearity where unsupervised learning is used; then we explore half-wave rectification nonlinearity and Cartesian-to-spherical nonlinearity where semi-supervised learning is used. The performance of state estimation is shown to be competitive vis-\\`a-vis particle filters that have complete knowledge of the STM of the Lorenz-$63$ system.",
    "fetched_at": "2025-11-05T02:19:05.052850Z"
  },
  {
    "id": "2510.27504v1",
    "title": "DP-FedPGN: Finding Global Flat Minima for Differentially Private   Federated Learning via Penalizing Gradient Norm",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Junkang Liu",
      "Yuxuan Tian",
      "Fanhua Shang",
      "Yuanyuan Liu",
      "Hongying Liu",
      "Junchao Zhou",
      "Daorui Ding"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27504v1",
    "abstract": "To prevent inference attacks in Federated Learning (FL) and reduce the leakage of sensitive information, Client-level Differentially Private Federated Learning (CL-DPFL) is widely used. However, current CL-DPFL methods usually result in sharper loss landscapes, which leads to a decrease in model generalization after differential privacy protection. By using Sharpness Aware Minimization (SAM), the current popular federated learning methods are to find a local flat minimum value to alleviate this problem. However, the local flatness may not reflect the global flatness in CL-DPFL. Therefore, to address this issue and seek global flat minima of models, we propose a new CL-DPFL algorithm, DP-FedPGN, in which we introduce a global gradient norm penalty to the local loss to find the global flat minimum. Moreover, by using our global gradient norm penalty, we not only find a flatter global minimum but also reduce the locally updated norm, which means that we further reduce the error of gradient clipping. From a theoretical perspective, we analyze how DP-FedPGN mitigates the performance degradation caused by DP. Meanwhile, the proposed DP-FedPGN algorithm eliminates the impact of data heterogeneity and achieves fast convergence. We also use R\\'enyi DP to provide strict privacy guarantees and provide sensitivity analysis for local updates. Finally, we conduct effectiveness tests on both ResNet and Transformer models, and achieve significant improvements in six visual and natural language processing tasks compared to existing state-of-the-art algorithms. The code is available at https://github.com/junkangLiu0/DP-FedPGN",
    "fetched_at": "2025-11-05T02:19:05.052796Z"
  },
  {
    "id": "2510.27506v1",
    "title": "Asynchronous Risk-Aware Multi-Agent Packet Routing for Ultra-Dense LEO   Satellite Networks",
    "date": "2025-10-31",
    "tags": [
      "cs.NI",
      "NI",
      "cs.IT",
      "IT",
      "cs.LG",
      "LG",
      "math.IT"
    ],
    "authors": [
      "Ke He",
      "Thang X. Vu",
      "Le He",
      "Lisheng Fan",
      "Symeon Chatzinotas",
      "Bjorn Ottersten"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27506v1",
    "abstract": "The rise of ultra-dense LEO constellations creates a complex and asynchronous network environment, driven by their massive scale, dynamic topologies, and significant delays. This unique complexity demands an adaptive packet routing algorithm that is asynchronous, risk-aware, and capable of balancing diverse and often conflicting QoS objectives in a decentralized manner. However, existing methods fail to address this need, as they typically rely on impractical synchronous decision-making and/or risk-oblivious approaches. To tackle this gap, we introduce PRIMAL, an event-driven multi-agent routing framework designed specifically to allow each satellite to act independently on its own event-driven timeline, while managing the risk of worst-case performance degradation via a principled primal-dual approach. This is achieved by enabling agents to learn the full cost distribution of the targeted QoS objectives and constrain tail-end risks. Extensive simulations on a LEO constellation with 1584 satellites validate its superiority in effectively optimizing latency and balancing load. Compared to a recent risk-oblivious baseline, it reduces queuing delay by over 70%, and achieves a nearly 12 ms end-to-end delay reduction in loaded scenarios. This is accomplished by resolving the core conflict between naive shortest-path finding and congestion avoidance, highlighting such autonomous risk-awareness as a key to robust routing.",
    "fetched_at": "2025-11-05T02:19:05.052733Z"
  },
  {
    "id": "2510.27508v1",
    "title": "Context-Gated Cross-Modal Perception with Visual Mamba for PET-CT Lung   Tumor Segmentation",
    "date": "2025-10-31",
    "tags": [
      "cs.CV",
      "CV",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Elena Mulero Ayllón",
      "Linlin Shen",
      "Pierangelo Veltri",
      "Fabrizia Gelardi",
      "Arturo Chiti",
      "Paolo Soda",
      "Matteo Tortora"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27508v1",
    "abstract": "Accurate lung tumor segmentation is vital for improving diagnosis and treatment planning, and effectively combining anatomical and functional information from PET and CT remains a major challenge. In this study, we propose vMambaX, a lightweight multimodal framework integrating PET and CT scan images through a Context-Gated Cross-Modal Perception Module (CGM). Built on the Visual Mamba architecture, vMambaX adaptively enhances inter-modality feature interaction, emphasizing informative regions while suppressing noise. Evaluated on the PCLT20K dataset, the model outperforms baseline models while maintaining lower computational complexity. These results highlight the effectiveness of adaptive cross-modal gating for multimodal tumor segmentation and demonstrate the potential of vMambaX as an efficient and scalable framework for advanced lung cancer analysis. The code is available at https://github.com/arco-group/vMambaX.",
    "fetched_at": "2025-11-05T02:19:05.052674Z"
  },
  {
    "id": "2510.27512v1",
    "title": "Effect of Domain Generalization Techniques in Low Resource Systems",
    "date": "2025-10-31",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Mahi Aminu",
      "Chisom Chibuike",
      "Fatimo Adebanjo",
      "Omokolade Awosanya",
      "Samuel Oyeneye"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27512v1",
    "abstract": "Machine learning models typically assume that training and test data follow the same distribution, an assumption that often fails in real-world scenarios due to distribution shifts. This issue is especially pronounced in low-resource settings, where data scarcity and limited domain diversity hinder robust generalization. Domain generalization (DG) approaches address this challenge by learning features that remain invariant across domains, often using causal mechanisms to improve model robustness. In this study, we examine two distinct causal DG techniques in low-resource natural language tasks. First, we investigate a causal data augmentation (CDA) approach that automatically generates counterfactual examples to improve robustness to spurious correlations. We apply this method to sentiment classification on the NaijaSenti Twitter corpus, expanding the training data with semantically equivalent paraphrases to simulate controlled distribution shifts. Second, we explore an invariant causal representation learning (ICRL) approach using the DINER framework, originally proposed for debiasing aspect-based sentiment analysis. We adapt DINER to a multilingual setting. Our findings demonstrate that both approaches enhance robustness to unseen domains: counterfactual data augmentation yields consistent cross-domain accuracy gains in sentiment classification, while causal representation learning with DINER improves out-of-distribution performance in multilingual sentiment analysis, albeit with varying gains across languages.",
    "fetched_at": "2025-11-05T02:19:05.052619Z"
  },
  {
    "id": "2510.27516v1",
    "title": "BiSparse-AAS: Bilinear Sparse Attention and Adaptive Spans Framework for   Scalable and Efficient Text Summarization",
    "date": "2025-10-31",
    "tags": [
      "cs.CL",
      "CL",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Desta Haileselassie Hagos",
      "Legand L. Burge",
      "Anietie Andy",
      "Anis Yazidi",
      "Vladimir Vlassov"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27516v1",
    "abstract": "Transformer-based architectures have advanced text summarization, yet their quadratic complexity limits scalability on long documents. This paper introduces BiSparse-AAS (Bilinear Sparse Attention with Adaptive Spans), a novel framework that combines sparse attention, adaptive spans, and bilinear attention to address these limitations. Sparse attention reduces computational costs by focusing on the most relevant parts of the input, while adaptive spans dynamically adjust the attention ranges. Bilinear attention complements both by modeling complex token interactions within this refined context. BiSparse-AAS consistently outperforms state-of-the-art baselines in both extractive and abstractive summarization tasks, achieving average ROUGE improvements of about 68.1% on CNN/DailyMail and 52.6% on XSum, while maintaining strong performance on OpenWebText and Gigaword datasets. By addressing efficiency, scalability, and long-sequence modeling, BiSparse-AAS provides a unified, practical solution for real-world text summarization applications.",
    "fetched_at": "2025-11-05T02:19:05.052566Z"
  },
  {
    "id": "2510.27517v1",
    "title": "Learning Sparse Approximate Inverse Preconditioners for Conjugate   Gradient Solvers on GPUs",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG",
      "cs.NA",
      "NA",
      "math.NA"
    ],
    "authors": [
      "Zherui Yang",
      "Zhehao Li",
      "Kangbo Lyu",
      "Yixuan Li",
      "Tao Du",
      "Ligang Liu"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27517v1",
    "abstract": "The conjugate gradient solver (CG) is a prevalent method for solving symmetric and positive definite linear systems Ax=b, where effective preconditioners are crucial for fast convergence. Traditional preconditioners rely on prescribed algorithms to offer rigorous theoretical guarantees, while limiting their ability to exploit optimization from data. Existing learning-based methods often utilize Graph Neural Networks (GNNs) to improve the performance and speed up the construction. However, their reliance on incomplete factorization leads to significant challenges: the associated triangular solve hinders GPU parallelization in practice, and introduces long-range dependencies which are difficult for GNNs to model. To address these issues, we propose a learning-based method to generate GPU-friendly preconditioners, particularly using GNNs to construct Sparse Approximate Inverse (SPAI) preconditioners, which avoids triangular solves and requires only two matrix-vector products at each CG step. The locality of matrix-vector product is compatible with the local propagation mechanism of GNNs. The flexibility of GNNs also allows our approach to be applied in a wide range of scenarios. Furthermore, we introduce a statistics-based scale-invariant loss function. Its design matches CG's property that the convergence rate depends on the condition number, rather than the absolute scale of A, leading to improved performance of the learned preconditioner. Evaluations on three PDE-derived datasets and one synthetic dataset demonstrate that our method outperforms standard preconditioners (Diagonal, IC, and traditional SPAI) and previous learning-based preconditioners on GPUs. We reduce solution time on GPUs by 40%-53% (68%-113% faster), along with better condition numbers and superior generalization performance. Source code available at https://github.com/Adversarr/LearningSparsePreconditioner4GPU",
    "fetched_at": "2025-11-05T02:19:05.052512Z"
  },
  {
    "id": "2510.27522v1",
    "title": "Leveraging Generic Time Series Foundation Models for EEG Classification",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Théo Gnassounou",
      "Yessin Moakher",
      "Shifeng Xie",
      "Vasilii Feofanov",
      "Ievgen Redko"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27522v1",
    "abstract": "Foundation models for time series are emerging as powerful general-purpose backbones, yet their potential for domain-specific biomedical signals such as electroencephalography (EEG) remains rather unexplored. In this work, we investigate the applicability a recently proposed time series classification foundation model, to a different EEG tasks such as motor imagery classification and sleep stage prediction. We test two pretraining regimes: (a) pretraining on heterogeneous real-world time series from multiple domains, and (b) pretraining on purely synthetic data. We find that both variants yield strong performance, consistently outperforming EEGNet, a widely used convolutional baseline, and CBraMod, the most recent EEG-specific foundation model. These results suggest that generalist time series foundation models, even when pretrained on data of non-neural origin or on synthetic signals, can transfer effectively to EEG. Our findings highlight the promise of leveraging cross-domain pretrained models for brain signal analysis, suggesting that EEG may benefit from advances in the broader time series literature.",
    "fetched_at": "2025-11-05T02:19:05.052451Z"
  },
  {
    "id": "2510.27525v1",
    "title": "Active transfer learning for structural health monitoring",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG"
    ],
    "authors": [
      "J. Poole",
      "N. Dervilis",
      "K. Worden",
      "P. Gardner",
      "V. Giglioni",
      "R. S. Mills",
      "A. J. Hughes"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27525v1",
    "abstract": "Data for training structural health monitoring (SHM) systems are often expensive and/or impractical to obtain, particularly for labelled data. Population-based SHM (PBSHM) aims to address this limitation by leveraging data from multiple structures. However, data from different structures will follow distinct distributions, potentially leading to large generalisation errors for models learnt via conventional machine learning methods. To address this issue, transfer learning -- in the form of domain adaptation (DA) -- can be used to align the data distributions. Most previous approaches have only considered \\emph{unsupervised} DA, where no labelled target data are available; they do not consider how to incorporate these technologies in an online framework -- updating as labels are obtained throughout the monitoring campaign. This paper proposes a Bayesian framework for DA in PBSHM, that can improve unsupervised DA mappings using a limited quantity of labelled target data. In addition, this model is integrated into an active sampling strategy to guide inspections to select the most informative observations to label -- leading to further reductions in the required labelled data to learn a target classifier. The effectiveness of this methodology is evaluated on a population of experimental bridges. Specifically, this population includes data corresponding to several damage states, as well as, a comprehensive set of environmental conditions. It is found that combining transfer learning and active learning can improve data efficiency when learning classification models in label-scarce scenarios. This result has implications for data-informed operation and maintenance of structures, suggesting a reduction in inspections over the operational lifetime of a structure -- and therefore a reduction in operational costs -- can be achieved.",
    "fetched_at": "2025-11-05T02:19:05.052401Z"
  },
  {
    "id": "2510.27527v1",
    "title": "TetraJet-v2: Accurate NVFP4 Training for Large Language Models with   Oscillation Suppression and Outlier Control",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Yuxiang Chen",
      "Xiaoming Xu",
      "Pengle Zhang",
      "Michael Beyer",
      "Martin Rapp",
      "Jun Zhu",
      "Jianfei Chen"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27527v1",
    "abstract": "Large Language Models (LLMs) training is prohibitively expensive, driving interest in low-precision fully-quantized training (FQT). While novel 4-bit formats like NVFP4 offer substantial efficiency gains, achieving near-lossless training at such low precision remains challenging. We introduce TetraJet-v2, an end-to-end 4-bit FQT method that leverages NVFP4 for activations, weights, and gradients in all linear layers. We identify two critical issues hindering low-precision LLM training: weight oscillation and outliers. To address these, we propose: 1) an unbiased double-block quantization method for NVFP4 linear layers, 2) OsciReset, an algorithm to suppress weight oscillation, and 3) OutControl, an algorithm to retain outlier accuracy. TetraJet-v2 consistently outperforms prior FP4 training methods on pre-training LLMs across varying model sizes up to 370M and data sizes up to 200B tokens, reducing the performance gap to full-precision training by an average of 51.3%.",
    "fetched_at": "2025-11-05T02:19:05.052338Z"
  },
  {
    "id": "2510.27530v1",
    "title": "Representing Classical Compositions through Implication-Realization   Temporal-Gestalt Graphs",
    "date": "2025-10-31",
    "tags": [
      "cs.SD",
      "SD",
      "cs.LG",
      "LG",
      "cs.SI",
      "SI",
      "H.5.5; G.2.2; I.5.4",
      "4"
    ],
    "authors": [
      "A. V. Bomediano",
      "R. J. Conanan",
      "L. D. Santuyo",
      "A. Coronel"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27530v1",
    "abstract": "Understanding the structural and cognitive underpinnings of musical compositions remains a key challenge in music theory and computational musicology. While traditional methods focus on harmony and rhythm, cognitive models such as the Implication-Realization (I-R) model and Temporal Gestalt theory offer insight into how listeners perceive and anticipate musical structure. This study presents a graph-based computational approach that operationalizes these models by segmenting melodies into perceptual units and annotating them with I-R patterns. These segments are compared using Dynamic Time Warping and organized into k-nearest neighbors graphs to model intra- and inter-segment relationships.   Each segment is represented as a node in the graph, and nodes are further labeled with melodic expectancy values derived from Schellenberg's two-factor I-R model-quantifying pitch proximity and pitch reversal at the segment level. This labeling enables the graphs to encode both structural and cognitive information, reflecting how listeners experience musical tension and resolution.   To evaluate the expressiveness of these graphs, we apply the Weisfeiler-Lehman graph kernel to measure similarity between and within compositions. Results reveal statistically significant distinctions between intra- and inter-graph structures. Segment-level analysis via multidimensional scaling confirms that structural similarity at the graph level reflects perceptual similarity at the segment level. Graph2vec embeddings and clustering demonstrate that these representations capture stylistic and structural features that extend beyond composer identity.   These findings highlight the potential of graph-based methods as a structured, cognitively informed framework for computational music analysis, enabling a more nuanced understanding of musical structure and style through the lens of listener perception.",
    "fetched_at": "2025-11-05T02:19:05.052279Z"
  },
  {
    "id": "2510.27532v1",
    "title": "SQLSpace: A Representation Space for Text-to-SQL to Discover and   Mitigate Robustness Gaps",
    "date": "2025-10-31",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Neha Srikanth",
      "Victor Bursztyn",
      "Puneet Mathur",
      "Ani Nenkova"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27532v1",
    "abstract": "We introduce SQLSpace, a human-interpretable, generalizable, compact representation for text-to-SQL examples derived with minimal human intervention. We demonstrate the utility of these representations in evaluation with three use cases: (i) closely comparing and contrasting the composition of popular text-to-SQL benchmarks to identify unique dimensions of examples they evaluate, (ii) understanding model performance at a granular level beyond overall accuracy scores, and (iii) improving model performance through targeted query rewriting based on learned correctness estimation. We show that SQLSpace enables analysis that would be difficult with raw examples alone: it reveals compositional differences between benchmarks, exposes performance patterns obscured by accuracy alone, and supports modeling of query success.",
    "fetched_at": "2025-11-05T02:19:05.052222Z"
  },
  {
    "id": "2510.27535v1",
    "title": "Patient-Centered Summarization Framework for AI Clinical Summarization:   A Mixed-Methods Design",
    "date": "2025-10-31",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Maria Lizarazo Jimenez",
      "Ana Gabriela Claros",
      "Kieran Green",
      "David Toro-Tobon",
      "Felipe Larios",
      "Sheena Asthana",
      "Camila Wenczenovicz",
      "Kerly Guevara Maldonado",
      "Luis Vilatuna-Andrango",
      "Cristina Proano-Velez",
      "Satya Sai Sri Bandi",
      "Shubhangi Bagewadi",
      "Megan E. Branda",
      "Misk Al Zahidy",
      "Saturnino Luz",
      "Mirella Lapata",
      "Juan P. Brito",
      "Oscar J. Ponce-Ponte"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27535v1",
    "abstract": "Large Language Models (LLMs) are increasingly demonstrating the potential to reach human-level performance in generating clinical summaries from patient-clinician conversations. However, these summaries often focus on patients' biology rather than their preferences, values, wishes, and concerns. To achieve patient-centered care, we propose a new standard for Artificial Intelligence (AI) clinical summarization tasks: Patient-Centered Summaries (PCS). Our objective was to develop a framework to generate PCS that capture patient values and ensure clinical utility and to assess whether current open-source LLMs can achieve human-level performance in this task. We used a mixed-methods process. Two Patient and Public Involvement groups (10 patients and 8 clinicians) in the United Kingdom participated in semi-structured interviews exploring what personal and contextual information should be included in clinical summaries and how it should be structured for clinical use. Findings informed annotation guidelines used by eight clinicians to create gold-standard PCS from 88 atrial fibrillation consultations. Sixteen consultations were used to refine a prompt aligned with the guidelines. Five open-source LLMs (Llama-3.2-3B, Llama-3.1-8B, Mistral-8B, Gemma-3-4B, and Qwen3-8B) generated summaries for 72 consultations using zero-shot and few-shot prompting, evaluated with ROUGE-L, BERTScore, and qualitative metrics. Patients emphasized lifestyle routines, social support, recent stressors, and care values. Clinicians sought concise functional, psychosocial, and emotional context. The best zero-shot performance was achieved by Mistral-8B (ROUGE-L 0.189) and Llama-3.1-8B (BERTScore 0.673); the best few-shot by Llama-3.1-8B (ROUGE-L 0.206, BERTScore 0.683). Completeness and fluency were similar between experts and models, while correctness and patient-centeredness favored human PCS.",
    "fetched_at": "2025-11-05T02:19:05.052178Z"
  },
  {
    "id": "2510.27537v1",
    "title": "AstuteRAG-FQA: Task-Aware Retrieval-Augmented Generation Framework for   Proprietary Data Challenges in Financial Question Answering",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Mohammad Zahangir Alam",
      "Khandoker Ashik Uz Zaman",
      "Mahdi H. Miraz"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27537v1",
    "abstract": "Retrieval-Augmented Generation (RAG) shows significant promise in knowledge-intensive tasks by improving domain specificity, enhancing temporal relevance, and reducing hallucinations. However, applying RAG to finance encounters critical challenges: restricted access to proprietary datasets, limited retrieval accuracy, regulatory constraints, and sensitive data interpretation. We introduce AstuteRAG-FQA, an adaptive RAG framework tailored for Financial Question Answering (FQA), leveraging task-aware prompt engineering to address these challenges. The framework uses a hybrid retrieval strategy integrating both open-source and proprietary financial data while maintaining strict security protocols and regulatory compliance. A dynamic prompt framework adapts in real time to query complexity, improving precision and contextual relevance. To systematically address diverse financial queries, we propose a four-tier task classification: explicit factual, implicit factual, interpretable rationale, and hidden rationale involving implicit causal reasoning. For each category, we identify key challenges, datasets, and optimization techniques within the retrieval and generation process. The framework incorporates multi-layered security mechanisms including differential privacy, data anonymization, and role-based access controls to protect sensitive financial information. Additionally, AstuteRAG-FQA implements real-time compliance monitoring through automated regulatory validation systems that verify responses against industry standards and legal obligations. We evaluate three data integration techniques - contextual embedding, small model augmentation, and targeted fine-tuning - analyzing their efficiency and feasibility across varied financial environments.",
    "fetched_at": "2025-11-05T02:19:05.052075Z"
  },
  {
    "id": "2510.27543v1",
    "title": "DialectalArabicMMLU: Benchmarking Dialectal Capabilities in Arabic and   Multilingual Language Models",
    "date": "2025-10-31",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Malik H. Altakrori",
      "Nizar Habash",
      "Abdelhakim Freihat",
      "Younes Samih",
      "Kirill Chirkunov",
      "Muhammed AbuOdeh",
      "Radu Florian",
      "Teresa Lynn",
      "Preslav Nakov",
      "Alham Fikri Aji"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27543v1",
    "abstract": "We present DialectalArabicMMLU, a new benchmark for evaluating the performance of large language models (LLMs) across Arabic dialects. While recently developed Arabic and multilingual benchmarks have advanced LLM evaluation for Modern Standard Arabic (MSA), dialectal varieties remain underrepresented despite their prevalence in everyday communication. DialectalArabicMMLU extends the MMLU-Redux framework through manual translation and adaptation of 3K multiple-choice question-answer pairs into five major dialects (Syrian, Egyptian, Emirati, Saudi, and Moroccan), yielding a total of 15K QA pairs across 32 academic and professional domains (22K QA pairs when also including English and MSA). The benchmark enables systematic assessment of LLM reasoning and comprehension beyond MSA, supporting both task-based and linguistic analysis. We evaluate 19 open-weight Arabic and multilingual LLMs (1B-13B parameters) and report substantial performance variation across dialects, revealing persistent gaps in dialectal generalization. DialectalArabicMMLU provides the first unified, human-curated resource for measuring dialectal understanding in Arabic, thus promoting more inclusive evaluation and future model development.",
    "fetched_at": "2025-11-05T02:19:05.052024Z"
  },
  {
    "id": "2510.27545v1",
    "title": "EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities",
    "date": "2025-10-31",
    "tags": [
      "cs.RO",
      "RO",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Travis Davies",
      "Yiqi Huang",
      "Alexi Gladstone",
      "Yunxin Liu",
      "Xiang Chen",
      "Heng Ji",
      "Huxian Liu",
      "Luhui Hu"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27545v1",
    "abstract": "Implicit policies parameterized by generative models, such as Diffusion Policy, have become the standard for policy learning and Vision-Language-Action (VLA) models in robotics. However, these approaches often suffer from high computational cost, exposure bias, and unstable inference dynamics, which lead to divergence under distribution shifts. Energy-Based Models (EBMs) address these issues by learning energy landscapes end-to-end and modeling equilibrium dynamics, offering improved robustness and reduced exposure bias. Yet, policies parameterized by EBMs have historically struggled to scale effectively. Recent work on Energy-Based Transformers (EBTs) demonstrates the scalability of EBMs to high-dimensional spaces, but their potential for solving core challenges in physically embodied models remains underexplored. We introduce a new energy-based architecture, EBT-Policy, that solves core issues in robotic and real-world settings. Across simulated and real-world tasks, EBT-Policy consistently outperforms diffusion-based policies, while requiring less training and inference computation. Remarkably, on some tasks it converges within just two inference steps, a 50x reduction compared to Diffusion Policy's 100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior models, such as zero-shot recovery from failed action sequences using only behavior cloning and without explicit retry training. By leveraging its scalar energy for uncertainty-aware inference and dynamic compute allocation, EBT-Policy offers a promising path toward robust, generalizable robot behavior under distribution shifts.",
    "fetched_at": "2025-11-05T02:19:05.051887Z"
  },
  {
    "id": "2510.27552v1",
    "title": "Multilingual BERT language model for medical tasks: Evaluation on   domain-specific adaptation and cross-linguality",
    "date": "2025-10-31",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Yinghao Luo",
      "Lang Zhou",
      "Amrish Jhingoer",
      "Klaske Vliegenthart Jongbloed",
      "Carlijn Jordans",
      "Ben Werkhoven",
      "Tom Seinen",
      "Erik van Mulligen",
      "Casper Rokx",
      "Yunlei Li"
    ],
    "institution": "Department of Data and Analytics, Erasmus University Medical Center Rotterdam, Department of Internal Medicine, Erasmus University Medical Center Rotterdam, Department of Medical Informatics, Erasmus University Medical Center Rotterdam, Department of Medical Microbiology and Infectious Diseases, Erasmus University Medical Center Rotterdam, Department of Pathology & Clinical Bioinformatics, Erasmus University Medical Center Rotterdam",
    "link": "http://arxiv.org/pdf/2510.27552v1",
    "abstract": "In multilingual healthcare applications, the availability of domain-specific natural language processing(NLP) tools is limited, especially for low-resource languages. Although multilingual bidirectional encoder representations from transformers (BERT) offers a promising motivation to mitigate the language gap, the medical NLP tasks in low-resource languages are still underexplored. Therefore, this study investigates how further pre-training on domain-specific corpora affects model performance on medical tasks, focusing on three languages: Dutch, Romanian and Spanish. In terms of further pre-training, we conducted four experiments to create medical domain models. Then, these models were fine-tuned on three downstream tasks: Automated patient screening in Dutch clinical notes, named entity recognition in Romanian and Spanish clinical notes. Results show that domain adaptation significantly enhanced task performance. Furthermore, further differentiation of domains, e.g. clinical and general biomedical domains, resulted in diverse performances. The clinical domain-adapted model outperformed the more general biomedical domain-adapted model. Moreover, we observed evidence of cross-lingual transferability. Moreover, we also conducted further investigations to explore potential reasons contributing to these performance differences. These findings highlight the feasibility of domain adaptation and cross-lingual ability in medical NLP. Within the low-resource language settings, these findings can provide meaningful guidance for developing multilingual medical NLP systems to mitigate the lack of training data and thereby improve the model performance.",
    "fetched_at": "2025-11-05T02:19:05.051821Z"
  },
  {
    "id": "2510.27554v1",
    "title": "Sybil-Resistant Service Discovery for Agent Economies",
    "date": "2025-10-31",
    "tags": [
      "cs.CR",
      "CR",
      "cs.AI",
      "AI",
      "cs.SI",
      "SI",
      "68T42, 68R10, 68M14, 68P20, 91D30",
      "H.3.3; H.2.8; I.2.11; K.4.4; G.2.2; C.2.4",
      "4"
    ],
    "authors": [
      "David Shi",
      "Kevin Joo"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27554v1",
    "abstract": "x402 enables Hypertext Transfer Protocol (HTTP) services like application programming interfaces (APIs), data feeds, and inference providers to accept cryptocurrency payments for access. As agents increasingly consume these services, discovery becomes critical: which swap interface should an agent trust? Which data provider is the most reliable? We introduce TraceRank, a reputation-weighted ranking algorithm where payment transactions serve as endorsements. TraceRank seeds addresses with precomputed reputation metrics and propagates reputation through payment flows weighted by transaction value and temporal recency. Applied to x402's payment graph, this surfaces services preferred by high-reputation users rather than those with high transaction volume. Our system combines TraceRank with semantic search to respond to natural language queries with high quality results. We argue that reputation propagation resists Sybil attacks by making spam services with many low-reputation payers rank below legitimate services with few high-reputation payers. Ultimately, we aim to construct a search method for x402 enabled services that avoids infrastructure bias and has better performance than purely volume based or semantic methods.",
    "fetched_at": "2025-11-05T02:19:05.051757Z"
  },
  {
    "id": "2510.27556v1",
    "title": "Data-Efficient Domain Adaptation for LLM-based MT using Contrastive   Preference Optimization",
    "date": "2025-10-31",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Inacio Vieira",
      "Antonio Castaldo",
      "James O'Doherty",
      "Sheila Castilho"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27556v1",
    "abstract": "LLMs often require adaptation to domain-specific requirements, a process that can be expensive when relying solely on SFT. We present an empirical study on applying CPO to simulate a post-editing workflow for data-efficient domain adaptation. Our approach synthesizes preference pairs by treating the base model's own raw output as the 'rejected' translation and the human-approved TM entry as the 'chosen' one. This method provides direct feedback on the model's current knowledge, guiding it to align with domain-specific standards. Experiments in English-Brazilian Portuguese and English-Korean show that, by using just 14.7k preference pairs, the model achieves performance close to that of a model trained on 160k+ samples with SFT, demonstrating significant data efficiency. Although we showcase its effectiveness in MT, this application of CPO naturally generalizes to other generative tasks where a model's initial drafts can serve as a contrastive signal against a golden reference.",
    "fetched_at": "2025-11-05T02:19:05.051713Z"
  },
  {
    "id": "2510.27558v1",
    "title": "Toward Accurate Long-Horizon Robotic Manipulation: Language-to-Action   with Foundation Models via Scene Graphs",
    "date": "2025-10-31",
    "tags": [
      "cs.RO",
      "RO",
      "cs.AI",
      "AI",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Sushil Samuel Dinesh",
      "Shinkyu Park"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27558v1",
    "abstract": "This paper presents a framework that leverages pre-trained foundation models for robotic manipulation without domain-specific training. The framework integrates off-the-shelf models, combining multimodal perception from foundation models with a general-purpose reasoning model capable of robust task sequencing. Scene graphs, dynamically maintained within the framework, provide spatial awareness and enable consistent reasoning about the environment. The framework is evaluated through a series of tabletop robotic manipulation experiments, and the results highlight its potential for building robotic manipulation systems directly on top of off-the-shelf foundation models.",
    "fetched_at": "2025-11-05T02:19:05.051667Z"
  },
  {
    "id": "2510.27562v1",
    "title": "Optimal Convergence Analysis of DDPM for General Distributions",
    "date": "2025-10-31",
    "tags": [
      "stat.ML",
      "ML",
      "cs.LG",
      "LG",
      "math.ST",
      "ST",
      "stat.TH",
      "TH"
    ],
    "authors": [
      "Yuchen Jiao",
      "Yuchen Zhou",
      "Gen Li"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27562v1",
    "abstract": "Score-based diffusion models have achieved remarkable empirical success in generating high-quality samples from target data distributions. Among them, the Denoising Diffusion Probabilistic Model (DDPM) is one of the most widely used samplers, generating samples via estimated score functions. Despite its empirical success, a tight theoretical understanding of DDPM -- especially its convergence properties -- remains limited.   In this paper, we provide a refined convergence analysis of the DDPM sampler and establish near-optimal convergence rates under general distributional assumptions. Specifically, we introduce a relaxed smoothness condition parameterized by a constant $L$, which is small for many practical distributions (e.g., Gaussian mixture models). We prove that the DDPM sampler with accurate score estimates achieves a convergence rate of $$\\widetilde{O}\\left(\\frac{d\\min\\{d,L^2\\}}{T^2}\\right)~\\text{in Kullback-Leibler divergence},$$ where $d$ is the data dimension, $T$ is the number of iterations, and $\\widetilde{O}$ hides polylogarithmic factors in $T$. This result substantially improves upon the best-known $d^2/T^2$ rate when $L < \\sqrt{d}$. By establishing a matching lower bound, we show that our convergence analysis is tight for a wide array of target distributions. Moreover, it reveals that DDPM and DDIM share the same dependence on $d$, raising an interesting question of why DDIM often appears empirically faster.",
    "fetched_at": "2025-11-05T02:19:05.051630Z"
  },
  {
    "id": "2510.27565v1",
    "title": "CodeAlignBench: Assessing Code Generation Models on Developer-Preferred   Code Adjustments",
    "date": "2025-10-31",
    "tags": [
      "cs.SE",
      "SE",
      "cs.AI",
      "AI",
      "cs.HC",
      "HC"
    ],
    "authors": [
      "Forough Mehralian",
      "Ryan Shar",
      "James R. Rae",
      "Alireza Hashemi"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27565v1",
    "abstract": "As large language models become increasingly capable of generating code, evaluating their performance remains a complex and evolving challenge. Existing benchmarks primarily focus on functional correctness, overlooking the diversity of real-world coding tasks and developer expectations. To this end, we introduce a multi-language benchmark that evaluates LLM instruction-following capabilities and is extensible to operate on any set of standalone coding problems. Our benchmark evaluates instruction following in two key settings: adherence to pre-defined constraints specified with the initial problem, and the ability to perform refinements based on follow-up instructions. For this paper's analysis, we empirically evaluated our benchmarking pipeline with programming tasks from LiveBench, that are also automatically translated from Python into Java and JavaScript. Our automated benchmark reveals that models exhibit differing levels of performance across multiple dimensions of instruction-following. Our benchmarking pipeline provides a more comprehensive evaluation of code generation models, highlighting their strengths and limitations across languages and generation goals.",
    "fetched_at": "2025-11-05T02:19:05.051584Z"
  },
  {
    "id": "2510.27568v1",
    "title": "SIGMA: Search-Augmented On-Demand Knowledge Integration for Agentic   Mathematical Reasoning",
    "date": "2025-10-31",
    "tags": [
      "cs.AI",
      "AI",
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Ali Asgarov",
      "Umid Suleymanov",
      "Aadyant Khatri"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27568v1",
    "abstract": "Solving mathematical reasoning problems requires not only accurate access to relevant knowledge but also careful, multi-step thinking. However, current retrieval-augmented models often rely on a single perspective, follow inflexible search strategies, and struggle to effectively combine information from multiple sources. We introduce SIGMA (Search-Augmented On-Demand Knowledge Integration for AGentic Mathematical reAsoning), a unified framework that orchestrates specialized agents to independently reason, perform targeted searches, and synthesize findings through a moderator mechanism. Each agent generates hypothetical passages to optimize retrieval for its analytic perspective, ensuring knowledge integration is both context-sensitive and computation-efficient. When evaluated on challenging benchmarks such as MATH500, AIME, and PhD-level science QA GPQA, SIGMA consistently outperforms both open- and closed-source systems, achieving an absolute performance improvement of 7.4%. Our results demonstrate that multi-agent, on-demand knowledge integration significantly enhances both reasoning accuracy and efficiency, offering a scalable approach for complex, knowledge-intensive problem-solving. We will release the code upon publication.",
    "fetched_at": "2025-11-05T02:19:05.051536Z"
  },
  {
    "id": "2510.27571v1",
    "title": "Towards Universal Video Retrieval: Generalizing Video Embedding via   Synthesized Multimodal Pyramid Curriculum",
    "date": "2025-10-31",
    "tags": [
      "cs.CV",
      "CV",
      "cs.AI",
      "AI",
      "cs.CL",
      "CL",
      "cs.IR",
      "IR",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Zhuoning Guo",
      "Mingxin Li",
      "Yanzhao Zhang",
      "Dingkun Long",
      "Pengjun Xie",
      "Xiaowen Chu"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27571v1",
    "abstract": "The prevailing video retrieval paradigm is structurally misaligned, as narrow benchmarks incentivize correspondingly limited data and single-task training. Therefore, universal capability is suppressed due to the absence of a diagnostic evaluation that defines and demands multi-dimensional generalization. To break this cycle, we introduce a framework built on the co-design of evaluation, data, and modeling. First, we establish the Universal Video Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to measure performance but also to diagnose critical capability gaps across tasks and domains. Second, guided by UVRB's diagnostics, we introduce a scalable synthesis workflow that generates 1.55 million high-quality pairs to populate the semantic space required for universality. Finally, we devise the Modality Pyramid, a curriculum that trains our General Video Embedder (GVE) by explicitly leveraging the latent interconnections within our diverse data. Extensive experiments show GVE achieves state-of-the-art zero-shot generalization on UVRB. In particular, our analysis reveals that popular benchmarks are poor predictors of general ability and that partially relevant retrieval is a dominant but overlooked scenario. Overall, our co-designed framework provides a practical path to escape the limited scope and advance toward truly universal video retrieval.",
    "fetched_at": "2025-11-05T02:19:05.051427Z"
  },
  {
    "id": "2510.27584v2",
    "title": "Image Hashing via Cross-View Code Alignment in the Age of Foundation   Models",
    "date": "2025-10-31",
    "tags": [
      "cs.CV",
      "CV",
      "cs.IR",
      "IR",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Ilyass Moummad",
      "Kawtar Zaher",
      "Hervé Goëau",
      "Alexis Joly"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27584v2",
    "abstract": "Efficient large-scale retrieval requires representations that are both compact and discriminative. Foundation models provide powerful visual and multimodal embeddings, but nearest neighbor search in these high-dimensional spaces is computationally expensive. Hashing offers an efficient alternative by enabling fast Hamming distance search with binary codes, yet existing approaches often rely on complex pipelines, multi-term objectives, designs specialized for a single learning paradigm, and long training times. We introduce CroVCA (Cross-View Code Alignment), a simple and unified principle for learning binary codes that remain consistent across semantically aligned views. A single binary cross-entropy loss enforces alignment, while coding-rate maximization serves as an anti-collapse regularizer to promote balanced and diverse codes. To implement this, we design HashCoder, a lightweight MLP hashing network with a final batch normalization layer to enforce balanced codes. HashCoder can be used as a probing head on frozen embeddings or to adapt encoders efficiently via LoRA fine-tuning. Across benchmarks, CroVCA achieves state-of-the-art results in just 5 training epochs. At 16 bits, it particularly well-for instance, unsupervised hashing on COCO completes in under 2 minutes and supervised hashing on ImageNet100 in about 3 minutes on a single GPU. These results highlight CroVCA's efficiency, adaptability, and broad applicability.",
    "fetched_at": "2025-11-05T02:19:05.051370Z"
  },
  {
    "id": "2510.27588v1",
    "title": "Learned Static Function Data Structures",
    "date": "2025-10-31",
    "tags": [
      "cs.DS",
      "DS",
      "cs.DB",
      "DB",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Stefan Hermann",
      "Hans-Peter Lehmann",
      "Giorgio Vinciguerra",
      "Stefan Walzer"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27588v1",
    "abstract": "We consider the task of constructing a data structure for associating a static set of keys with values, while allowing arbitrary output values for queries involving keys outside the set. Compared to hash tables, these so-called static function data structures do not need to store the key set and thus use significantly less memory. Several techniques are known, with compressed static functions approaching the zero-order empirical entropy of the value sequence. In this paper, we introduce learned static functions, which use machine learning to capture correlations between keys and values. For each key, a model predicts a probability distribution over the values, from which we derive a key-specific prefix code to compactly encode the true value. The resulting codeword is stored in a classic static function data structure. This design allows learned static functions to break the zero-order entropy barrier while still supporting point queries. Our experiments show substantial space savings: up to one order of magnitude on real data, and up to three orders of magnitude on synthetic data.",
    "fetched_at": "2025-11-05T02:19:05.051319Z"
  },
  {
    "id": "2510.27606v1",
    "title": "Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised   Reinforcement Learning",
    "date": "2025-10-31",
    "tags": [
      "cs.CV",
      "CV",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Yuhong Liu",
      "Beichen Zhang",
      "Yuhang Zang",
      "Yuhang Cao",
      "Long Xing",
      "Xiaoyi Dong",
      "Haodong Duan",
      "Dahua Lin",
      "Jiaqi Wang"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27606v1",
    "abstract": "Spatial understanding remains a weakness of Large Vision-Language Models (LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcement learning with verifiable rewards (RLVR) pipelines depend on costly supervision, specialized tools, or constrained environments that limit scale. We introduce Spatial-SSRL, a self-supervised RL paradigm that derives verifiable signals directly from ordinary RGB or RGB-D images. Spatial-SSRL automatically formulates five pretext tasks that capture 2D and 3D spatial structure: shuffled patch reordering, flipped patch recognition, cropped patch inpainting, regional depth ordering, and relative 3D position prediction. These tasks provide ground-truth answers that are easy to verify and require no human or LVLM annotation. Training on our tasks substantially improves spatial reasoning while preserving general visual capabilities. On seven spatial understanding benchmarks in both image and video settings, Spatial-SSRL delivers average accuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Our results show that simple, intrinsic supervision enables RLVR at scale and provides a practical route to stronger spatial intelligence in LVLMs.",
    "fetched_at": "2025-11-05T02:19:05.051180Z"
  },
  {
    "id": "2510.27610v1",
    "title": "ORGEval: Graph-Theoretic Evaluation of LLMs in Optimization Modeling",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Zhuohan Wang",
      "Ziwei Zhu",
      "Ziniu Li",
      "Congliang Chen",
      "Yizhou Han",
      "Yufeng Lin",
      "Zhihang Lin",
      "Angyang Gu",
      "Xinglin Hu",
      "Ruoyu Sun",
      "Tian Ding"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27610v1",
    "abstract": "Formulating optimization problems for industrial applications demands significant manual effort and domain expertise. While Large Language Models (LLMs) show promise in automating this process, evaluating their performance remains difficult due to the absence of robust metrics. Existing solver-based approaches often face inconsistency, infeasibility issues, and high computational costs. To address these issues, we propose ORGEval, a graph-theoretic evaluation framework for assessing LLMs' capabilities in formulating linear and mixed-integer linear programs. ORGEval represents optimization models as graphs, reducing equivalence detection to graph isomorphism testing. We identify and prove a sufficient condition, when the tested graphs are symmetric decomposable (SD), under which the Weisfeiler-Lehman (WL) test is guaranteed to correctly detect isomorphism. Building on this, ORGEval integrates a tailored variant of the WL-test with an SD detection algorithm to evaluate model equivalence. By focusing on structural equivalence rather than instance-level configurations, ORGEval is robust to numerical variations. Experimental results show that our method can successfully detect model equivalence and produce 100\\% consistent results across random parameter configurations, while significantly outperforming solver-based methods in runtime, especially on difficult problems. Leveraging ORGEval, we construct the Bench4Opt dataset and benchmark state-of-the-art LLMs on optimization modeling. Our results reveal that although optimization modeling remains challenging for all LLMs, DeepSeek-V3 and Claude-Opus-4 achieve the highest accuracies under direct prompting, outperforming even leading reasoning models.",
    "fetched_at": "2025-11-05T02:19:05.051114Z"
  },
  {
    "id": "2510.27629v3",
    "title": "Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation   Models",
    "date": "2025-10-31",
    "tags": [
      "cs.CR",
      "CR",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Boyi Wei",
      "Zora Che",
      "Nathaniel Li",
      "Udari Madhushani Sehwag",
      "Jasper Götting",
      "Samira Nedungadi",
      "Julian Michael",
      "Summer Yue",
      "Dan Hendrycks",
      "Peter Henderson",
      "Zifan Wang",
      "Seth Donoughe",
      "Mantas Mazeika"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27629v3",
    "abstract": "Open-weight bio-foundation models present a dual-use dilemma. While holding great promise for accelerating scientific research and drug development, they could also enable bad actors to develop more deadly bioweapons. To mitigate the risk posed by these models, current approaches focus on filtering biohazardous data during pre-training. However, the effectiveness of such an approach remains unclear, particularly against determined actors who might fine-tune these models for malicious use. To address this gap, we propose BioRiskEval, a framework to evaluate the robustness of procedures that are intended to reduce the dual-use capabilities of bio-foundation models. BioRiskEval assesses models' virus understanding through three lenses, including sequence modeling, mutational effects prediction, and virulence prediction. Our results show that current filtering practices may not be particularly effective: Excluded knowledge can be rapidly recovered in some cases via fine-tuning, and exhibits broader generalizability in sequence modeling. Furthermore, dual-use signals may already reside in the pretrained representations, and can be elicited via simple linear probing. These findings highlight the challenges of data filtering as a standalone procedure, underscoring the need for further research into robust safety and security strategies for open-weight bio-foundation models.",
    "fetched_at": "2025-11-05T02:19:05.050809Z"
  },
  {
    "id": "2510.27632v1",
    "title": "Sketch-to-Layout: Sketch-Guided Multimodal Layout Generation",
    "date": "2025-10-31",
    "tags": [
      "cs.CV",
      "CV",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Riccardo Brioschi",
      "Aleksandr Alekseev",
      "Emanuele Nevali",
      "Berkay Döner",
      "Omar El Malki",
      "Blagoj Mitrevski",
      "Leandro Kieliger",
      "Mark Collier",
      "Andrii Maksai",
      "Jesse Berent",
      "Claudiu Musat",
      "Efi Kokiopoulou"
    ],
    "institution": "Google, DeepMind",
    "link": "http://arxiv.org/pdf/2510.27632v1",
    "abstract": "Graphic layout generation is a growing research area focusing on generating aesthetically pleasing layouts ranging from poster designs to documents. While recent research has explored ways to incorporate user constraints to guide the layout generation, these constraints often require complex specifications which reduce usability. We introduce an innovative approach exploiting user-provided sketches as intuitive constraints and we demonstrate empirically the effectiveness of this new guidance method, establishing the sketch-to-layout problem as a promising research direction, which is currently under-explored. To tackle the sketch-to-layout problem, we propose a multimodal transformer-based solution using the sketch and the content assets as inputs to produce high quality layouts. Since collecting sketch training data from human annotators to train our model is very costly, we introduce a novel and efficient method to synthetically generate training sketches at scale. We train and evaluate our model on three publicly available datasets: PubLayNet, DocLayNet and SlidesVQA, demonstrating that it outperforms state-of-the-art constraint-based methods, while offering a more intuitive design experience. In order to facilitate future sketch-to-layout research, we release O(200k) synthetically-generated sketches for the public datasets above. The datasets are available at https://github.com/google-deepmind/sketch_to_layout.",
    "fetched_at": "2025-11-05T02:19:05.050629Z"
  },
  {
    "id": "2510.27638v1",
    "title": "Panprediction: Optimal Predictions for Any Downstream Task and Loss",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Sivaraman Balakrishnan",
      "Nika Haghtalab",
      "Daniel Hsu",
      "Brian Lee",
      "Eric Zhao"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27638v1",
    "abstract": "Supervised learning is classically formulated as training a model to minimize a fixed loss function over a fixed distribution, or task. However, an emerging paradigm instead views model training as extracting enough information from data so that the model can be used to minimize many losses on many downstream tasks. We formalize a mathematical framework for this paradigm, which we call panprediction, and study its statistical complexity. Formally, panprediction generalizes omniprediction and sits upstream from multi-group learning, which respectively focus on predictions that generalize to many downstream losses or many downstream tasks, but not both. Concretely, we design algorithms that learn deterministic and randomized panpredictors with $\\tilde{O}(1/\\varepsilon^3)$ and $\\tilde{O}(1/\\varepsilon^2)$ samples, respectively. Our results demonstrate that under mild assumptions, simultaneously minimizing infinitely many losses on infinitely many tasks can be as statistically easy as minimizing one loss on one task. Along the way, we improve the best known sample complexity guarantee of deterministic omniprediction by a factor of $1/\\varepsilon$, and match all other known sample complexity guarantees of omniprediction and multi-group learning. Our key technical ingredient is a nearly lossless reduction from panprediction to a statistically efficient notion of calibration, called step calibration.",
    "fetched_at": "2025-11-05T02:19:05.050546Z"
  },
  {
    "id": "2510.27640v1",
    "title": "Enhancing software product lines with machine learning components",
    "date": "2025-10-31",
    "tags": [
      "cs.SE",
      "SE",
      "cs.LG",
      "LG",
      "D.2",
      "2"
    ],
    "authors": [
      "Luz-Viviana Cobaleda",
      "Julián Carvajal",
      "Paola Vallejo",
      "Andrés López",
      "Raúl Mazo"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27640v1",
    "abstract": "Modern software systems increasingly integrate machine learning (ML) due to its advancements and ability to enhance data-driven decision-making. However, this integration introduces significant challenges for software engineering, especially in software product lines (SPLs), where managing variability and reuse becomes more complex with the inclusion of ML components. Although existing approaches have addressed variability management in SPLs and the integration of ML components in isolated systems, few have explored the intersection of both domains. Specifically, there is limited support for modeling and managing variability in SPLs that incorporate ML components. To bridge this gap, this article proposes a structured framework designed to extend Software Product Line engineering, facilitating the integration of ML components. It facilitates the design of SPLs with ML capabilities by enabling systematic modeling of variability and reuse. The proposal has been partially implemented with the VariaMos tool.",
    "fetched_at": "2025-11-05T02:19:05.050493Z"
  },
  {
    "id": "2510.27641v1",
    "title": "SpecAttn: Speculating Sparse Attention",
    "date": "2025-10-31",
    "tags": [
      "cs.CL",
      "CL",
      "cs.LG",
      "LG",
      "cs.SY",
      "SY",
      "eess.SY"
    ],
    "authors": [
      "Harsh Shah"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27641v1",
    "abstract": "Large Language Models (LLMs) face significant computational bottlenecks during inference due to the quadratic complexity of self-attention mechanisms, particularly as context lengths increase. We introduce SpecAttn, a novel training-free approach that seamlessly integrates with existing speculative decoding techniques to enable efficient sparse attention in pre-trained transformers. Our key insight is to exploit the attention weights already computed by the draft model during speculative decoding to identify important tokens for the target model, eliminating redundant computation while maintaining output quality. SpecAttn employs three core techniques: KL divergence-based layer alignment between draft and target models, a GPU-optimized sorting-free algorithm for top-p token selection from draft attention patterns, and dynamic key-value cache pruning guided by these predictions. By leveraging the computational work already performed in standard speculative decoding pipelines, SpecAttn achieves over 75% reduction in key-value cache accesses with a mere 15.29% increase in perplexity on the PG-19 dataset, significantly outperforming existing sparse attention methods. Our approach demonstrates that speculative execution can be enhanced to provide approximate verification without significant performance degradation.",
    "fetched_at": "2025-11-05T02:19:05.050441Z"
  },
  {
    "id": "2510.27643v1",
    "title": "Bayesian Optimization on Networks",
    "date": "2025-10-31",
    "tags": [
      "stat.ML",
      "ML",
      "cs.LG",
      "LG",
      "cs.NA",
      "NA",
      "math.NA",
      "math.OC",
      "OC",
      "stat.CO",
      "CO"
    ],
    "authors": [
      "Wenwen Li",
      "Daniel Sanz-Alonso",
      "Ruiyi Yang"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27643v1",
    "abstract": "This paper studies optimization on networks modeled as metric graphs. Motivated by applications where the objective function is expensive to evaluate or only available as a black box, we develop Bayesian optimization algorithms that sequentially update a Gaussian process surrogate model of the objective to guide the acquisition of query points. To ensure that the surrogates are tailored to the network's geometry, we adopt Whittle-Mat\\'ern Gaussian process prior models defined via stochastic partial differential equations on metric graphs. In addition to establishing regret bounds for optimizing sufficiently smooth objective functions, we analyze the practical case in which the smoothness of the objective is unknown and the Whittle-Mat\\'ern prior is represented using finite elements. Numerical results demonstrate the effectiveness of our algorithms for optimizing benchmark objective functions on a synthetic metric graph and for Bayesian inversion via maximum a posteriori estimation on a telecommunication network.",
    "fetched_at": "2025-11-05T02:19:05.050400Z"
  },
  {
    "id": "2510.27646v1",
    "title": "VessShape: Few-shot 2D blood vessel segmentation by leveraging shape   priors from synthetic images",
    "date": "2025-10-31",
    "tags": [
      "cs.CV",
      "CV",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Cesar H. Comin",
      "Wesley N. Galvão"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27646v1",
    "abstract": "Semantic segmentation of blood vessels is an important task in medical image analysis, but its progress is often hindered by the scarcity of large annotated datasets and the poor generalization of models across different imaging modalities. A key aspect is the tendency of Convolutional Neural Networks (CNNs) to learn texture-based features, which limits their performance when applied to new domains with different visual characteristics. We hypothesize that leveraging geometric priors of vessel shapes, such as their tubular and branching nature, can lead to more robust and data-efficient models. To investigate this, we introduce VessShape, a methodology for generating large-scale 2D synthetic datasets designed to instill a shape bias in segmentation models. VessShape images contain procedurally generated tubular geometries combined with a wide variety of foreground and background textures, encouraging models to learn shape cues rather than textures. We demonstrate that a model pre-trained on VessShape images achieves strong few-shot segmentation performance on two real-world datasets from different domains, requiring only four to ten samples for fine-tuning. Furthermore, the model exhibits notable zero-shot capabilities, effectively segmenting vessels in unseen domains without any target-specific training. Our results indicate that pre-training with a strong shape bias can be an effective strategy to overcome data scarcity and improve model generalization in blood vessel segmentation.",
    "fetched_at": "2025-11-05T02:19:05.050355Z"
  },
  {
    "id": "2510.27650v1",
    "title": "Imbalanced Classification through the Lens of Spurious Correlations",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG",
      "cs.CV",
      "CV"
    ],
    "authors": [
      "Jakob Hackstein",
      "Sidney Bender"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27650v1",
    "abstract": "Class imbalance poses a fundamental challenge in machine learning, frequently leading to unreliable classification performance. While prior methods focus on data- or loss-reweighting schemes, we view imbalance as a data condition that amplifies Clever Hans (CH) effects by underspecification of minority classes. In a counterfactual explanations-based approach, we propose to leverage Explainable AI to jointly identify and eliminate CH effects emerging under imbalance. Our method achieves competitive classification performance on three datasets and demonstrates how CH effects emerge under imbalance, a perspective largely overlooked by existing approaches.",
    "fetched_at": "2025-11-05T02:19:05.050311Z"
  },
  {
    "id": "2510.27651v1",
    "title": "Information-Theoretic Greedy Layer-wise Training for Traffic Sign   Recognition",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Shuyan Lyu",
      "Zhanzimo Wu",
      "Junliang Du"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27651v1",
    "abstract": "Modern deep neural networks (DNNs) are typically trained with a global cross-entropy loss in a supervised end-to-end manner: neurons need to store their outgoing weights; training alternates between a forward pass (computation) and a top-down backward pass (learning) which is biologically implausible. Alternatively, greedy layer-wise training eliminates the need for cross-entropy loss and backpropagation. By avoiding the computation of intermediate gradients and the storage of intermediate outputs, it reduces memory usage and helps mitigate issues such as vanishing or exploding gradients. However, most existing layer-wise training approaches have been evaluated only on relatively small datasets with simple deep architectures. In this paper, we first systematically analyze the training dynamics of popular convolutional neural networks (CNNs) trained by stochastic gradient descent (SGD) through an information-theoretic lens. Our findings reveal that networks converge layer-by-layer from bottom to top and that the flow of information adheres to a Markov information bottleneck principle. Building on these observations, we propose a novel layer-wise training approach based on the recently developed deterministic information bottleneck (DIB) and the matrix-based R\\'enyi's $\\alpha$-order entropy functional. Specifically, each layer is trained jointly with an auxiliary classifier that connects directly to the output layer, enabling the learning of minimal sufficient task-relevant representations. We empirically validate the effectiveness of our training procedure on CIFAR-10 and CIFAR-100 using modern deep CNNs and further demonstrate its applicability to a practical task involving traffic sign recognition. Our approach not only outperforms existing layer-wise training baselines but also achieves performance comparable to SGD.",
    "fetched_at": "2025-11-05T02:19:05.050276Z"
  },
  {
    "id": "2510.27655v1",
    "title": "Community Detection on Model Explanation Graphs for Explainable AI",
    "date": "2025-10-31",
    "tags": [
      "cs.SI",
      "SI",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Ehsan Moradi"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27655v1",
    "abstract": "Feature-attribution methods (e.g., SHAP, LIME) explain individual predictions but often miss higher-order structure: sets of features that act in concert. We propose Modules of Influence (MoI), a framework that (i) constructs a model explanation graph from per-instance attributions, (ii) applies community detection to find feature modules that jointly affect predictions, and (iii) quantifies how these modules relate to bias, redundancy, and causality patterns. Across synthetic and real datasets, MoI uncovers correlated feature groups, improves model debugging via module-level ablations, and localizes bias exposure to specific modules. We release stability and synergy metrics, a reference implementation, and evaluation protocols to benchmark module discovery in XAI.",
    "fetched_at": "2025-11-05T02:19:05.050225Z"
  },
  {
    "id": "2510.27663v1",
    "title": "Bayesian model selection and misspecification testing in imaging inverse   problems only from noisy and partial measurements",
    "date": "2025-10-31",
    "tags": [
      "eess.IV",
      "IV",
      "cs.LG",
      "LG",
      "stat.ME",
      "ME",
      "stat.ML",
      "ML"
    ],
    "authors": [
      "Tom Sprunck",
      "Marcelo Pereyra",
      "Tobias Liaudat"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27663v1",
    "abstract": "Modern imaging techniques heavily rely on Bayesian statistical models to address difficult image reconstruction and restoration tasks. This paper addresses the objective evaluation of such models in settings where ground truth is unavailable, with a focus on model selection and misspecification diagnosis. Existing unsupervised model evaluation methods are often unsuitable for computational imaging due to their high computational cost and incompatibility with modern image priors defined implicitly via machine learning models. We herein propose a general methodology for unsupervised model selection and misspecification detection in Bayesian imaging sciences, based on a novel combination of Bayesian cross-validation and data fission, a randomized measurement splitting technique. The approach is compatible with any Bayesian imaging sampler, including diffusion and plug-and-play samplers. We demonstrate the methodology through experiments involving various scoring rules and types of model misspecification, where we achieve excellent selection and detection accuracy with a low computational cost.",
    "fetched_at": "2025-11-05T02:19:05.050143Z"
  },
  {
    "id": "2510.27671v1",
    "title": "MolChord: Structure-Sequence Alignment for Protein-Guided Drug Design",
    "date": "2025-10-31",
    "tags": [
      "cs.AI",
      "AI",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Wei Zhang",
      "Zekun Guo",
      "Yingce Xia",
      "Peiran Jin",
      "Shufang Xie",
      "Tao Qin",
      "Xiang-Yang Li"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27671v1",
    "abstract": "Structure-based drug design (SBDD), which maps target proteins to candidate molecular ligands, is a fundamental task in drug discovery. Effectively aligning protein structural representations with molecular representations, and ensuring alignment between generated drugs and their pharmacological properties, remains a critical challenge. To address these challenges, we propose MolChord, which integrates two key techniques: (1) to align protein and molecule structures with their textual descriptions and sequential representations (e.g., FASTA for proteins and SMILES for molecules), we leverage NatureLM, an autoregressive model unifying text, small molecules, and proteins, as the molecule generator, alongside a diffusion-based structure encoder; and (2) to guide molecules toward desired properties, we curate a property-aware dataset by integrating preference data and refine the alignment process using Direct Preference Optimization (DPO). Experimental results on CrossDocked2020 demonstrate that our approach achieves state-of-the-art performance on key evaluation metrics, highlighting its potential as a practical tool for SBDD.",
    "fetched_at": "2025-11-05T02:19:05.050097Z"
  },
  {
    "id": "2510.27672v1",
    "title": "Culture Cartography: Mapping the Landscape of Cultural Knowledge",
    "date": "2025-10-31",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Caleb Ziems",
      "William Held",
      "Jane Yu",
      "Amir Goldberg",
      "David Grusky",
      "Diyi Yang"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27672v1",
    "abstract": "To serve global users safely and productively, LLMs need culture-specific knowledge that might not be learned during pre-training. How do we find such knowledge that is (1) salient to in-group users, but (2) unknown to LLMs? The most common solutions are single-initiative: either researchers define challenging questions that users passively answer (traditional annotation), or users actively produce data that researchers structure as benchmarks (knowledge extraction). The process would benefit from mixed-initiative collaboration, where users guide the process to meaningfully reflect their cultures, and LLMs steer the process towards more challenging questions that meet the researcher's goals. We propose a mixed-initiative methodology called CultureCartography. Here, an LLM initializes annotation with questions for which it has low-confidence answers, making explicit both its prior knowledge and the gaps therein. This allows a human respondent to fill these gaps and steer the model towards salient topics through direct edits. We implement this methodology as a tool called CultureExplorer. Compared to a baseline where humans answer LLM-proposed questions, we find that CultureExplorer more effectively produces knowledge that leading models like DeepSeek R1 and GPT-4o are missing, even with web search. Fine-tuning on this data boosts the accuracy of Llama-3.1-8B by up to 19.2% on related culture benchmarks.",
    "fetched_at": "2025-11-05T02:19:05.050037Z"
  },
  {
    "id": "2510.27675v1",
    "title": "On Selecting Few-Shot Examples for LLM-based Code Vulnerability   Detection",
    "date": "2025-10-31",
    "tags": [
      "cs.SE",
      "SE",
      "cs.CR",
      "CR",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Md Abdul Hannan",
      "Ronghao Ni",
      "Chi Zhang",
      "Limin Jia",
      "Ravi Mangal",
      "Corina S. Pasareanu"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27675v1",
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities for many coding tasks, including summarization, translation, completion, and code generation. However, detecting code vulnerabilities remains a challenging task for LLMs. An effective way to improve LLM performance is in-context learning (ICL) - providing few-shot examples similar to the query, along with correct answers, can improve an LLM's ability to generate correct solutions. However, choosing the few-shot examples appropriately is crucial to improving model performance. In this paper, we explore two criteria for choosing few-shot examples for ICL used in the code vulnerability detection task. The first criterion considers if the LLM (consistently) makes a mistake or not on a sample with the intuition that LLM performance on a sample is informative about its usefulness as a few-shot example. The other criterion considers similarity of the examples with the program under query and chooses few-shot examples based on the $k$-nearest neighbors to the given sample. We perform evaluations to determine the benefits of these criteria individually as well as under various combinations, using open-source models on multiple datasets.",
    "fetched_at": "2025-11-05T02:19:05.049965Z"
  },
  {
    "id": "2510.27679v1",
    "title": "Dark-Field X-Ray Imaging Significantly Improves Deep-Learning based   Detection of Synthetic Early-Stage Lung Tumors in Preclinical Models",
    "date": "2025-10-31",
    "tags": [
      "physics.med-ph",
      "med-ph",
      "cs.CV",
      "CV",
      "cs.LG",
      "LG",
      "eess.IV",
      "IV",
      "physics.optics",
      "optics"
    ],
    "authors": [
      "Joyoni Dey",
      "Hunter C. Meyer",
      "Murtuza S. Taqi"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27679v1",
    "abstract": "Low-dose computed tomography (LDCT) is the current standard for lung cancer screening, yet its adoption and accessibility remain limited. Many regions lack LDCT infrastructure, and even among those screened, early-stage cancer detection often yield false positives, as shown in the National Lung Screening Trial (NLST) with a sensitivity of 93.8 percent and a false-positive rate of 26.6 percent. We aim to investigate whether X-ray dark-field imaging (DFI) radiograph, a technique sensitive to small-angle scatter from alveolar microstructure and less susceptible to organ shadowing, can significantly improve early-stage lung tumor detection when coupled with deep-learning segmentation. Using paired attenuation (ATTN) and DFI radiograph images of euthanized mouse lungs, we generated realistic synthetic tumors with irregular boundaries and intensity profiles consistent with physical lung contrast. A U-Net segmentation network was trained on small patches using either ATTN, DFI, or a combination of ATTN and DFI channels. Results show that the DFI-only model achieved a true-positive detection rate of 83.7 percent, compared with 51 percent for ATTN-only, while maintaining comparable specificity (90.5 versus 92.9 percent). The combined ATTN and DFI input achieved 79.6 percent sensitivity and 97.6 percent specificity. In conclusion, DFI substantially improves early-tumor detectability in comparison to standard attenuation radiography and shows potential as an accessible, low-cost, low-dose alternative for pre-clinical or limited-resource screening where LDCT is unavailable.",
    "fetched_at": "2025-11-05T02:19:05.049905Z"
  },
  {
    "id": "2510.27680v1",
    "title": "PETAR: Localized Findings Generation with Mask-Aware Vision-Language   Modeling for PET Automated Reporting",
    "date": "2025-10-31",
    "tags": [
      "cs.CV",
      "CV",
      "cs.AI",
      "AI",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Danyal Maqbool",
      "Changhee Lee",
      "Zachary Huemann",
      "Samuel D. Church",
      "Matthew E. Larson",
      "Scott B. Perlman",
      "Tomas A. Romero",
      "Joshua D. Warner",
      "Meghan Lubner",
      "Xin Tie",
      "Jameson Merkow",
      "Junjie Hu",
      "Steve Y. Cho",
      "Tyler J. Bradshaw"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27680v1",
    "abstract": "Recent advances in vision-language models (VLMs) have enabled impressive multimodal reasoning, yet most medical applications remain limited to 2D imaging. In this work, we extend VLMs to 3D positron emission tomography and computed tomography (PET/CT), a domain characterized by large volumetric data, small and dispersed lesions, and lengthy radiology reports. We introduce a large-scale dataset comprising over 11,000 lesion-level descriptions paired with 3D segmentations from more than 5,000 PET/CT exams, extracted via a hybrid rule-based and large language model (LLM) pipeline. Building upon this dataset, we propose PETAR-4B, a 3D mask-aware vision-language model that integrates PET, CT, and lesion contours for spatially grounded report generation. PETAR bridges global contextual reasoning with fine-grained lesion awareness, producing clinically coherent and localized findings. Comprehensive automated and human evaluations demonstrate that PETAR substantially improves PET/CT report generation quality, advancing 3D medical vision-language understanding.",
    "fetched_at": "2025-11-05T02:19:05.049849Z"
  },
  {
    "id": "2510.27688v1",
    "title": "Continuous Autoregressive Language Models",
    "date": "2025-10-31",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Chenze Shao",
      "Darren Li",
      "Fandong Meng",
      "Jie Zhou"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27688v1",
    "abstract": "The efficiency of large language models (LLMs) is fundamentally limited by their sequential, token-by-token generation process. We argue that overcoming this bottleneck requires a new design axis for LLM scaling: increasing the semantic bandwidth of each generative step. To this end, we introduce Continuous Autoregressive Language Models (CALM), a paradigm shift from discrete next-token prediction to continuous next-vector prediction. CALM uses a high-fidelity autoencoder to compress a chunk of K tokens into a single continuous vector, from which the original tokens can be reconstructed with over 99.9\\% accuracy. This allows us to model language as a sequence of continuous vectors instead of discrete tokens, which reduces the number of generative steps by a factor of K. The paradigm shift necessitates a new modeling toolkit; therefore, we develop a comprehensive likelihood-free framework that enables robust training, evaluation, and controllable sampling in the continuous domain. Experiments show that CALM significantly improves the performance-compute trade-off, achieving the performance of strong discrete baselines at a significantly lower computational cost. More importantly, these findings establish next-vector prediction as a powerful and scalable pathway towards ultra-efficient language models. Code: https://github.com/shaochenze/calm. Project: https://shaochenze.github.io/blog/2025/CALM.",
    "fetched_at": "2025-11-05T02:19:05.049752Z"
  },
  {
    "id": "2510.27069v1",
    "title": "Distributed Precoding for Cell-free Massive MIMO in O-RAN: A Multi-agent   Deep Reinforcement Learning Framework",
    "date": "2025-10-31",
    "tags": [
      "eess.SP",
      "SP"
    ],
    "authors": [
      "Mohammad Hossein Shokouhi",
      "Vincent W. S. Wong"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27069v1",
    "abstract": "Cell-free massive multiple-input multiple-output (MIMO) is a key technology for next-generation wireless systems. The integration of cell-free massive MIMO within the open radio access network (O-RAN) architecture addresses the growing need for decentralized, scalable, and high-capacity networks that can support different use cases. Precoding is a crucial step in the operation of cell-free massive MIMO, where O-RUs steer their beams towards the intended users while mitigating interference to other users. Current precoding schemes for cell-free massive MIMO are either fully centralized or fully distributed. Centralized schemes are not scalable, whereas distributed schemes may lead to a high inter-O-RU interference. In this paper, we propose a distributed and scalable precoding framework for cell-free massive MIMO that uses limited information exchange among precoding agents to mitigate interference. We formulate an optimization problem for precoding that maximizes the aggregate throughput while guaranteeing the minimum data rate requirements of users. The formulated problem is nonconvex. We propose a multi-timescale framework that combines multi-agent deep reinforcement learning (DRL) with expert insights from an iterative algorithm to determine the precoding matrices efficiently. We conduct simulations and compare the proposed framework with the centralized precoding and distributed precoding methods for different numbers of O-RUs, users, and transmit antennas. The results show that the proposed framework achieves a higher aggregate throughput than the distributed regularized zero-forcing (D-RZF) scheme and the weighted minimum mean square error (WMMSE) algorithm. When compared with the centralized regularized zero-forcing (C-RZF) scheme, the proposed framework achieves similar aggregate throughput performance but with a lower signaling overhead.",
    "fetched_at": "2025-11-05T02:19:02.082591Z"
  },
  {
    "id": "2510.27266v1",
    "title": "HyperClick: Advancing Reliable GUI Grounding via Uncertainty Calibration",
    "date": "2025-10-31",
    "tags": [
      "cs.CV",
      "CV"
    ],
    "authors": [
      "Shaojie Zhang",
      "Pei Fu",
      "Ruoceng Zhang",
      "Jiahui Yang",
      "Anan Du",
      "Xiuwen Xi",
      "Shaokang Wang",
      "Ying Huang",
      "Bin Qin",
      "Zhenbo Luo",
      "Jian Luan"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27266v1",
    "abstract": "Autonomous Graphical User Interface (GUI) agents rely on accurate GUI grounding, which maps language instructions to on-screen coordinates, to execute user commands. However, current models, whether trained via supervised fine-tuning (SFT) or reinforcement fine-tuning (RFT), lack self-awareness of their capability boundaries, leading to overconfidence and unreliable predictions. We first systematically evaluate probabilistic and verbalized confidence in general and GUI-specific models, revealing a misalignment between confidence and actual accuracy, which is particularly critical in dynamic GUI automation tasks, where single errors can cause task failure. To address this, we propose HyperClick, a novel framework that enhances reliable GUI grounding through uncertainty calibration. HyperClick introduces a dual reward mechanism, combining a binary reward for correct actions with a truncated Gaussian-based spatial confidence modeling, calibrated using the Brier score. This approach jointly optimizes grounding accuracy and confidence reliability, fostering introspective self-criticism. Extensive experiments on seven challenge benchmarks show that HyperClick achieves state-of-the-art performance while providing well-calibrated confidence. By enabling explicit confidence calibration and introspective self-criticism, HyperClick reduces overconfidence and supports more reliable GUI automation.",
    "fetched_at": "2025-11-05T02:19:02.082263Z"
  },
  {
    "id": "2510.27289v1",
    "title": "A Digital Twin-based Multi-Agent Reinforcement Learning Framework for   Vehicle-to-Grid Coordination",
    "date": "2025-10-31",
    "tags": [
      "cs.DC",
      "DC"
    ],
    "authors": [
      "Zhengchang Hua",
      "Panagiotis Oikonomou",
      "Karim Djemame",
      "Nikos Tziritas",
      "Georgios Theodoropoulos"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27289v1",
    "abstract": "The coordination of large-scale, decentralised systems, such as a fleet of Electric Vehicles (EVs) in a Vehicle-to-Grid (V2G) network, presents a significant challenge for modern control systems. While collaborative Digital Twins have been proposed as a solution to manage such systems without compromising the privacy of individual agents, deriving globally optimal control policies from the high-level information they share remains an open problem. This paper introduces Digital Twin Assisted Multi-Agent Deep Deterministic Policy Gradient (DT-MADDPG) algorithm, a novel hybrid architecture that integrates a multi-agent reinforcement learning framework with a collaborative DT network. Our core contribution is a simulation-assisted learning algorithm where the centralised critic is enhanced by a predictive global model that is collaboratively built from the privacy-preserving data shared by individual DTs. This approach removes the need for collecting sensitive raw data at a centralised entity, a requirement of traditional multi-agent learning algorithms. Experimental results in a simulated V2G environment demonstrate that DT-MADDPG can achieve coordination performance comparable to the standard MADDPG algorithm while offering significant advantages in terms of data privacy and architectural decentralisation. This work presents a practical and robust framework for deploying intelligent, learning-based coordination in complex, real-world cyber-physical systems.",
    "fetched_at": "2025-11-05T02:19:02.082191Z"
  },
  {
    "id": "2510.27329v1",
    "title": "Reinforcement Learning for Long-Horizon Unordered Tasks: From Boolean to   Coupled Reward Machines",
    "date": "2025-10-31",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Kristina Levina",
      "Nikolaos Pappas",
      "Athanasios Karapantelakis",
      "Aneta Vulgarakis Feljan",
      "Jendrik Seipp"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27329v1",
    "abstract": "Reward machines (RMs) inform reinforcement learning agents about the reward structure of the environment. This is particularly advantageous for complex non-Markovian tasks because agents with access to RMs can learn more efficiently from fewer samples. However, learning with RMs is ill-suited for long-horizon problems in which a set of subtasks can be executed in any order. In such cases, the amount of information to learn increases exponentially with the number of unordered subtasks. In this work, we address this limitation by introducing three generalisations of RMs: (1) Numeric RMs allow users to express complex tasks in a compact form. (2) In Agenda RMs, states are associated with an agenda that tracks the remaining subtasks to complete. (3) Coupled RMs have coupled states associated with each subtask in the agenda. Furthermore, we introduce a new compositional learning algorithm that leverages coupled RMs: Q-learning with coupled RMs (CoRM). Our experiments show that CoRM scales better than state-of-the-art RM algorithms for long-horizon problems with unordered subtasks.",
    "fetched_at": "2025-11-05T02:19:02.082137Z"
  },
  {
    "id": "2510.27334v1",
    "title": "When AI Trading Agents Compete: Adverse Selection of Meta-Orders by   Reinforcement Learning-Based Market Making",
    "date": "2025-10-31",
    "tags": [
      "q-fin.TR",
      "TR",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Ali Raza Jafree",
      "Konark Jain",
      "Nick Firoozye"
    ],
    "institution": "Meta, MIT",
    "link": "http://arxiv.org/pdf/2510.27334v1",
    "abstract": "We investigate the mechanisms by which medium-frequency trading agents are adversely selected by opportunistic high-frequency traders. We use reinforcement learning (RL) within a Hawkes Limit Order Book (LOB) model in order to replicate the behaviours of high-frequency market makers. In contrast to the classical models with exogenous price impact assumptions, the Hawkes model accounts for endogenous price impact and other key properties of the market (Jain et al. 2024a). Given the real-world impracticalities of the market maker updating strategies for every event in the LOB, we formulate the high-frequency market making agent via an impulse control reinforcement learning framework (Jain et al. 2025). The RL used in the simulation utilises Proximal Policy Optimisation (PPO) and self-imitation learning. To replicate the adverse selection phenomenon, we test the RL agent trading against a medium frequency trader (MFT) executing a meta-order and demonstrate that, with training against the MFT meta-order execution agent, the RL market making agent learns to capitalise on the price drift induced by the meta-order. Recent empirical studies have shown that medium-frequency traders are increasingly subject to adverse selection by high-frequency trading agents. As high-frequency trading continues to proliferate across financial markets, the slippage costs incurred by medium-frequency traders are likely to increase over time. However, we do not observe that increased profits for the market making RL agent necessarily cause significantly increased slippages for the MFT agent.",
    "fetched_at": "2025-11-05T02:19:02.082084Z"
  },
  {
    "id": "2510.27383v1",
    "title": "Realistic pedestrian-driver interaction modelling using multi-agent RL   with human perceptual-motor constraints",
    "date": "2025-10-31",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Yueyang Wang",
      "Mehmet Dogar",
      "Gustav Markkula"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27383v1",
    "abstract": "Modelling pedestrian-driver interactions is critical for understanding human road user behaviour and developing safe autonomous vehicle systems. Existing approaches often rely on rule-based logic, game-theoretic models, or 'black-box' machine learning methods. However, these models typically lack flexibility or overlook the underlying mechanisms, such as sensory and motor constraints, which shape how pedestrians and drivers perceive and act in interactive scenarios. In this study, we propose a multi-agent reinforcement learning (RL) framework that integrates both visual and motor constraints of pedestrian and driver agents. Using a real-world dataset from an unsignalised pedestrian crossing, we evaluate four model variants, one without constraints, two with either motor or visual constraints, and one with both, across behavioural metrics of interaction realism. Results show that the combined model with both visual and motor constraints performs best. Motor constraints lead to smoother movements that resemble human speed adjustments during crossing interactions. The addition of visual constraints introduces perceptual uncertainty and field-of-view limitations, leading the agents to exhibit more cautious and variable behaviour, such as less abrupt deceleration. In this data-limited setting, our model outperforms a supervised behavioural cloning model, demonstrating that our approach can be effective without large training datasets. Finally, our framework accounts for individual differences by modelling parameters controlling the human constraints as population-level distributions, a perspective that has not been explored in previous work on pedestrian-vehicle interaction modelling. Overall, our work demonstrates that multi-agent RL with human constraints is a promising modelling approach for simulating realistic road user interactions.",
    "fetched_at": "2025-11-05T02:19:02.081989Z"
  },
  {
    "id": "2510.27410v1",
    "title": "Dialogue as Discovery: Navigating Human Intent Through Principled   Inquiry",
    "date": "2025-10-31",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Jianwen Sun",
      "Yukang Feng",
      "Yifan Chang",
      "Chuanhao Li",
      "Zizhen Li",
      "Jiaxin Ai",
      "Fanrui Zhang",
      "Yu Dai",
      "Kaipeng Zhang"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27410v1",
    "abstract": "A fundamental bottleneck in human-AI collaboration is the \"intention expression gap,\" the difficulty for humans to effectively convey complex, high-dimensional thoughts to AI. This challenge often traps users in inefficient trial-and-error loops and is exacerbated by the diverse expertise levels of users. We reframe this problem from passive instruction following to a Socratic collaboration paradigm, proposing an agent that actively probes for information to resolve its uncertainty about user intent. we name the proposed agent Nous, trained to acquire proficiency in this inquiry policy. The core mechanism of Nous is a training framework grounded in the first principles of information theory. Within this framework, we define the information gain from dialogue as an intrinsic reward signal, which is fundamentally equivalent to the reduction of Shannon entropy over a structured task space. This reward design enables us to avoid reliance on costly human preference annotations or external reward models. To validate our framework, we develop an automated simulation pipeline to generate a large-scale, preference-based dataset for the challenging task of scientific diagram generation. Comprehensive experiments, including ablations, subjective and objective evaluations, and tests across user expertise levels, demonstrate the effectiveness of our proposed framework. Nous achieves leading efficiency and output quality, while remaining robust to varying user expertise. Moreover, its design is domain-agnostic, and we show evidence of generalization beyond diagram generation. Experimental results prove that our work offers a principled, scalable, and adaptive paradigm for resolving uncertainty about user intent in complex human-AI collaboration.",
    "fetched_at": "2025-11-05T02:19:02.081922Z"
  },
  {
    "id": "2510.27420v1",
    "title": "Towards a Multi-Embodied Grasping Agent",
    "date": "2025-10-31",
    "tags": [
      "cs.RO",
      "RO",
      "I.2.9",
      "9"
    ],
    "authors": [
      "Roman Freiberg",
      "Alexander Qualmann",
      "Ngo Anh Vien",
      "Gerhard Neumann"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27420v1",
    "abstract": "Multi-embodiment grasping focuses on developing approaches that exhibit generalist behavior across diverse gripper designs. Existing methods often learn the kinematic structure of the robot implicitly and face challenges due to the difficulty of sourcing the required large-scale data. In this work, we present a data-efficient, flow-based, equivariant grasp synthesis architecture that can handle different gripper types with variable degrees of freedom and successfully exploit the underlying kinematic model, deducing all necessary information solely from the gripper and scene geometry. Unlike previous equivariant grasping methods, we translated all modules from the ground up to JAX and provide a model with batching capabilities over scenes, grippers, and grasps, resulting in smoother learning, improved performance and faster inference time. Our dataset encompasses grippers ranging from humanoid hands to parallel yaw grippers and includes 25,000 scenes and 20 million grasps.",
    "fetched_at": "2025-11-05T02:19:02.081813Z"
  },
  {
    "id": "2510.27659v1",
    "title": "Challenges in Credit Assignment for Multi-Agent Reinforcement Learning   in Open Agent Systems",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI",
      "cs.MA",
      "MA"
    ],
    "authors": [
      "Alireza Saleh Abadi",
      "Leen-Kiat Soh"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27659v1",
    "abstract": "In the rapidly evolving field of multi-agent reinforcement learning (MARL), understanding the dynamics of open systems is crucial. Openness in MARL refers to the dynam-ic nature of agent populations, tasks, and agent types with-in a system. Specifically, there are three types of openness as reported in (Eck et al. 2023) [2]: agent openness, where agents can enter or leave the system at any time; task openness, where new tasks emerge, and existing ones evolve or disappear; and type openness, where the capabil-ities and behaviors of agents change over time. This report provides a conceptual and empirical review, focusing on the interplay between openness and the credit assignment problem (CAP). CAP involves determining the contribution of individual agents to the overall system performance, a task that becomes increasingly complex in open environ-ments. Traditional credit assignment (CA) methods often assume static agent populations, fixed and pre-defined tasks, and stationary types, making them inadequate for open systems. We first conduct a conceptual analysis, in-troducing new sub-categories of openness to detail how events like agent turnover or task cancellation break the assumptions of environmental stationarity and fixed team composition that underpin existing CAP methods. We then present an empirical study using representative temporal and structural algorithms in an open environment. The results demonstrate that openness directly causes credit misattribution, evidenced by unstable loss functions and significant performance degradation.",
    "fetched_at": "2025-11-05T02:19:02.081345Z"
  },
  {
    "id": "2510.27094v1",
    "title": "CombiGraph-Vis: A Curated Multimodal Olympiad Benchmark for Discrete   Mathematical Reasoning",
    "date": "2025-10-31",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Hamed Mahdavi",
      "Pouria Mahdavinia",
      "Alireza Farhadi",
      "Pegah Mohammadipour",
      "Samira Malek",
      "Majid Daliri",
      "Pedram Mohammadipour",
      "Alireza Hashemi",
      "Amir Khasahmadi",
      "Vasant Honavar"
    ],
    "institution": "Amirkabir University of Technology, Autodesk, City University of New York, New York University, Pennsylvania State University",
    "link": "http://arxiv.org/pdf/2510.27094v1",
    "abstract": "State-of-the-art (SOTA) LLMs have progressed from struggling on proof-based Olympiad problems to solving most of the IMO 2025 problems, with leading systems reportedly handling 5 of 6 problems. Given this progress, we assess how well these models can grade proofs: detecting errors, judging their severity, and assigning fair scores beyond binary correctness. We study proof-analysis capabilities using a corpus of 90 Gemini 2.5 Pro-generated solutions that we grade on a 1-4 scale with detailed error annotations, and on MathArena solution sets for IMO/USAMO 2025 scored on a 0-7 scale. Our analysis shows that models can reliably flag incorrect (including subtly incorrect) solutions but exhibit calibration gaps in how partial credit is assigned. To address this, we introduce agentic workflows that extract and analyze reference solutions and automatically derive problem-specific rubrics for a multi-step grading process. We instantiate and compare different design choices for the grading workflows, and evaluate their trade-offs. Across our annotated corpus and MathArena, our proposed workflows achieve higher agreement with human grades and more consistent handling of partial credit across metrics. We release all code, data, and prompts/logs to facilitate future research.",
    "fetched_at": "2025-11-05T02:19:00.095742Z"
  },
  {
    "id": "2510.27107v1",
    "title": "A Memory-Efficient Retrieval Architecture for RAG-Enabled Wearable   Medical LLMs-Agents",
    "date": "2025-10-31",
    "tags": [
      "cs.AR",
      "AR"
    ],
    "authors": [
      "Zhipeng Liao",
      "Kunming Shao",
      "Jiangnan Yu",
      "Liang Zhao",
      "Tim Kwang-Ting Cheng",
      "Chi-Ying Tsui",
      "Jie Yang",
      "Mohamad Sawan"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27107v1",
    "abstract": "With powerful and integrative large language models (LLMs), medical AI agents have demonstrated unique advantages in providing personalized medical consultations, continuous health monitoring, and precise treatment plans. Retrieval-Augmented Generation (RAG) integrates personal medical documents into LLMs by an external retrievable database to address the costly retraining or fine-tuning issues in deploying customized agents. While deploying medical agents in edge devices ensures privacy protection, RAG implementations impose substantial memory access and energy consumption during the retrieval stage. This paper presents a hierarchical retrieval architecture for edge RAG, leveraging a two-stage retrieval scheme that combines approximate retrieval for candidate set generation, followed by high-precision retrieval on pre-selected document embeddings. The proposed architecture significantly reduces energy consumption and external memory access while maintaining retrieval accuracy. Simulation results show that, under TSMC 28nm technology, the proposed hierarchical retrieval architecture has reduced the overall memory access by nearly 50% and the computation by 75% compared to pure INT8 retrieval, and the total energy consumption for 1 MB data retrieval is 177.76 {\\mu}J/query.",
    "fetched_at": "2025-11-05T02:19:00.095680Z"
  },
  {
    "id": "2510.27130v1",
    "title": "AI Agents in Drug Discovery",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Srijit Seal",
      "Dinh Long Huynh",
      "Moudather Chelbi",
      "Sara Khosravi",
      "Ankur Kumar",
      "Mattson Thieme",
      "Isaac Wilks",
      "Mark Davies",
      "Jessica Mustali",
      "Yannick Sun",
      "Nick Edwards",
      "Daniil Boiko",
      "Andrei Tyrin",
      "Douglas W. Selinger",
      "Ayaan Parikh",
      "Rahul Vijayan",
      "Shoman Kasbekar",
      "Dylan Reid",
      "Andreas Bender",
      "Ola Spjuth"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27130v1",
    "abstract": "Artificial intelligence (AI) agents are emerging as transformative tools in drug discovery, with the ability to autonomously reason, act, and learn through complicated research workflows. Building on large language models (LLMs) coupled with perception, computation, action, and memory tools, these agentic AI systems could integrate diverse biomedical data, execute tasks, carry out experiments via robotic platforms, and iteratively refine hypotheses in closed loops. We provide a conceptual and technical overview of agentic AI architectures, ranging from ReAct and Reflection to Supervisor and Swarm systems, and illustrate their applications across key stages of drug discovery, including literature synthesis, toxicity prediction, automated protocol generation, small-molecule synthesis, drug repurposing, and end-to-end decision-making. To our knowledge, this represents the first comprehensive work to present real-world implementations and quantifiable impacts of agentic AI systems deployed in operational drug discovery settings. Early implementations demonstrate substantial gains in speed, reproducibility, and scalability, compressing workflows that once took months into hours while maintaining scientific traceability. We discuss the current challenges related to data heterogeneity, system reliability, privacy, and benchmarking, and outline future directions towards technology in support of science and translation.",
    "fetched_at": "2025-11-05T02:19:00.095618Z"
  },
  {
    "id": "2510.27140v1",
    "title": "Measuring the Security of Mobile LLM Agents under Adversarial Prompts   from Untrusted Third-Party Channels",
    "date": "2025-10-31",
    "tags": [
      "cs.CR",
      "CR"
    ],
    "authors": [
      "Chenghao Du",
      "Quanfeng Huang",
      "Tingxuan Tang",
      "Zihao Wang",
      "Yue Xiao"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27140v1",
    "abstract": "Large Language Models (LLMs) have transformed software development, enabling AI-powered applications known as LLM-based agents that promise to automate tasks across diverse apps and workflows. Yet, the security implications of deploying such agents in adversarial mobile environments remain poorly understood. In this paper, we present the first systematic study of security risks in mobile LLM agents. We design and evaluate a suite of adversarial case studies, ranging from opportunistic manipulations such as pop-up advertisements to advanced, end-to-end workflows involving malware installation and cross-app data exfiltration. Our evaluation covers eight state-of-the-art mobile agents across three architectures, with over 2,000 adversarial and paired benign trials. The results reveal systemic vulnerabilities: low-barrier vectors such as fraudulent ads succeed with over 80% reliability, while even workflows requiring the circumvention of operating-system warnings, such as malware installation, are consistently completed by advanced multi-app agents. By mapping these attacks to the MITRE ATT&CK Mobile framework, we uncover novel privilege-escalation and persistence pathways unique to LLM-driven automation. Collectively, our findings provide the first end-to-end evidence that mobile LLM agents are exploitable in realistic adversarial settings, where untrusted third-party channels (e.g., ads, embedded webviews, cross-app notifications) are an inherent part of the mobile ecosystem.",
    "fetched_at": "2025-11-05T02:19:00.095516Z"
  },
  {
    "id": "2510.27157v1",
    "title": "A Survey on Generative Recommendation: Data, Model, and Tasks",
    "date": "2025-10-31",
    "tags": [
      "cs.IR",
      "IR"
    ],
    "authors": [
      "Min Hou",
      "Le Wu",
      "Yuxin Liao",
      "Yonghui Yang",
      "Zhen Zhang",
      "Changlong Zheng",
      "Han Wu",
      "Richang Hong"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27157v1",
    "abstract": "Recommender systems serve as foundational infrastructure in modern information ecosystems, helping users navigate digital content and discover items aligned with their preferences. At their core, recommender systems address a fundamental problem: matching users with items. Over the past decades, the field has experienced successive paradigm shifts, from collaborative filtering and matrix factorization in the machine learning era to neural architectures in the deep learning era. Recently, the emergence of generative models, especially large language models (LLMs) and diffusion models, have sparked a new paradigm: generative recommendation, which reconceptualizes recommendation as a generation task rather than discriminative scoring. This survey provides a comprehensive examination through a unified tripartite framework spanning data, model, and task dimensions. Rather than simply categorizing works, we systematically decompose approaches into operational stages-data augmentation and unification, model alignment and training, task formulation and execution. At the data level, generative models enable knowledge-infused augmentation and agent-based simulation while unifying heterogeneous signals. At the model level, we taxonomize LLM-based methods, large recommendation models, and diffusion approaches, analyzing their alignment mechanisms and innovations. At the task level, we illuminate new capabilities including conversational interaction, explainable reasoning, and personalized content generation. We identify five key advantages: world knowledge integration, natural language understanding, reasoning capabilities, scaling laws, and creative generation. We critically examine challenges in benchmark design, model robustness, and deployment efficiency, while charting a roadmap toward intelligent recommendation assistants that fundamentally reshape human-information interaction.",
    "fetched_at": "2025-11-05T02:19:00.095461Z"
  },
  {
    "id": "2510.27176v1",
    "title": "Glia: A Human-Inspired AI for Automated Systems Design and Optimization",
    "date": "2025-10-31",
    "tags": [
      "cs.AI",
      "AI",
      "cs.CL",
      "CL",
      "cs.DC",
      "DC"
    ],
    "authors": [
      "Pouya Hamadanian",
      "Pantea Karimi",
      "Arash Nasr-Esfahany",
      "Kimia Noorbakhsh",
      "Joseph Chandler",
      "Ali ParandehGheibi",
      "Mohammad Alizadeh",
      "Hari Balakrishnan"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27176v1",
    "abstract": "Can an AI autonomously design mechanisms for computer systems on par with the creativity and reasoning of human experts? We present Glia, an AI architecture for networked systems design that uses large language models (LLMs) in a human-inspired, multi-agent workflow. Each agent specializes in reasoning, experimentation, and analysis, collaborating through an evaluation framework that grounds abstract reasoning in empirical feedback. Unlike prior ML-for-systems methods that optimize black-box policies, Glia generates interpretable designs and exposes its reasoning process. When applied to a distributed GPU cluster for LLM inference, it produces new algorithms for request routing, scheduling, and auto-scaling that perform at human-expert levels in significantly less time, while yielding novel insights into workload behavior. Our results suggest that by combining reasoning LLMs with structured experimentation, an AI can produce creative and understandable designs for complex systems problems.",
    "fetched_at": "2025-11-05T02:19:00.095394Z"
  },
  {
    "id": "2510.27196v1",
    "title": "MemeArena: Automating Context-Aware Unbiased Evaluation of Harmfulness   Understanding for Multimodal Large Language Models",
    "date": "2025-10-31",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Zixin Chen",
      "Hongzhan Lin",
      "Kaixin Li",
      "Ziyang Luo",
      "Yayue Deng",
      "Jing Ma"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27196v1",
    "abstract": "The proliferation of memes on social media necessitates the capabilities of multimodal Large Language Models (mLLMs) to effectively understand multimodal harmfulness. Existing evaluation approaches predominantly focus on mLLMs' detection accuracy for binary classification tasks, which often fail to reflect the in-depth interpretive nuance of harmfulness across diverse contexts. In this paper, we propose MemeArena, an agent-based arena-style evaluation framework that provides a context-aware and unbiased assessment for mLLMs' understanding of multimodal harmfulness. Specifically, MemeArena simulates diverse interpretive contexts to formulate evaluation tasks that elicit perspective-specific analyses from mLLMs. By integrating varied viewpoints and reaching consensus among evaluators, it enables fair and unbiased comparisons of mLLMs' abilities to interpret multimodal harmfulness. Extensive experiments demonstrate that our framework effectively reduces the evaluation biases of judge agents, with judgment results closely aligning with human preferences, offering valuable insights into reliable and comprehensive mLLM evaluations in multimodal harmfulness understanding. Our code and data are publicly available at https://github.com/Lbotirx/MemeArena.",
    "fetched_at": "2025-11-05T02:19:00.095334Z"
  },
  {
    "id": "2510.27210v1",
    "title": "GUI-Rise: Structured Reasoning and History Summarization for GUI   Navigation",
    "date": "2025-10-31",
    "tags": [
      "cs.AI",
      "AI",
      "cs.CV",
      "CV"
    ],
    "authors": [
      "Tao Liu",
      "Chongyu Wang",
      "Rongjie Li",
      "Yingchen Yu",
      "Xuming He",
      "Bai Song"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27210v1",
    "abstract": "While Multimodal Large Language Models (MLLMs) have advanced GUI navigation agents, current approaches face limitations in cross-domain generalization and effective history utilization. We present a reasoning-enhanced framework that systematically integrates structured reasoning, action prediction, and history summarization. The structured reasoning component generates coherent Chain-of-Thought analyses combining progress estimation and decision reasoning, which inform both immediate action predictions and compact history summaries for future steps. Based on this framework, we train a GUI agent, \\textbf{GUI-Rise}, through supervised fine-tuning on pseudo-labeled trajectories and reinforcement learning with Group Relative Policy Optimization (GRPO). This framework employs specialized rewards, including a history-aware objective, directly linking summary quality to subsequent action performance. Comprehensive evaluations on standard benchmarks demonstrate state-of-the-art results under identical training data conditions, with particularly strong performance in out-of-domain scenarios. These findings validate our framework's ability to maintain robust reasoning and generalization across diverse GUI navigation tasks. Code is available at https://leon022.github.io/GUI-Rise.",
    "fetched_at": "2025-11-05T02:19:00.095278Z"
  },
  {
    "id": "2510.27251v1",
    "title": "FinPos: A Position-Aware Trading Agent System for Real Financial Markets",
    "date": "2025-10-31",
    "tags": [
      "cs.MA",
      "MA"
    ],
    "authors": [
      "Bijia Liu",
      "Ronghao Dang"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27251v1",
    "abstract": "The exceptional potential of large language models (LLMs) in handling text information has garnered significant attention in the field of financial trading. However, current trading agents primarily focus on single-step trading tasks and lack awareness of continuous position management. Therefore, we propose a position-aware trading task designed to simulate a more realistic market. To address this task, we develop a trading agent system, FinPos, optimized for position management. FinPos is able to interpret various types of market information from a professional perspective, providing a reliable basis for positioning decisions. To mitigate the substantial market risks arising from position fluctuations, FinPos employs dual decision agents. Furthermore, the continuous nature of position management necessitates our adoption of multi-timescale rewards, which in turn empowers FinPos to effectively balance short-term fluctuations against long-term trends. Extensive experiments demonstrate that FinPos surpasses state-of-the-art trading agents in the position-aware trading task, which closely mirrors real market conditions. More importantly, our findings reveal that LLM-centered agent systems exhibit a vast, largely unexplored potential in long-term market decision-making.",
    "fetched_at": "2025-11-05T02:19:00.095219Z"
  },
  {
    "id": "2510.27275v1",
    "title": "Prevalence of Security and Privacy Risk-Inducing Usage of AI-based   Conversational Agents",
    "date": "2025-10-31",
    "tags": [
      "cs.CR",
      "CR"
    ],
    "authors": [
      "Kathrin Grosse",
      "Nico Ebert"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27275v1",
    "abstract": "Recent improvement gains in large language models (LLMs) have lead to everyday usage of AI-based Conversational Agents (CAs). At the same time, LLMs are vulnerable to an array of threats, including jailbreaks and, for example, causing remote code execution when fed specific inputs. As a result, users may unintentionally introduce risks, for example, by uploading malicious files or disclosing sensitive information. However, the extent to which such user behaviors occur and thus potentially facilitate exploits remains largely unclear. To shed light on this issue, we surveyed a representative sample of 3,270 UK adults in 2024 using Prolific. A third of these use CA services such as ChatGPT or Gemini at least once a week. Of these ``regular users'', up to a third exhibited behaviors that may enable attacks, and a fourth have tried jailbreaking (often out of understandable reasons such as curiosity, fun or information seeking). Half state that they sanitize data and most participants report not sharing sensitive data. However, few share very sensitive data such as passwords. The majority are unaware that their data can be used to train models and that they can opt-out. Our findings suggest that current academic threat models manifest in the wild, and mitigations or guidelines for the secure usage of CAs should be developed. In areas critical to security and privacy, CAs must be equipped with effective AI guardrails to prevent, for example, revealing sensitive information to curious employees. Vendors need to increase efforts to prevent the entry of sensitive data, and to create transparency with regard to data usage policies and settings.",
    "fetched_at": "2025-11-05T02:19:00.095179Z"
  },
  {
    "id": "2510.27287v1",
    "title": "Can LLMs Help You at Work? A Sandbox for Evaluating LLM Agents in   Enterprise Environments",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Harsh Vishwakarma",
      "Ankush Agarwal",
      "Ojas Patil",
      "Chaitanya Devaguptapu",
      "Mahesh Chandran"
    ],
    "institution": "Meta",
    "link": "http://arxiv.org/pdf/2510.27287v1",
    "abstract": "Enterprise systems are crucial for enhancing productivity and decision-making among employees and customers. Integrating LLM based systems into enterprise systems enables intelligent automation, personalized experiences, and efficient information retrieval, driving operational efficiency and strategic growth. However, developing and evaluating such systems is challenging due to the inherent complexity of enterprise environments, where data is fragmented across multiple sources and governed by sophisticated access controls. We present EnterpriseBench, a comprehensive benchmark that simulates enterprise settings, featuring 500 diverse tasks across software engineering, HR, finance, and administrative domains. Our benchmark uniquely captures key enterprise characteristics including data source fragmentation, access control hierarchies, and cross-functional workflows. Additionally, we provide a novel data generation pipeline that creates internally consistent enterprise tasks from organizational metadata. Experiments with state-of-the-art LLM agents demonstrate that even the most capable models achieve only 41.8% task completion, highlighting significant opportunities for improvement in enterprise-focused AI systems.",
    "fetched_at": "2025-11-05T02:19:00.095134Z"
  },
  {
    "id": "2510.27363v1",
    "title": "ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool   Use",
    "date": "2025-10-31",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Mengjie Deng",
      "Guanting Dong",
      "Zhicheng Dou"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27363v1",
    "abstract": "Recently, large language models (LLMs) have demonstrated remarkable problem-solving capabilities by autonomously integrating with external tools for collaborative reasoning. However, due to the inherently complex and diverse nature of multimodal information, enabling multimodal large language models (MLLMs) to flexibly and efficiently utilize external tools during reasoning remains an underexplored challenge. In this work, we introduce ToolScope, an agentic framework designed to unify global planning with local multimodal perception, adopting a specialized Perceive tool to mitigates visual context degradation in long-horizon VQA task. ToolScope comprises three primary components: the Global Navigator, the Agentic Executor, and the Response Synthesizer. The Global Navigator functions as a \"telescope\", offering high-level strategic guidance. The Agentic Executor operates iteratively to augment MLLM with local perception through the integration of external tools-Search, Code, and Perceive. Finally, the Response Synthesizer consolidates and organizes the reasoning process into a coherent, user-friendly output. We evaluate ToolScope on four VQA benchmarks across diverse domains, including VQA 2.0, ScienceQA, MAT-Search and MathVista. It demonstrates strong generalization capabilities, achieving an average performance improvement of up to +6.69% across all datasets.",
    "fetched_at": "2025-11-05T02:19:00.095079Z"
  },
  {
    "id": "2510.27417v1",
    "title": "Agentic LLMs for REST API Test Amplification: A Comparative Study Across   Cloud Applications",
    "date": "2025-10-31",
    "tags": [
      "cs.SE",
      "SE"
    ],
    "authors": [
      "Jarne Besjes",
      "Robbe Nooyens",
      "Tolgahan Bardakci",
      "Mutlu Beyazit",
      "Serge Demeyer"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27417v1",
    "abstract": "Representational State Transfer (REST) APIs are a cornerstone of modern cloud native systems. Ensuring their reliability demands automated test suites that exercise diverse and boundary level behaviors. Nevertheless, designing such test cases remains a challenging and resource intensive endeavor. This study extends prior work on Large Language Model (LLM) based test amplification by evaluating single agent and multi agent configurations across four additional cloud applications. The amplified test suites maintain semantic validity with minimal human intervention. The results demonstrate that agentic LLM systems can effectively generalize across heterogeneous API architectures, increasing endpoint and parameter coverage while revealing defects. Moreover, a detailed analysis of computational cost, runtime, and energy consumption highlights trade-offs between accuracy, scalability, and efficiency. These findings underscore the potential of LLM driven test amplification to advance the automation and sustainability of REST API testing in complex cloud environments.",
    "fetched_at": "2025-11-05T02:19:00.095033Z"
  },
  {
    "id": "2510.27418v1",
    "title": "Dynamic Affective Memory Management for Personalized LLM Agents",
    "date": "2025-10-31",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Junfeng Lu",
      "Yueyan Li"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27418v1",
    "abstract": "Advances in large language models are making personalized AI agents a new research focus. While current agent systems primarily rely on personalized external memory databases to deliver customized experiences, they face challenges such as memory redundancy, memory staleness, and poor memory-context integration, largely due to the lack of effective memory updates during interaction. To tackle these issues, we propose a new memory management system designed for affective scenarios. Our approach employs a Bayesian-inspired memory update algorithm with the concept of memory entropy, enabling the agent to autonomously maintain a dynamically updated memory vector database by minimizing global entropy to provide more personalized services. To better evaluate the system's effectiveness in this context, we propose DABench, a benchmark focusing on emotional expression and emotional change toward objects. Experimental results demonstrate that, our system achieves superior performance in personalization, logical coherence, and accuracy. Ablation studies further validate the effectiveness of the Bayesian-inspired update mechanism in alleviating memory bloat. Our work offers new insights into the design of long-term memory systems.",
    "fetched_at": "2025-11-05T02:19:00.094966Z"
  },
  {
    "id": "2510.27452v1",
    "title": "From Pixels to Paths: A Multi-Agent Framework for Editable Scientific   Illustration",
    "date": "2025-10-31",
    "tags": [
      "cs.CV",
      "CV"
    ],
    "authors": [
      "Jianwen Sun",
      "Fanrui Zhang",
      "Yukang Feng",
      "Chuanhao Li",
      "Zizhen Li",
      "Jiaxin Ai",
      "Yifan Chang",
      "Yu Dai",
      "Kaipeng Zhang"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27452v1",
    "abstract": "Scientific illustrations demand both high information density and post-editability. However, current generative models have two major limitations: Frist, image generation models output rasterized images lacking semantic structure, making it impossible to access, edit, or rearrange independent visual components in the images. Second, code-based generation methods (TikZ or SVG), although providing element-level control, force users into the cumbersome cycle of \"writing-compiling-reviewing\" and lack the intuitiveness of manipulation. Neither of these two approaches can well meet the needs for efficiency, intuitiveness, and iterative modification in scientific creation. To bridge this gap, we introduce VisPainter, a multi-agent framework for scientific illustration built upon the model context protocol. VisPainter orchestrates three specialized modules-a Manager, a Designer, and a Toolbox-to collaboratively produce diagrams compatible with standard vector graphics software. This modular, role-based design allows each element to be explicitly represented and manipulated, enabling true element-level control and any element can be added and modified later. To systematically evaluate the quality of scientific illustrations, we introduce VisBench, a benchmark with seven-dimensional evaluation metrics. It assesses high-information-density scientific illustrations from four aspects: content, layout, visual perception, and interaction cost. To this end, we conducted extensive ablation experiments to verify the rationality of our architecture and the reliability of our evaluation methods. Finally, we evaluated various vision-language models, presenting fair and credible model rankings along with detailed comparisons of their respective capabilities. Additionally, we isolated and quantified the impacts of role division, step control,and description on the quality of illustrations.",
    "fetched_at": "2025-11-05T02:19:00.094925Z"
  },
  {
    "id": "2510.27484v1",
    "title": "Thought Branches: Interpreting LLM Reasoning Requires Resampling",
    "date": "2025-10-31",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI",
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Uzay Macar",
      "Paul C. Bogdan",
      "Senthooran Rajamanoharan",
      "Neel Nanda"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27484v1",
    "abstract": "Most work interpreting reasoning models studies only a single chain-of-thought (CoT), yet these models define distributions over many possible CoTs. We argue that studying a single sample is inadequate for understanding causal influence and the underlying computation. Though fully specifying this distribution is intractable, it can be understood by sampling. We present case studies using resampling to investigate model decisions. First, when a model states a reason for its action, does that reason actually cause the action? In \"agentic misalignment\" scenarios, we resample specific sentences to measure their downstream effects. Self-preservation sentences have small causal impact, suggesting they do not meaningfully drive blackmail. Second, are artificial edits to CoT sufficient for steering reasoning? These are common in literature, yet take the model off-policy. Resampling and selecting a completion with the desired property is a principled on-policy alternative. We find off-policy interventions yield small and unstable effects compared to resampling in decision-making tasks. Third, how do we understand the effect of removing a reasoning step when the model may repeat it post-edit? We introduce a resilience metric that repeatedly resamples to prevent similar content from reappearing downstream. Critical planning statements resist removal but have large effects when eliminated. Fourth, since CoT is sometimes \"unfaithful\", can our methods teach us anything in these settings? Adapting causal mediation analysis, we find that hints that have a causal effect on the output without being explicitly mentioned exert a subtle and cumulative influence on the CoT that persists even if the hint is removed. Overall, studying distributions via resampling enables reliable causal analysis, clearer narratives of model reasoning, and principled CoT interventions.",
    "fetched_at": "2025-11-05T02:19:00.094854Z"
  },
  {
    "id": "2510.27489v1",
    "title": "Auditing LLM Editorial Bias in News Media Exposure",
    "date": "2025-10-31",
    "tags": [
      "cs.CY",
      "CY"
    ],
    "authors": [
      "Marco Minici",
      "Cristian Consonni",
      "Federico Cinus",
      "Giuseppe Manco"
    ],
    "institution": "Google",
    "link": "http://arxiv.org/pdf/2510.27489v1",
    "abstract": "Large Language Models (LLMs) increasingly act as gateways to web content, shaping how millions of users encounter online information. Unlike traditional search engines, whose retrieval and ranking mechanisms are well studied, the selection processes of web-connected LLMs add layers of opacity to how answers are generated. By determining which news outlets users see, these systems can influence public opinion, reinforce echo chambers, and pose risks to civic discourse and public trust.   This work extends two decades of research in algorithmic auditing to examine how LLMs function as news engines. We present the first audit comparing three leading agents, GPT-4o-Mini, Claude-3.7-Sonnet, and Gemini-2.0-Flash, against Google News, asking: \\textit{How do LLMs differ from traditional aggregators in the diversity, ideology, and reliability of the media they expose to users?}   Across 24 global topics, we find that, compared to Google News, LLMs surface significantly fewer unique outlets and allocate attention more unevenly. In the same way, GPT-4o-Mini emphasizes more factual and right-leaning sources; Claude-3.7-Sonnet favors institutional and civil-society domains and slightly amplifies right-leaning exposure; and Gemini-2.0-Flash exhibits a modest left-leaning tilt without significant changes in factuality. These patterns remain robust under prompt variations and alternative reliability benchmarks. Together, our findings show that LLMs already enact \\textit{agentic editorial policies}, curating information in ways that diverge from conventional aggregators. Understanding and governing their emerging editorial power will be critical for ensuring transparency, pluralism, and trust in digital information ecosystems.",
    "fetched_at": "2025-11-05T02:19:00.094798Z"
  },
  {
    "id": "2510.27544v1",
    "title": "Mechanics of Learned Reasoning 1: TempoBench, A Benchmark for   Interpretable Deconstruction of Reasoning System Performance",
    "date": "2025-10-31",
    "tags": [
      "cs.AI",
      "AI",
      "cs.FL",
      "FL"
    ],
    "authors": [
      "Nikolaus Holzer",
      "William Fishell",
      "Baishakhi Ray",
      "Mark Santolucito"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27544v1",
    "abstract": "Large Language Models (LLMs) are increasingly excelling and outpacing human performance on many tasks. However, to improve LLM reasoning, researchers either rely on ad-hoc generated datasets or formal mathematical proof systems such as the Lean proof assistant. Whilst ad-hoc generated methods can capture the decision chains of real-world reasoning processes, they may encode some inadvertent bias in the space of reasoning they cover; they also cannot be formally verified. On the other hand, systems like Lean can guarantee verifiability, but are not well-suited to capture the nature of agentic decision chain-based tasks. This creates a gap both in performance for functions such as business agents or code assistants, and in the usefulness of LLM reasoning benchmarks, whereby these fall short in reasoning structure or real-world alignment. We introduce TempoBench, the first formally grounded and verifiable diagnostic benchmark that parametrizes difficulty to systematically analyze how LLMs perform reasoning. TempoBench uses two evaluation benchmarks to break down reasoning ability. First, temporal trace evaluation (TTE) tests the ability of an LLM to understand and simulate the execution of a given multi-step reasoning system. Subsequently, temporal causal evaluation (TCE) tests an LLM's ability to perform multi-step causal reasoning and to distill cause-and-effect relations from complex systems. We find that models score 65.6% on TCE-normal, and 7.5% on TCE-hard. This shows that state-of-the-art LLMs clearly understand the TCE task but perform poorly as system complexity increases. Our code is available at our \\href{https://github.com/nik-hz/tempobench}{GitHub repository}.",
    "fetched_at": "2025-11-05T02:19:00.094746Z"
  },
  {
    "id": "2510.27566v1",
    "title": "Interact-RAG: Reason and Interact with the Corpus, Beyond Black-Box   Retrieval",
    "date": "2025-10-31",
    "tags": [
      "cs.IR",
      "IR"
    ],
    "authors": [
      "Yulong Hui",
      "Chao Chen",
      "Zhihang Fu",
      "Yihao Liu",
      "Jieping Ye",
      "Huanchen Zhang"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27566v1",
    "abstract": "Retrieval-Augmented Generation (RAG) has significantly enhanced LLMs by incorporating external information. However, prevailing agentic RAG approaches are constrained by a critical limitation: they treat the retrieval process as a black-box querying operation. This confines agents' actions to query issuing, hindering its ability to tackle complex information-seeking tasks. To address this, we introduce Interact-RAG, a new paradigm that elevates the LLM agent from a passive query issuer into an active manipulator of the retrieval process. We dismantle the black-box with a Corpus Interaction Engine, equipping the agent with a set of action primitives for fine-grained control over information retrieval. To further empower the agent on the entire RAG pipeline, we first develop a reasoning-enhanced workflow, which enables both zero-shot execution and the synthesis of interaction trajectories. We then leverage this synthetic data to train a fully autonomous end-to-end agent via Supervised Fine-Tuning (SFT), followed by refinement with Reinforcement Learning (RL). Extensive experiments across six benchmarks demonstrate that Interact-RAG significantly outperforms other advanced methods, validating the efficacy of our reasoning-interaction strategy.",
    "fetched_at": "2025-11-05T02:19:00.094691Z"
  },
  {
    "id": "2510.27569v1",
    "title": "MARAG-R1: Beyond Single Retriever via Reinforcement-Learned Multi-Tool   Agentic Retrieval",
    "date": "2025-10-31",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Qi Luo",
      "Xiaonan Li",
      "Yuxin Wang",
      "Tingshuo Fan",
      "Yuan Li",
      "Xinchi Chen",
      "Xipeng Qiu"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27569v1",
    "abstract": "Large Language Models (LLMs) excel at reasoning and generation but are inherently limited by static pretraining data, resulting in factual inaccuracies and weak adaptability to new information. Retrieval-Augmented Generation (RAG) addresses this issue by grounding LLMs in external knowledge; However, the effectiveness of RAG critically depends on whether the model can adequately access relevant information. Existing RAG systems rely on a single retriever with fixed top-k selection, restricting access to a narrow and static subset of the corpus. As a result, this single-retriever paradigm has become the primary bottleneck for comprehensive external information acquisition, especially in tasks requiring corpus-level reasoning. To overcome this limitation, we propose MARAG-R1, a reinforcement-learned multi-tool RAG framework that enables LLMs to dynamically coordinate multiple retrieval mechanisms for broader and more precise information access. MARAG-R1 equips the model with four retrieval tools -- semantic search, keyword search, filtering, and aggregation -- and learns both how and when to use them through a two-stage training process: supervised fine-tuning followed by reinforcement learning. This design allows the model to interleave reasoning and retrieval, progressively gathering sufficient evidence for corpus-level synthesis. Experiments on GlobalQA, HotpotQA, and 2WikiMultiHopQA demonstrate that MARAG-R1 substantially outperforms strong baselines and achieves new state-of-the-art results in corpus-level reasoning tasks.",
    "fetched_at": "2025-11-05T02:19:00.094636Z"
  },
  {
    "id": "2510.27598v2",
    "title": "InnovatorBench: Evaluating Agents' Ability to Conduct Innovative LLM   Research",
    "date": "2025-10-31",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Yunze Wu",
      "Dayuan Fu",
      "Weiye Si",
      "Zhen Huang",
      "Mohan Jiang",
      "Keyu Li",
      "Shijie Xia",
      "Jie Sun",
      "Tianze Xu",
      "Xiangkun Hu",
      "Pengrui Lu",
      "Xiaojie Cai",
      "Lyumanshan Ye",
      "Wenhong Zhu",
      "Yang Xiao",
      "Pengfei Liu"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27598v2",
    "abstract": "AI agents could accelerate scientific discovery by automating hypothesis formation, experiment design, coding, execution, and analysis, yet existing benchmarks probe narrow skills in simplified settings. To address this gap, we introduce InnovatorBench, a benchmark-platform pair for realistic, end-to-end assessment of agents performing Large Language Model (LLM) research. It comprises 20 tasks spanning Data Construction, Filtering, Augmentation, Loss Design, Reward Design, and Scaffold Construction, which require runnable artifacts and assessment of correctness, performance, output quality, and uncertainty. To support agent operation, we develop ResearchGym, a research environment offering rich action spaces, distributed and long-horizon execution, asynchronous monitoring, and snapshot saving. We also implement a lightweight ReAct agent that couples explicit reasoning with executable planning using frontier models such as Claude-4, GPT-5, GLM-4.5, and Kimi-K2. Our experiments demonstrate that while frontier models show promise in code-driven research tasks, they struggle with fragile algorithm-related tasks and long-horizon decision making, such as impatience, poor resource management, and overreliance on template-based reasoning. Furthermore, agents require over 11 hours to achieve their best performance on InnovatorBench, underscoring the benchmark's difficulty and showing the potential of InnovatorBench to be the next generation of code-based research benchmark.",
    "fetched_at": "2025-11-05T02:19:00.094569Z"
  },
  {
    "id": "2510.27617v1",
    "title": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation",
    "date": "2025-10-31",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Heng Ping",
      "Arijit Bhattacharjee",
      "Peiyu Zhang",
      "Shixuan Li",
      "Wei Yang",
      "Anzhe Cheng",
      "Xiaole Zhang",
      "Jesse Thomason",
      "Ali Jannesari",
      "Nesreen Ahmed",
      "Paul Bogdan"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27617v1",
    "abstract": "Automation of Register Transfer Level (RTL) design can help developers meet increasing computational demands. Large Language Models (LLMs) show promise for Hardware Description Language (HDL) generation, but face challenges due to limited parametric knowledge and domain-specific constraints. While prompt engineering and fine-tuning have limitations in knowledge coverage and training costs, multi-agent architectures offer a training-free paradigm to enhance reasoning through collaborative generation. However, current multi-agent approaches suffer from two critical deficiencies: susceptibility to noise propagation and constrained reasoning space exploration. We propose VeriMoA, a training-free mixture-of-agents (MoA) framework with two synergistic innovations. First, a quality-guided caching mechanism to maintain all intermediate HDL outputs and enables quality-based ranking and selection across the entire generation process, encouraging knowledge accumulation over layers of reasoning. Second, a multi-path generation strategy that leverages C++ and Python as intermediate representations, decomposing specification-to-HDL translation into two-stage processes that exploit LLM fluency in high-resource languages while promoting solution diversity. Comprehensive experiments on VerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves 15--30% improvements in Pass@1 across diverse LLM backbones, especially enabling smaller models to match larger models and fine-tuned alternatives without requiring costly training.",
    "fetched_at": "2025-11-05T02:19:00.094476Z"
  },
  {
    "id": "2510.27623v1",
    "title": "Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive   Trigger Learning",
    "date": "2025-10-31",
    "tags": [
      "cs.AI",
      "AI",
      "cs.CL",
      "CL",
      "cs.CV",
      "CV"
    ],
    "authors": [
      "Qiusi Zhan",
      "Hyeonjeong Ha",
      "Rui Yang",
      "Sirui Xu",
      "Hanyang Chen",
      "Liang-Yan Gui",
      "Yu-Xiong Wang",
      "Huan Zhang",
      "Heng Ji",
      "Daniel Kang"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.27623v1",
    "abstract": "Multimodal large language models (MLLMs) have advanced embodied agents by enabling direct perception, reasoning, and planning task-oriented actions from visual inputs. However, such vision driven embodied agents open a new attack surface: visual backdoor attacks, where the agent behaves normally until a visual trigger appears in the scene, then persistently executes an attacker-specified multi-step policy. We introduce BEAT, the first framework to inject such visual backdoors into MLLM-based embodied agents using objects in the environments as triggers. Unlike textual triggers, object triggers exhibit wide variation across viewpoints and lighting, making them difficult to implant reliably. BEAT addresses this challenge by (1) constructing a training set that spans diverse scenes, tasks, and trigger placements to expose agents to trigger variability, and (2) introducing a two-stage training scheme that first applies supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning (CTL). CTL formulates trigger discrimination as preference learning between trigger-present and trigger-free inputs, explicitly sharpening the decision boundaries to ensure precise backdoor activation. Across various embodied agent benchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while maintaining strong benign task performance, and generalizes reliably to out-of-distribution trigger placements. Notably, compared to naive SFT, CTL boosts backdoor activation accuracy up to 39% under limited backdoor data. These findings expose a critical yet unexplored security risk in MLLM-based embodied agents, underscoring the need for robust defenses before real-world deployment.",
    "fetched_at": "2025-11-05T02:19:00.094398Z"
  },
  {
    "id": "2510.27628v1",
    "title": "Validity Is What You Need",
    "date": "2025-10-31",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Sebastian Benthall",
      "Andrew Clark"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27628v1",
    "abstract": "While AI agents have long been discussed and studied in computer science, today's Agentic AI systems are something new. We consider other definitions of Agentic AI and propose a new realist definition. Agentic AI is a software delivery mechanism, comparable to software as a service (SaaS), which puts an application to work autonomously in a complex enterprise setting. Recent advances in large language models (LLMs) as foundation models have driven excitement in Agentic AI. We note, however, that Agentic AI systems are primarily applications, not foundations, and so their success depends on validation by end users and principal stakeholders. The tools and techniques needed by the principal users to validate their applications are quite different from the tools and techniques used to evaluate foundation models. Ironically, with good validation measures in place, in many cases the foundation models can be replaced with much simpler, faster, and more interpretable models that handle core logic. When it comes to Agentic AI, validity is what you need. LLMs are one option that might achieve it.",
    "fetched_at": "2025-11-05T02:19:00.094315Z"
  },
  {
    "id": "2510.27630v2",
    "title": "Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout   for Long-Horizon Task Training",
    "date": "2025-10-31",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Dayuan Fu",
      "Yunze Wu",
      "Xiaojie Cai",
      "Lyumanshan Ye",
      "Shijie Xia",
      "Zhen Huang",
      "Weiye Si",
      "Tianze Xu",
      "Jie Sun",
      "Keyu Li",
      "Mohan Jiang",
      "Junfei Wang",
      "Qishuo Hua",
      "Pengrui Lu",
      "Yang Xiao",
      "Pengfei Liu"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27630v2",
    "abstract": "Large Language Model (LLM) agents have recently shown strong potential in domains such as automated coding, deep research, and graphical user interface manipulation. However, training them to succeed on long-horizon, domain-specialized tasks remains challenging. Current methods primarily fall into two categories. The first relies on dense human annotations through behavior cloning, which is prohibitively expensive for long-horizon tasks that can take days or months. The second depends on outcome-driven sampling, which often collapses due to the rarity of valid positive trajectories on domain-specialized tasks. We introduce Apollo, a sampling framework that integrates asynchronous human guidance with action-level data filtering. Instead of requiring annotators to shadow every step, Apollo allows them to intervene only when the agent drifts from a promising trajectory, by providing prior knowledge, strategic advice, etc. This lightweight design makes it possible to sustain interactions for over 30 hours and produces valuable trajectories at a lower cost. Apollo then applies supervision control to filter out sub-optimal actions and prevent error propagation. Together, these components enable reliable and effective data collection in long-horizon environments. To demonstrate the effectiveness of Apollo, we evaluate it using InnovatorBench. Our experiments show that when applied to train the GLM-4.5 model on InnovatorBench, Apollo achieves more than a 50% improvement over the untrained baseline and a 28% improvement over a variant trained without human interaction. These results highlight the critical role of human-in-the-loop sampling and the robustness of Apollo's design in handling long-horizon, domain-specialized tasks.",
    "fetched_at": "2025-11-05T02:19:00.094252Z"
  },
  {
    "id": "2510.26040v1",
    "title": "Accelerating Real-World Overtaking in F1TENTH Racing Employing   Reinforcement Learning Methods",
    "date": "2025-10-30",
    "tags": [
      "cs.RO",
      "RO",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Emily Steiner",
      "Daniel van der Spuy",
      "Futian Zhou",
      "Afereti Pama",
      "Minas Liarokapis",
      "Henry Williams"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.26040v1",
    "abstract": "While autonomous racing performance in Time-Trial scenarios has seen significant progress and development, autonomous wheel-to-wheel racing and overtaking are still severely limited. These limitations are particularly apparent in real-life driving scenarios where state-of-the-art algorithms struggle to safely or reliably complete overtaking manoeuvres. This is important, as reliable navigation around other vehicles is vital for safe autonomous wheel-to-wheel racing. The F1Tenth Competition provides a useful opportunity for developing wheel-to-wheel racing algorithms on a standardised physical platform. The competition format makes it possible to evaluate overtaking and wheel-to-wheel racing algorithms against the state-of-the-art. This research presents a novel racing and overtaking agent capable of learning to reliably navigate a track and overtake opponents in both simulation and reality. The agent was deployed on an F1Tenth vehicle and competed against opponents running varying competitive algorithms in the real world. The results demonstrate that the agent's training against opponents enables deliberate overtaking behaviours with an overtaking rate of 87% compared 56% for an agent trained just to race.",
    "fetched_at": "2025-11-05T02:19:02.084239Z"
  },
  {
    "id": "2510.26089v1",
    "title": "Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle   Routing",
    "date": "2025-10-30",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI",
      "cs.MA",
      "MA"
    ],
    "authors": [
      "Fazel Arasteh",
      "Arian Haghparast",
      "Manos Papagelis"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26089v1",
    "abstract": "Traffic congestion in urban road networks leads to longer trip times and higher emissions, especially during peak periods. While the Shortest Path First (SPF) algorithm is optimal for a single vehicle in a static network, it performs poorly in dynamic, multi-vehicle settings, often worsening congestion by routing all vehicles along identical paths. We address dynamic vehicle routing through a multi-agent reinforcement learning (MARL) framework for coordinated, network-aware fleet navigation. We first propose Adaptive Navigation (AN), a decentralized MARL model where each intersection agent provides routing guidance based on (i) local traffic and (ii) neighborhood state modeled using Graph Attention Networks (GAT). To improve scalability in large networks, we further propose Hierarchical Hub-based Adaptive Navigation (HHAN), an extension of AN that assigns agents only to key intersections (hubs). Vehicles are routed hub-to-hub under agent control, while SPF handles micro-routing within each hub region. For hub coordination, HHAN adopts centralized training with decentralized execution (CTDE) under the Attentive Q-Mixing (A-QMIX) framework, which aggregates asynchronous vehicle decisions via attention. Hub agents use flow-aware state features that combine local congestion and predictive dynamics for proactive routing. Experiments on synthetic grids and real urban maps (Toronto, Manhattan) show that AN reduces average travel time versus SPF and learning baselines, maintaining 100% routing success. HHAN scales to networks with hundreds of intersections, achieving up to 15.9% improvement under heavy traffic. These findings highlight the potential of network-constrained MARL for scalable, coordinated, and congestion-aware routing in intelligent transportation systems.",
    "fetched_at": "2025-11-05T02:19:02.084181Z"
  },
  {
    "id": "2510.26236v1",
    "title": "PHUMA: Physically-Grounded Humanoid Locomotion Dataset",
    "date": "2025-10-30",
    "tags": [
      "cs.RO",
      "RO"
    ],
    "authors": [
      "Kyungmin Lee",
      "Sibeen Kim",
      "Minho Park",
      "Hyunseung Kim",
      "Dongyoon Hwang",
      "Hojoon Lee",
      "Jaegul Choo"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.26236v1",
    "abstract": "Motion imitation is a promising approach for humanoid locomotion, enabling agents to acquire humanlike behaviors. Existing methods typically rely on high-quality motion capture datasets such as AMASS, but these are scarce and expensive, limiting scalability and diversity. Recent studies attempt to scale data collection by converting large-scale internet videos, exemplified by Humanoid-X. However, they often introduce physical artifacts such as floating, penetration, and foot skating, which hinder stable imitation. In response, we introduce PHUMA, a Physically-grounded HUMAnoid locomotion dataset that leverages human video at scale, while addressing physical artifacts through careful data curation and physics-constrained retargeting. PHUMA enforces joint limits, ensures ground contact, and eliminates foot skating, producing motions that are both large-scale and physically reliable. We evaluated PHUMA in two sets of conditions: (i) imitation of unseen motion from self-recorded test videos and (ii) path following with pelvis-only guidance. In both cases, PHUMA-trained policies outperform Humanoid-X and AMASS, achieving significant gains in imitating diverse motions. The code is available at https://davian-robotics.github.io/PHUMA.",
    "fetched_at": "2025-11-05T02:19:02.083880Z"
  },
  {
    "id": "2510.26363v1",
    "title": "Towards Reinforcement Learning Based Log Loading Automation",
    "date": "2025-10-30",
    "tags": [
      "cs.RO",
      "RO"
    ],
    "authors": [
      "Ilya Kurinov",
      "Miroslav Ivanov",
      "Grzegorz Orzechowski",
      "Aki Mikkola"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26363v1",
    "abstract": "Forestry forwarders play a central role in mechanized timber harvesting by picking up and moving logs from the felling site to a processing area or a secondary transport vehicle. Forwarder operation is challenging and physically and mentally exhausting for the operator who must control the machine in remote areas for prolonged periods of time. Therefore, even partial automation of the process may reduce stress on the operator. This study focuses on continuing previous research efforts in application of reinforcement learning agents in automating log handling process, extending the task from grasping which was studied in previous research to full log loading operation. The resulting agent will be capable to automate a full loading procedure from locating and grappling to transporting and delivering the log to a forestry forwarder bed. To train the agent, a trailer type forestry forwarder simulation model in NVIDIA's Isaac Gym and a virtual environment for a typical log loading scenario were developed. With reinforcement learning agents and a curriculum learning approach, the trained agent may be a stepping stone towards application of reinforcement learning agents in automation of the forestry forwarder. The agent learnt grasping a log in a random position from grapple's random position and transport it to the bed with 94% success rate of the best performing agent.",
    "fetched_at": "2025-11-05T02:19:02.083711Z"
  },
  {
    "id": "2510.26389v1",
    "title": "Adaptive Context Length Optimization with Low-Frequency Truncation for   Multi-Agent Reinforcement Learning",
    "date": "2025-10-30",
    "tags": [
      "cs.LG",
      "LG",
      "cs.MA",
      "MA"
    ],
    "authors": [
      "Wenchang Duan",
      "Yaoliang Yu",
      "Jiwan He",
      "Yi Shi"
    ],
    "institution": "Google, MIT",
    "link": "http://arxiv.org/pdf/2510.26389v1",
    "abstract": "Recently, deep multi-agent reinforcement learning (MARL) has demonstrated promising performance for solving challenging tasks, such as long-term dependencies and non-Markovian environments. Its success is partly attributed to conditioning policies on large fixed context length. However, such large fixed context lengths may lead to limited exploration efficiency and redundant information. In this paper, we propose a novel MARL framework to obtain adaptive and effective contextual information. Specifically, we design a central agent that dynamically optimizes context length via temporal gradient analysis, enhancing exploration to facilitate convergence to global optima in MARL. Furthermore, to enhance the adaptive optimization capability of the context length, we present an efficient input representation for the central agent, which effectively filters redundant information. By leveraging a Fourier-based low-frequency truncation method, we extract global temporal trends across decentralized agents, providing an effective and efficient representation of the MARL environment. Extensive experiments demonstrate that the proposed method achieves state-of-the-art (SOTA) performance on long-term dependency tasks, including PettingZoo, MiniGrid, Google Research Football (GRF), and StarCraft Multi-Agent Challenge v2 (SMACv2).",
    "fetched_at": "2025-11-05T02:19:02.083664Z"
  },
  {
    "id": "2510.26423v1",
    "title": "Nexus: Execution-Grounded Multi-Agent Test Oracle Synthesis",
    "date": "2025-10-30",
    "tags": [
      "cs.SE",
      "SE",
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Dong Huang",
      "Mingzhe Du",
      "Jie M. Zhang",
      "Zheng Lin",
      "Meng Luo",
      "Qianru Zhang",
      "See-Kiong Ng"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26423v1",
    "abstract": "Test oracle generation in non-regression testing is a longstanding challenge in software engineering, where the goal is to produce oracles that can accurately determine whether a function under test (FUT) behaves as intended for a given input. In this paper, we introduce Nexus, a novel multi-agent framework to address this challenge. Nexus generates test oracles by leveraging a diverse set of specialized agents that synthesize test oracles through a structured process of deliberation, validation, and iterative self-refinement. During the deliberation phase, a panel of four specialist agents, each embodying a distinct testing philosophy, collaboratively critiques and refines an initial set of test oracles. Then, in the validation phase, Nexus generates a plausible candidate implementation of the FUT and executes the proposed oracles against it in a secure sandbox. For any oracle that fails this execution-based check, Nexus activates an automated selfrefinement loop, using the specific runtime error to debug and correct the oracle before re-validation. Our extensive evaluation on seven diverse benchmarks demonstrates that Nexus consistently and substantially outperforms state-of-theart baselines. For instance, Nexus improves the test-level oracle accuracy on the LiveCodeBench from 46.30% to 57.73% for GPT-4.1-Mini. The improved accuracy also significantly enhances downstream tasks: the bug detection rate of GPT4.1-Mini generated test oracles on HumanEval increases from 90.91% to 95.45% for Nexus compared to baselines, and the success rate of automated program repair improves from 35.23% to 69.32%.",
    "fetched_at": "2025-11-05T02:19:02.083615Z"
  },
  {
    "id": "2510.26438v2",
    "title": "An Impulse Control Approach to Market Making in a Hawkes LOB Market",
    "date": "2025-10-30",
    "tags": [
      "q-fin.TR",
      "TR",
      "q-fin.CP",
      "CP"
    ],
    "authors": [
      "Konark Jain",
      "Nick Firoozye",
      "Jonathan Kochems",
      "Philip Treleaven"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.26438v2",
    "abstract": "We study the optimal Market Making problem in a Limit Order Book (LOB) market simulated using a high-fidelity, mutually exciting Hawkes process. Departing from traditional Brownian-driven mid-price models, our setup captures key microstructural properties such as queue dynamics, inter-arrival clustering, and endogenous price impact. Recognizing the realistic constraint that market makers cannot update strategies at every LOB event, we formulate the control problem within an impulse control framework, where interventions occur discretely via limit, cancel, or market orders. This leads to a high-dimensional, non-local Hamilton-Jacobi-Bellman Quasi-Variational Inequality (HJB-QVI), whose solution is analytically intractable and computationally expensive due to the curse of dimensionality. To address this, we propose a novel Reinforcement Learning (RL) approximation inspired by auxiliary control formulations. Using a two-network PPO-based architecture with self-imitation learning, we demonstrate strong empirical performance with limited training, achieving Sharpe ratios above 30 in a realistic simulated LOB. In addition to that, we solve the HJB-QVI using a deep learning method inspired by Sirignano and Spiliopoulos 2018 and compare the performance with the RL agent. Our findings highlight the promise of combining impulse control theory with modern deep RL to tackle optimal execution problems in jump-driven microstructural markets.",
    "fetched_at": "2025-11-05T02:19:02.083554Z"
  },
  {
    "id": "2510.26536v1",
    "title": "RoboOS-NeXT: A Unified Memory-based Framework for Lifelong, Scalable,   and Robust Multi-Robot Collaboration",
    "date": "2025-10-30",
    "tags": [
      "cs.RO",
      "RO"
    ],
    "authors": [
      "Huajie Tan",
      "Cheng Chi",
      "Xiansheng Chen",
      "Yuheng Ji",
      "Zhongxia Zhao",
      "Xiaoshuai Hao",
      "Yaoxu Lyu",
      "Mingyu Cao",
      "Junkai Zhao",
      "Huaihai Lyu",
      "Enshen Zhou",
      "Ning Chen",
      "Yankai Fu",
      "Cheng Peng",
      "Wei Guo",
      "Dong Liang",
      "Zhuo Chen",
      "Mengsi Lyu",
      "Chenrui He",
      "Yulong Ao",
      "Yonghua Lin",
      "Pengwei Wang",
      "Zhongyuan Wang",
      "Shanghang Zhang"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.26536v1",
    "abstract": "The proliferation of collaborative robots across diverse tasks and embodiments presents a central challenge: achieving lifelong adaptability, scalable coordination, and robust scheduling in multi-agent systems. Existing approaches, from vision-language-action (VLA) models to hierarchical frameworks, fall short due to their reliance on limited or dividual-agent memory. This fundamentally constrains their ability to learn over long horizons, scale to heterogeneous teams, or recover from failures, highlighting the need for a unified memory representation. To address these limitations, we introduce RoboOS-NeXT, a unified memory-based framework for lifelong, scalable, and robust multi-robot collaboration. At the core of RoboOS-NeXT is the novel Spatio-Temporal-Embodiment Memory (STEM), which integrates spatial scene geometry, temporal event history, and embodiment profiles into a shared representation. This memory-centric design is integrated into a brain-cerebellum framework, where a high-level brain model performs global planning by retrieving and updating STEM, while low-level controllers execute actions locally. This closed loop between cognition, memory, and execution enables dynamic task allocation, fault-tolerant collaboration, and consistent state synchronization. We conduct extensive experiments spanning complex coordination tasks in restaurants, supermarkets, and households. Our results demonstrate that RoboOS-NeXT achieves superior performance across heterogeneous embodiments, validating its effectiveness in enabling lifelong, scalable, and robust multi-robot collaboration. Project website: https://flagopen.github.io/RoboOS/",
    "fetched_at": "2025-11-05T02:19:02.083434Z"
  },
  {
    "id": "2510.26578v1",
    "title": "Two-Timescale Optimization Framework for IAB-Enabled Heterogeneous UAV   Networks",
    "date": "2025-10-30",
    "tags": [
      "eess.SY",
      "SY",
      "cs.SY"
    ],
    "authors": [
      "Jikang Deng",
      "Hui Zhou",
      "Mohamed-Slim Alouini"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.26578v1",
    "abstract": "In post-disaster scenarios, the rapid deployment of adequate communication infrastructure is essential to support disaster search, rescue, and recovery operations. To achieve this, uncrewed aerial vehicle (UAV) has emerged as a promising solution for emergency communication due to its low cost and deployment flexibility. However, conventional untethered UAV (U-UAV) is constrained by size, weight, and power (SWaP) limitations, making it incapable of maintaining the operation of a macro base station. To address this limitation, we propose a heterogeneous UAV-based framework that integrates tethered UAV (T-UAV) and U-UAVs, where U-UAVs are utilized to enhance the throughput of cell-edge ground user equipments (G-UEs) and guarantee seamless connectivity during G-UEs' mobility to safe zones. It is noted that the integrated access and backhaul (IAB) technique is adopted to support the wireless backhaul of U-UAVs. Accordingly, we formulate a two-timescale joint user scheduling and trajectory control optimization problem, aiming to maximize the downlink throughput under asymmetric traffic demands and G-UEs' mobility. To solve the formulated problem, we proposed a two-timescale multi-agent deep deterministic policy gradient (TTS-MADDPG) algorithm based on the centralized training and distributed execution paradigm. Numerical results show that the proposed algorithm outperforms other benchmarks, including the two-timescale multi-agent proximal policy optimization (TTS-MAPPO) algorithm and MADDPG scheduling method, with robust and higher throughput. Specifically, the proposed algorithm obtains up to 12.2\\% average throughput gain compared to the MADDPG scheduling method.",
    "fetched_at": "2025-11-05T02:19:02.083255Z"
  },
  {
    "id": "2510.26610v1",
    "title": "A DRL-Empowered Multi-Level Jamming Approach for Secure Semantic   Communication",
    "date": "2025-10-30",
    "tags": [
      "cs.CR",
      "CR"
    ],
    "authors": [
      "Weixuan Chen",
      "Qianqian Yang"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.26610v1",
    "abstract": "Semantic communication (SemCom) aims to transmit only task-relevant information, thereby improving communication efficiency but also exposing semantic information to potential eavesdropping. In this paper, we propose a deep reinforcement learning (DRL)-empowered multi-level jamming approach to enhance the security of SemCom systems over MIMO fading wiretap channels. This approach combines semantic layer jamming, achieved by encoding task-irrelevant text, and physical layer jamming, achieved by encoding random Gaussian noise. These two-level jamming signals are superposed with task-relevant semantic information to protect the transmitted semantics from eavesdropping. A deep deterministic policy gradient (DDPG) algorithm is further introduced to dynamically design and optimize the precoding matrices for both taskrelevant semantic information and multi-level jamming signals, aiming to enhance the legitimate user's image reconstruction while degrading the eavesdropper's performance. To jointly train the SemCom model and the DDPG agent, we propose an alternating optimization strategy where the two modules are updated iteratively. Experimental results demonstrate that, compared with both the encryption-based (ESCS) and encoded jammer-based (EJ) benchmarks, our method achieves comparable security while improving the legitimate user's peak signalto-noise ratio (PSNR) by up to approximately 0.6 dB.",
    "fetched_at": "2025-11-05T02:19:02.083207Z"
  },
  {
    "id": "2510.26740v1",
    "title": "A General Incentives-Based Framework for Fairness in Multi-agent   Resource Allocation",
    "date": "2025-10-30",
    "tags": [
      "cs.MA",
      "MA",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Ashwin Kumar",
      "William Yeoh"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26740v1",
    "abstract": "We introduce the General Incentives-based Framework for Fairness (GIFF), a novel approach for fair multi-agent resource allocation that infers fair decision-making from standard value functions. In resource-constrained settings, agents optimizing for efficiency often create inequitable outcomes. Our approach leverages the action-value (Q-)function to balance efficiency and fairness without requiring additional training. Specifically, our method computes a local fairness gain for each action and introduces a counterfactual advantage correction term to discourage over-allocation to already well-off agents. This approach is formalized within a centralized control setting, where an arbitrator uses the GIFF-modified Q-values to solve an allocation problem.   Empirical evaluations across diverse domains, including dynamic ridesharing, homelessness prevention, and a complex job allocation task-demonstrate that our framework consistently outperforms strong baselines and can discover far-sighted, equitable policies. The framework's effectiveness is supported by a theoretical foundation; we prove its fairness surrogate is a principled lower bound on the true fairness improvement and that its trade-off parameter offers monotonic tuning. Our findings establish GIFF as a robust and principled framework for leveraging standard reinforcement learning components to achieve more equitable outcomes in complex multi-agent systems.",
    "fetched_at": "2025-11-05T02:19:02.083068Z"
  },
  {
    "id": "2510.26782v1",
    "title": "Clone Deterministic 3D Worlds with Geometrically-Regularized World   Models",
    "date": "2025-10-30",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI",
      "cs.CV",
      "CV"
    ],
    "authors": [
      "Zaishuo Xia",
      "Yukuan Lu",
      "Xinyi Li",
      "Yifan Xu",
      "Yubei Chen"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26782v1",
    "abstract": "A world model is an internal model that simulates how the world evolves. Given past observations and actions, it predicts the future of both the embodied agent and its environment. Accurate world models are essential for enabling agents to think, plan, and reason effectively in complex, dynamic settings. Despite rapid progress, current world models remain brittle and degrade over long horizons. We argue that a central cause is representation quality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or entangled latents make dynamics learning unnecessarily hard. We therefore ask whether improving representation learning alone can substantially improve world-model performance. In this work, we take a step toward building a truly accurate world model by addressing a fundamental yet open problem: constructing a model that can fully clone and overfit to a deterministic 3D world. We propose Geometrically-Regularized World Models (GRWM), which enforces that consecutive points along a natural sensory trajectory remain close in latent representation space. This approach yields significantly improved latent representations that align closely with the true topology of the environment. GRWM is plug-and-play, requires only minimal architectural modification, scales with trajectory length, and is compatible with diverse latent generative backbones. Across deterministic 3D settings and long-horizon prediction tasks, GRWM significantly increases rollout fidelity and stability. Analyses show that its benefits stem from learning a latent manifold with superior geometric structure. These findings support a clear takeaway: improving representation learning is a direct and useful path to robust world models, delivering reliable long-horizon predictions without enlarging the dynamics module.",
    "fetched_at": "2025-11-05T02:19:02.083019Z"
  },
  {
    "id": "2510.26787v1",
    "title": "Remote Labor Index: Measuring AI Automation of Remote Work",
    "date": "2025-10-30",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI",
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Mantas Mazeika",
      "Alice Gatti",
      "Cristina Menghini",
      "Udari Madhushani Sehwag",
      "Shivam Singhal",
      "Yury Orlovskiy",
      "Steven Basart",
      "Manasi Sharma",
      "Denis Peskoff",
      "Elaine Lau",
      "Jaehyuk Lim",
      "Lachlan Carroll",
      "Alice Blair",
      "Vinaya Sivakumar",
      "Sumana Basu",
      "Brad Kenstler",
      "Yuntao Ma",
      "Julian Michael",
      "Xiaoke Li",
      "Oliver Ingebretsen",
      "Aditya Mehta",
      "Jean Mottola",
      "John Teichmann",
      "Kevin Yu",
      "Zaina Shaik",
      "Adam Khoja",
      "Richard Ren",
      "Jason Hausenloy",
      "Long Phan",
      "Ye Htet",
      "Ankit Aich",
      "Tahseen Rabbani",
      "Vivswan Shah",
      "Andriy Novykov",
      "Felix Binder",
      "Kirill Chugunov",
      "Luis Ramirez",
      "Matias Geralnik",
      "Hernán Mesura",
      "Dean Lee",
      "Ed-Yeremai Hernandez Cardona",
      "Annette Diamond",
      "Summer Yue",
      "Alexandr Wang",
      "Bing Liu",
      "Ernesto Hernandez",
      "Dan Hendrycks"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26787v1",
    "abstract": "AIs have made rapid progress on research-oriented benchmarks of knowledge and reasoning, but it remains unclear how these gains translate into economic value and automation. To measure this, we introduce the Remote Labor Index (RLI), a broadly multi-sector benchmark comprising real-world, economically valuable projects designed to evaluate end-to-end agent performance in practical settings. AI agents perform near the floor on RLI, with the highest-performing agent achieving an automation rate of 2.5%. These results help ground discussions of AI automation in empirical evidence, setting a common basis for tracking AI impacts and enabling stakeholders to proactively navigate AI-driven labor automation.",
    "fetched_at": "2025-11-05T02:19:02.082943Z"
  },
  {
    "id": "2510.26887v1",
    "title": "The Denario project: Deep knowledge AI agents for scientific discovery",
    "date": "2025-10-30",
    "tags": [
      "cs.AI",
      "AI",
      "cs.CL",
      "CL",
      "cs.LG",
      "LG",
      "cs.MA",
      "MA"
    ],
    "authors": [
      "Francisco Villaescusa-Navarro",
      "Boris Bolliet",
      "Pablo Villanueva-Domingo",
      "Adrian E. Bayer",
      "Aidan Acquah",
      "Chetana Amancharla",
      "Almog Barzilay-Siegal",
      "Pablo Bermejo",
      "Camille Bilodeau",
      "Pablo Cárdenas Ramírez",
      "Miles Cranmer",
      "Urbano L. França",
      "ChangHoon Hahn",
      "Yan-Fei Jiang",
      "Raul Jimenez",
      "Jun-Young Lee",
      "Antonio Lerario",
      "Osman Mamun",
      "Thomas Meier",
      "Anupam A. Ojha",
      "Pavlos Protopapas",
      "Shimanto Roy",
      "David N. Spergel",
      "Pedro Tarancón-Álvarez",
      "Ujjwal Tiwari",
      "Matteo Viel",
      "Digvijay Wadekar",
      "Chi Wang",
      "Bonny Y. Wang",
      "Licong Xu",
      "Yossi Yovel",
      "Shuwen Yue",
      "Wen-Han Zhou",
      "Qiyao Zhu",
      "Jiajun Zou",
      "Íñigo Zubeldia"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.26887v1",
    "abstract": "We present Denario, an AI multi-agent system designed to serve as a scientific research assistant. Denario can perform many different tasks, such as generating ideas, checking the literature, developing research plans, writing and executing code, making plots, and drafting and reviewing a scientific paper. The system has a modular architecture, allowing it to handle specific tasks, such as generating an idea, or carrying out end-to-end scientific analysis using Cmbagent as a deep-research backend. In this work, we describe in detail Denario and its modules, and illustrate its capabilities by presenting multiple AI-generated papers generated by it in many different scientific disciplines such as astrophysics, biology, biophysics, biomedical informatics, chemistry, material science, mathematical physics, medicine, neuroscience and planetary science. Denario also excels at combining ideas from different disciplines, and we illustrate this by showing a paper that applies methods from quantum physics and machine learning to astrophysical data. We report the evaluations performed on these papers by domain experts, who provided both numerical scores and review-like feedback. We then highlight the strengths, weaknesses, and limitations of the current system. Finally, we discuss the ethical implications of AI-driven research and reflect on how such technology relates to the philosophy of science. We publicly release the code at https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run directly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and the full app will be deployed on the cloud.",
    "fetched_at": "2025-11-05T02:19:02.082752Z"
  },
  {
    "id": "2510.26037v1",
    "title": "SIRAJ: Diverse and Efficient Red-Teaming for LLM Agents via Distilled   Structured Reasoning",
    "date": "2025-10-30",
    "tags": [
      "cs.CR",
      "CR",
      "cs.AI",
      "AI",
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Kaiwen Zhou",
      "Ahmed Elgohary",
      "A S M Iftekhar",
      "Amin Saied"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26037v1",
    "abstract": "The ability of LLM agents to plan and invoke tools exposes them to new safety risks, making a comprehensive red-teaming system crucial for discovering vulnerabilities and ensuring their safe deployment. We present SIRAJ: a generic red-teaming framework for arbitrary black-box LLM agents. We employ a dynamic two-step process that starts with an agent definition and generates diverse seed test cases that cover various risk outcomes, tool-use trajectories, and risk sources. Then, it iteratively constructs and refines model-based adversarial attacks based on the execution trajectories of former attempts. To optimize the red-teaming cost, we present a model distillation approach that leverages structured forms of a teacher model's reasoning to train smaller models that are equally effective. Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. Our distilled 8B red-teamer model improves attack success rate by 100%, surpassing the 671B Deepseek-R1 model. Our ablations and analyses validate the effectiveness of the iterative framework, structured reasoning, and the generalization of our red-teamer models.",
    "fetched_at": "2025-11-05T02:19:00.097509Z"
  },
  {
    "id": "2510.26098v1",
    "title": "GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in   GUI Tasks",
    "date": "2025-10-30",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Chenrui Shi",
      "Zedong Yu",
      "Zhi Gao",
      "Ruining Feng",
      "Enqi Liu",
      "Yuwei Wu",
      "Yunde Jia",
      "Liuyu Xiang",
      "Zhaofeng He",
      "Qing Li"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26098v1",
    "abstract": "Large vision language models (VLMs) have advanced graphical user interface (GUI) task automation but still lag behind humans. We hypothesize this gap stems from missing core GUI knowledge, which existing training schemes (such as supervised fine tuning and reinforcement learning) alone cannot fully address. By analyzing common failure patterns in GUI task execution, we distill GUI knowledge into three dimensions: (1) interface perception, knowledge about recognizing widgets and system states; (2) interaction prediction, knowledge about reasoning action state transitions; and (3) instruction understanding, knowledge about planning, verifying, and assessing task completion progress. We further introduce GUI Knowledge Bench, a benchmark with multiple choice and yes/no questions across six platforms (Web, Android, MacOS, Windows, Linux, IOS) and 292 applications. Our evaluation shows that current VLMs identify widget functions but struggle with perceiving system states, predicting actions, and verifying task completion. Experiments on real world GUI tasks further validate the close link between GUI knowledge and task success. By providing a structured framework for assessing GUI knowledge, our work supports the selection of VLMs with greater potential prior to downstream training and provides insights for building more capable GUI agents.",
    "fetched_at": "2025-11-05T02:19:00.097460Z"
  },
  {
    "id": "2510.26114v1",
    "title": "OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script   Research",
    "date": "2025-10-30",
    "tags": [
      "cs.CV",
      "CV"
    ],
    "authors": [
      "Caoshuo Li",
      "Zengmao Ding",
      "Xiaobin Hu",
      "Bang Li",
      "Donghao Luo",
      "Xu Peng",
      "Taisong Jin",
      "Yongge Liu",
      "Shengwei Han",
      "Jing Yang",
      "Xiaoping He",
      "Feng Gao",
      "AndyPian Wu",
      "SevenShu",
      "Chaoyang Wang",
      "Chengjie Wang"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26114v1",
    "abstract": "As one of the earliest writing systems, Oracle Bone Script (OBS) preserves the cultural and intellectual heritage of ancient civilizations. However, current OBS research faces two major challenges: (1) the interpretation of OBS involves a complex workflow comprising multiple serial and parallel sub-tasks, and (2) the efficiency of OBS information organization and retrieval remains a critical bottleneck, as scholars often spend substantial effort searching for, compiling, and managing relevant resources. To address these challenges, we present OracleAgent, the first agent system designed for the structured management and retrieval of OBS-related information. OracleAgent seamlessly integrates multiple OBS analysis tools, empowered by large language models (LLMs), and can flexibly orchestrate these components. Additionally, we construct a comprehensive domain-specific multimodal knowledge base for OBS, which is built through a rigorous multi-year process of data collection, cleaning, and expert annotation. The knowledge base comprises over 1.4M single-character rubbing images and 80K interpretation texts. OracleAgent leverages this resource through its multimodal tools to assist experts in retrieval tasks of character, document, interpretation text, and rubbing image. Extensive experiments demonstrate that OracleAgent achieves superior performance across a range of multimodal reasoning and generation tasks, surpassing leading mainstream multimodal large language models (MLLMs) (e.g., GPT-4o). Furthermore, our case study illustrates that OracleAgent can effectively assist domain experts, significantly reducing the time cost of OBS research. These results highlight OracleAgent as a significant step toward the practical deployment of OBS-assisted research and automated interpretation systems.",
    "fetched_at": "2025-11-05T02:19:00.097390Z"
  },
  {
    "id": "2510.26125v1",
    "title": "WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging   Long-tail Scenarios",
    "date": "2025-10-30",
    "tags": [
      "cs.CV",
      "CV",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Runsheng Xu",
      "Hubert Lin",
      "Wonseok Jeon",
      "Hao Feng",
      "Yuliang Zou",
      "Liting Sun",
      "John Gorman",
      "Kate Tolstaya",
      "Sarah Tang",
      "Brandyn White",
      "Ben Sapp",
      "Mingxing Tan",
      "Jyh-Jing Hwang",
      "Drago Anguelov"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26125v1",
    "abstract": "Vision-based end-to-end (E2E) driving has garnered significant interest in the research community due to its scalability and synergy with multimodal large language models (MLLMs). However, current E2E driving benchmarks primarily feature nominal scenarios, failing to adequately test the true potential of these systems. Furthermore, existing open-loop evaluation metrics often fall short in capturing the multi-modal nature of driving or effectively evaluating performance in long-tail scenarios. To address these gaps, we introduce the Waymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021 driving segments (approximately 12 hours), specifically curated for challenging long-tail scenarios that that are rare in daily life with an occurring frequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the high-level routing information, ego states, and 360-degree camera views from 8 surrounding cameras. To evaluate the E2E driving performance on these long-tail situations, we propose a novel open-loop evaluation metric: Rater Feedback Score (RFS). Unlike conventional metrics that measure the distance between predicted way points and the logs, RFS measures how closely the predicted trajectory matches rater-annotated trajectory preference labels. We have released rater preference labels for all WOD-E2E validation set segments, while the held out test set labels have been used for the 2025 WOD-E2E Challenge. Through our work, we aim to foster state of the art research into generalizable, robust, and safe end-to-end autonomous driving agents capable of handling complex real-world situations.",
    "fetched_at": "2025-11-05T02:19:00.097296Z"
  },
  {
    "id": "2510.26144v1",
    "title": "The FM Agent",
    "date": "2025-10-30",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Annan Li",
      "Chufan Wu",
      "Zengle Ge",
      "Yee Hin Chong",
      "Zhinan Hou",
      "Lizhe Cao",
      "Cheng Ju",
      "Jianmin Wu",
      "Huaiming Li",
      "Haobo Zhang",
      "Shenghao Feng",
      "Mo Zhao",
      "Fengzhi Qiu",
      "Rui Yang",
      "Mengmeng Zhang",
      "Wenyi Zhu",
      "Yingying Sun",
      "Quan Sun",
      "Shunhao Yan",
      "Danyu Liu",
      "Dawei Yin",
      "Dou Shen"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26144v1",
    "abstract": "Large language models (LLMs) are catalyzing the development of autonomous AI research agents for scientific and engineering discovery. We present FM Agent, a novel and general-purpose multi-agent framework that leverages a synergistic combination of LLM-based reasoning and large-scale evolutionary search to address complex real-world challenges. The core of FM Agent integrates several key innovations: 1) a cold-start initialization phase incorporating expert guidance, 2) a novel evolutionary sampling strategy for iterative optimization, 3) domain-specific evaluators that combine correctness, effectiveness, and LLM-supervised feedback, and 4) a distributed, asynchronous execution infrastructure built on Ray. Demonstrating broad applicability, our system has been evaluated across diverse domains, including operations research, machine learning, GPU kernel optimization, and classical mathematical problems. FM Agent reaches state-of-the-art results autonomously, without human interpretation or tuning -- 1976.3 on ALE-Bench (+5.2\\%), 43.56\\% on MLE-Bench (+4.0pp), up to 20x speedups on KernelBench, and establishes new state-of-the-art(SOTA) results on several classical mathematical problems. Beyond academic benchmarks, FM Agent shows considerable promise for both large-scale enterprise R\\&D workflows and fundamental scientific research, where it can accelerate innovation, automate complex discovery processes, and deliver substantial engineering and scientific advances with broader societal impact.",
    "fetched_at": "2025-11-05T02:19:00.097211Z"
  },
  {
    "id": "2510.26163v1",
    "title": "Exploring Dissatisfaction in Bus Route Reduction through LLM-Calibrated   Agent-Based Modeling",
    "date": "2025-10-30",
    "tags": [
      "cs.CY",
      "CY",
      "I.6.3; J.1; J.4",
      "4"
    ],
    "authors": [
      "Qiumeng Li",
      "Xinxi Yang",
      "Suhong Zhou"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26163v1",
    "abstract": "As emerging mobility modes continue to expand, many cities face declining bus ridership, increasing fiscal pressure to sustain underutilized routes, and growing inefficiencies in resource allocation. This study employs an agent-based modelling (ABM) approach calibrated through a large language model (LLM) using few-shot learning to examine how progressive bus route cutbacks affect passenger dissatisfaction across demographic groups and overall network resilience. Using IC-card data from Beijing's Huairou District, the LLM-calibrated ABM estimated passenger sensitivity parameters related to travel time, waiting, transfers, and crowding. Results show that the structural configuration of the bus network exerts a stronger influence on system stability than capacity or operational factors. The elimination of high-connectivity routes led to an exponential rise in total dissatisfaction, particularly among passengers with disabilities and older adults. The evolution of dissatisfaction exhibited three distinct phases - stable, transitional, and critical. Through the analysis of each stage, this study found that the continuous bus route reduction scenario exhibits three-stage thresholds. Once these thresholds are crossed, even a small reduction in routes may lead to a significant loss of passenger flow. Research highlights the nonlinear response of user sentiment to service reductions and underscore the importance of maintaining structural critical routes and providing stable services to vulnerable groups for equitable and resilient transport planning.",
    "fetched_at": "2025-11-05T02:19:00.097099Z"
  },
  {
    "id": "2510.26167v1",
    "title": "One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient   Reasoning",
    "date": "2025-10-30",
    "tags": [
      "cs.AI",
      "AI",
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Renhao Li",
      "Jianhong Tu",
      "Yang Su",
      "Hamid Alinejad-Rokny",
      "Derek F. Wong",
      "Junyang Lin",
      "Min Yang"
    ],
    "institution": "OpenAI, MIT",
    "link": "http://arxiv.org/pdf/2510.26167v1",
    "abstract": "Reward models (RMs) play a critical role in aligning large language models (LLMs) with human preferences. Yet in the domain of tool learning, the lack of RMs specifically designed for function-calling tasks has limited progress toward more capable agentic AI. We introduce ToolRM, a family of lightweight generative RMs tailored for general tool-use scenarios. To build these models, we propose a novel pipeline that constructs pairwise preference data using rule-based scoring and multidimensional sampling. This yields ToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique tasks that supports reinforcement learning with verifiable feedback. To evaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on the agentic evaluation suite BFCL. Trained on our constructed data, models from the Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially outperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward judgments. Beyond training objectives, ToolRM generalizes to broader critique tasks, including Best-of-N sampling and self-correction. Experiments on ACEBench highlight its effectiveness and efficiency, enabling inference-time scaling and reducing output token usage by over 66%. We release data and model checkpoints to facilitate future research.",
    "fetched_at": "2025-11-05T02:19:00.097047Z"
  },
  {
    "id": "2510.26172v1",
    "title": "Linking Heterogeneous Data with Coordinated Agent Flows for Social Media   Analysis",
    "date": "2025-10-30",
    "tags": [
      "cs.HC",
      "HC",
      "cs.AI",
      "AI",
      "cs.SI",
      "SI"
    ],
    "authors": [
      "Shifu Chen",
      "Dazhen Deng",
      "Zhihong Xu",
      "Sijia Xu",
      "Tai-Quan Peng",
      "Yingcai Wu"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26172v1",
    "abstract": "Social media platforms generate massive volumes of heterogeneous data, capturing user behaviors, textual content, temporal dynamics, and network structures. Analyzing such data is crucial for understanding phenomena such as opinion dynamics, community formation, and information diffusion. However, discovering insights from this complex landscape is exploratory, conceptually challenging, and requires expertise in social media mining and visualization. Existing automated approaches, though increasingly leveraging large language models (LLMs), remain largely confined to structured tabular data and cannot adequately address the heterogeneity of social media analysis. We present SIA (Social Insight Agents), an LLM agent system that links heterogeneous multi-modal data -- including raw inputs (e.g., text, network, and behavioral data), intermediate outputs, mined analytical results, and visualization artifacts -- through coordinated agent flows. Guided by a bottom-up taxonomy that connects insight types with suitable mining and visualization techniques, SIA enables agents to plan and execute coherent analysis strategies. To ensure multi-modal integration, it incorporates a data coordinator that unifies tabular, textual, and network data into a consistent flow. Its interactive interface provides a transparent workflow where users can trace, validate, and refine the agent's reasoning, supporting both adaptability and trustworthiness. Through expert-centered case studies and quantitative evaluation, we show that SIA effectively discovers diverse and meaningful insights from social media while supporting human-agent collaboration in complex analytical tasks.",
    "fetched_at": "2025-11-05T02:19:00.096982Z"
  },
  {
    "id": "2510.26242v1",
    "title": "Retrieval Augmented Generation-Enhanced Distributed LLM Agents for   Generalizable Traffic Signal Control with Emergency Vehicles",
    "date": "2025-10-30",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Xinhang Li",
      "Qing Guo",
      "Junyu Chen",
      "Zheng Guo",
      "Shengzhe Xu",
      "Lei Li",
      "Lin Zhang"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.26242v1",
    "abstract": "With increasing urban traffic complexity, Traffic Signal Control (TSC) is essential for optimizing traffic flow and improving road safety. Large Language Models (LLMs) emerge as promising approaches for TSC. However, they are prone to hallucinations in emergencies, leading to unreliable decisions that may cause substantial delays for emergency vehicles. Moreover, diverse intersection types present substantial challenges for traffic state encoding and cross-intersection training, limiting generalization across heterogeneous intersections. Therefore, this paper proposes Retrieval Augmented Generation (RAG)-enhanced distributed LLM agents with Emergency response for Generalizable TSC (REG-TSC). Firstly, this paper presents an emergency-aware reasoning framework, which dynamically adjusts reasoning depth based on the emergency scenario and is equipped with a novel Reviewer-based Emergency RAG (RERAG) to distill specific knowledge and guidance from historical cases, enhancing the reliability and rationality of agents' emergency decisions. Secondly, this paper designs a type-agnostic traffic representation and proposes a Reward-guided Reinforced Refinement (R3) for heterogeneous intersections. R3 adaptively samples training experience from diverse intersections with environment feedback-based priority and fine-tunes LLM agents with a designed reward-weighted likelihood loss, guiding REG-TSC toward high-reward policies across heterogeneous intersections. On three real-world road networks with 17 to 177 heterogeneous intersections, extensive experiments show that REG-TSC reduces travel time by 42.00%, queue length by 62.31%, and emergency vehicle waiting time by 83.16%, outperforming other state-of-the-art methods.",
    "fetched_at": "2025-11-05T02:19:00.096911Z"
  },
  {
    "id": "2510.26270v1",
    "title": "Graph-Enhanced Policy Optimization in LLM Agent Training",
    "date": "2025-10-30",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Jiazhen Yuan",
      "Wei Zhao",
      "Zhengbiao Bai"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26270v1",
    "abstract": "Group based reinforcement learning (RL) has shown impressive results on complex reasoning and mathematical tasks. Yet, when applied to train multi-turn, interactive LLM agents, these methods often suffer from structural blindness-the inability to exploit the underlying connectivity of the environment. This manifests in three critical challenges: (1) inefficient, unguided exploration, (2) imprecise credit assignment due to overlooking pivotal states, and (3) myopic planning caused by static reward discounting. We address these issues with Graph-Enhanced Policy Optimization (GEPO), which dynamically constructs a state-transition graph from agent experience and employs graph-theoretic centrality to provide three synergistic learning signals: (1)structured intrinsic rewards that guide exploration toward high-impact states, (2) a graph-enhanced advantage function for topology-aware credit assignment, and (3) a dynamic discount factor adapted to each state's strategic value. On the ALFWorld, WebShop, and a proprietary Workbench benchmarks, GEPO demonstrates strong performance, achieving absolute success rate gains of +4.1%, +5.3%, and +10.9% over competitive baselines. These results highlight that explicitly modeling environmental structure is a robust, generalizable strategy for advancing LLM agent training.",
    "fetched_at": "2025-11-05T02:19:00.096849Z"
  },
  {
    "id": "2510.26287v1",
    "title": "Empowering RepoQA-Agent based on Reinforcement Learning Driven by   Monte-carlo Tree Search",
    "date": "2025-10-30",
    "tags": [
      "cs.SE",
      "SE"
    ],
    "authors": [
      "Guochang Li",
      "Yuchen Liu",
      "Zhen Qin",
      "Yunkun Wang",
      "Jianping Zhong",
      "Chen Zhi",
      "Binhua Li",
      "Fei Huang",
      "Yongbin Li",
      "Shuiguang Deng"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.26287v1",
    "abstract": "Repository-level software engineering tasks require large language models (LLMs) to efficiently navigate and extract information from complex codebases through multi-turn tool interactions. Existing approaches face significant limitations: training-free, in-context learning methods struggle to guide agents effectively in tool utilization and decision-making based on environmental feedback, while training-based approaches typically rely on costly distillation from larger LLMs, introducing data compliance concerns in enterprise environments. To address these challenges, we introduce RepoSearch-R1, a novel agentic reinforcement learning framework driven by Monte-carlo Tree Search (MCTS). This approach allows agents to generate diverse, high-quality reasoning trajectories via self-training without requiring model distillation or external supervision. Based on RepoSearch-R1, we construct a RepoQA-Agent specifically designed for repository question-answering tasks. Comprehensive evaluation on repository question-answering tasks demonstrates that RepoSearch-R1 achieves substantial improvements of answer completeness: 16.0% enhancement over no-retrieval methods, 19.5% improvement over iterative retrieval methods, and 33% increase in training efficiency compared to general agentic reinforcement learning approaches. Our cold-start training methodology eliminates data compliance concerns while maintaining robust exploration diversity and answer completeness across repository-level reasoning tasks.",
    "fetched_at": "2025-11-05T02:19:00.096801Z"
  },
  {
    "id": "2510.26328v1",
    "title": "Agent Skills Enable a New Class of Realistic and Trivially Simple Prompt   Injections",
    "date": "2025-10-30",
    "tags": [
      "cs.LG",
      "LG"
    ],
    "authors": [
      "David Schmotz",
      "Sahar Abdelnabi",
      "Maksym Andriushchenko"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26328v1",
    "abstract": "Enabling continual learning in LLMs remains a key unresolved research challenge. In a recent announcement, a frontier LLM company made a step towards this by introducing Agent Skills, a framework that equips agents with new knowledge based on instructions stored in simple markdown files. Although Agent Skills can be a very useful tool, we show that they are fundamentally insecure, since they enable trivially simple prompt injections. We demonstrate how to hide malicious instructions in long Agent Skill files and referenced scripts to exfiltrate sensitive data, such as internal files or passwords. Importantly, we show how to bypass system-level guardrails of a popular coding agent: a benign, task-specific approval with the \"Don't ask again\" option can carry over to closely related but harmful actions. Overall, we conclude that despite ongoing research efforts and scaling model capabilities, frontier LLMs remain vulnerable to very simple prompt injections in realistic scenarios. Our code is available at https://github.com/aisa-group/promptinject-agent-skills.",
    "fetched_at": "2025-11-05T02:19:00.096731Z"
  },
  {
    "id": "2510.26352v1",
    "title": "The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic   Teams for Multi-Agent Collaboration",
    "date": "2025-10-30",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI",
      "cs.MA",
      "MA"
    ],
    "authors": [
      "Kotaro Furuya",
      "Yuichi Kitagawa"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26352v1",
    "abstract": "While a multi-agent approach based on large language models (LLMs) represents a promising strategy to surpass the capabilities of single models, its success is critically dependent on synergistic team composition. However, forming optimal teams is a significant challenge, as the inherent opacity of most models obscures the internal characteristics necessary for effective collaboration. In this paper, we propose an interaction-centric framework for automatic team composition that does not require any prior knowledge including their internal architectures, training data, or task performances. Our method constructs a \"language model graph\" that maps relationships between models from the semantic coherence of pairwise conversations, and then applies community detection to identify synergistic model clusters. Our experiments with diverse LLMs demonstrate that the proposed method discovers functionally coherent groups that reflect their latent specializations. Priming conversations with specific topics identified synergistic teams which outperform random baselines on downstream benchmarks and achieve comparable accuracy to that of manually-curated teams based on known model specializations. Our findings provide a new basis for the automated design of collaborative multi-agent LLM teams.",
    "fetched_at": "2025-11-05T02:19:00.096687Z"
  },
  {
    "id": "2510.26494v1",
    "title": "Simulating and Experimenting with Social Media Mobilization Using LLM   Agents",
    "date": "2025-10-30",
    "tags": [
      "cs.SI",
      "SI",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Sadegh Shirani",
      "Mohsen Bayati"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26494v1",
    "abstract": "Online social networks have transformed the ways in which political mobilization messages are disseminated, raising new questions about how peer influence operates at scale. Building on the landmark 61-million-person Facebook experiment \\citep{bond201261}, we develop an agent-based simulation framework that integrates real U.S. Census demographic distributions, authentic Twitter network topology, and heterogeneous large language model (LLM) agents to examine the effect of mobilization messages on voter turnout. Each simulated agent is assigned demographic attributes, a personal political stance, and an LLM variant (\\texttt{GPT-4.1}, \\texttt{GPT-4.1-Mini}, or \\texttt{GPT-4.1-Nano}) reflecting its political sophistication. Agents interact over realistic social network structures, receiving personalized feeds and dynamically updating their engagement behaviors and voting intentions. Experimental conditions replicate the informational and social mobilization treatments of the original Facebook study. Across scenarios, the simulator reproduces qualitative patterns observed in field experiments, including stronger mobilization effects under social message treatments and measurable peer spillovers. Our framework provides a controlled, reproducible environment for testing counterfactual designs and sensitivity analyses in political mobilization research, offering a bridge between high-validity field experiments and flexible computational modeling.\\footnote{Code and data available at https://github.com/CausalMP/LLM-SocioPol}",
    "fetched_at": "2025-11-05T02:19:00.096645Z"
  },
  {
    "id": "2510.26498v1",
    "title": "A Multi-agent Large Language Model Framework to Automatically Assess   Performance of a Clinical AI Triage Tool",
    "date": "2025-10-30",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Adam E. Flanders",
      "Yifan Peng",
      "Luciano Prevedello",
      "Robyn Ball",
      "Errol Colak",
      "Prahlad Menon",
      "George Shih",
      "Hui-Ming Lin",
      "Paras Lakhani"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26498v1",
    "abstract": "Purpose: The purpose of this study was to determine if an ensemble of multiple LLM agents could be used collectively to provide a more reliable assessment of a pixel-based AI triage tool than a single LLM.   Methods: 29,766 non-contrast CT head exams from fourteen hospitals were processed by a commercial intracranial hemorrhage (ICH) AI detection tool. Radiology reports were analyzed by an ensemble of eight open-source LLM models and a HIPAA compliant internal version of GPT-4o using a single multi-shot prompt that assessed for presence of ICH. 1,726 examples were manually reviewed. Performance characteristics of the eight open-source models and consensus were compared to GPT-4o. Three ideal consensus LLM ensembles were tested for rating the performance of the triage tool.   Results: The cohort consisted of 29,766 head CTs exam-report pairs. The highest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78). The average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76). Llama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater precision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the ideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3 Ensemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522 (0.500-0.543). No statistically significant differences were observed between Top-3, Full-9, and Consensus (p > 0.05).   Conclusion: An ensemble of medium to large sized open-source LLMs provides a more consistent and reliable method to derive a ground truth retrospective evaluation of a clinical AI triage tool over a single LLM alone.",
    "fetched_at": "2025-11-05T02:19:00.096601Z"
  },
  {
    "id": "2510.26575v1",
    "title": "InfoFlow: Reinforcing Search Agent Via Reward Density Optimization",
    "date": "2025-10-30",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Kun Luo",
      "Hongjin Qian",
      "Zheng Liu",
      "Ziyi Xia",
      "Shitao Xiao",
      "Siqi Bao",
      "Jun Zhao",
      "Kang Liu"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26575v1",
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach for enhancing agentic deep search. However, its application is often hindered by low \\textbf{Reward Density} in deep search scenarios, where agents expend significant exploratory costs for infrequent and often null final rewards. In this paper, we formalize this challenge as the \\textbf{Reward Density Optimization} problem, which aims to improve the reward obtained per unit of exploration cost. This paper introduce \\textbf{InfoFlow}, a systematic framework that tackles this problem from three aspects. 1) \\textbf{Subproblem decomposition}: breaking down long-range tasks to assign process rewards, thereby providing denser learning signals. 2) \\textbf{Failure-guided hints}: injecting corrective guidance into stalled trajectories to increase the probability of successful outcomes. 3) \\textbf{Dual-agent refinement}: employing a dual-agent architecture to offload the cognitive burden of deep exploration. A refiner agent synthesizes the search history, which effectively compresses the researcher's perceived trajectory, thereby reducing exploration cost and increasing the overall reward density. We evaluate InfoFlow on multiple agentic search benchmarks, where it significantly outperforms strong baselines, enabling lightweight LLMs to achieve performance comparable to advanced proprietary LLMs.",
    "fetched_at": "2025-11-05T02:19:00.096533Z"
  },
  {
    "id": "2510.26585v1",
    "title": "Stop Wasting Your Tokens: Towards Efficient Runtime Multi-Agent Systems",
    "date": "2025-10-30",
    "tags": [
      "cs.MA",
      "MA",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Fulin Lin",
      "Shaowen Chen",
      "Ruishan Fang",
      "Hongwei Wang",
      "Tao Lin"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26585v1",
    "abstract": "While Multi-Agent Systems (MAS) excel at complex tasks, their growing autonomy with operational complexity often leads to critical inefficiencies, such as excessive token consumption and failures arising from misinformation. Existing methods primarily focus on post-hoc failure attribution, lacking proactive, real-time interventions to enhance robustness and efficiency. To this end, we introduce SupervisorAgent, a lightweight and modular framework for runtime, adaptive supervision that operates without altering the base agent's architecture. Triggered by an LLM-free adaptive filter, SupervisorAgent intervenes at critical junctures to proactively correct errors, guide inefficient behaviors, and purify observations. On the challenging GAIA benchmark, SupervisorAgent reduces the token consumption of the Smolagent framework by an average of 29.45% without compromising its success rate. Extensive experiments across five additional benchmarks (math reasoning, code generation, and question answering) and various SoTA foundation models validate the broad applicability and robustness of our approach. The code is available at https://github.com/LINs-lab/SupervisorAgent.",
    "fetched_at": "2025-11-05T02:19:00.096468Z"
  },
  {
    "id": "2510.26852v1",
    "title": "CATArena: Evaluation of LLM Agents through Iterative Tournament   Competitions",
    "date": "2025-10-30",
    "tags": [
      "cs.AI",
      "AI",
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Lingyue Fu",
      "Xin Ding",
      "Yaoming Zhu",
      "Shao Zhang",
      "Lin Qiu",
      "Weiwen Liu",
      "Weinan Zhang",
      "Xuezhi Cao",
      "Xunliang Cai",
      "Jiaxin Ding",
      "Yong Yu"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.26852v1",
    "abstract": "Large Language Model (LLM) agents have evolved from basic text generation to autonomously completing complex tasks through interaction with external tools. However, current benchmarks mainly assess end-to-end performance in fixed scenarios, restricting evaluation to specific skills and suffering from score saturation and growing dependence on expert annotation as agent capabilities improve. In this work, we emphasize the importance of learning ability, including both self-improvement and peer-learning, as a core driver for agent evolution toward human-level intelligence. We propose an iterative, competitive peer-learning framework, which allows agents to refine and optimize their strategies through repeated interactions and feedback, thereby systematically evaluating their learning capabilities. To address the score saturation issue in current benchmarks, we introduce CATArena, a tournament-style evaluation platform featuring four diverse board and card games with open-ended scoring. By providing tasks without explicit upper score limits, CATArena enables continuous and dynamic evaluation of rapidly advancing agent capabilities. Experimental results and analyses involving both minimal and commercial code agents demonstrate that CATArena provides reliable, stable, and scalable benchmarking for core agent abilities, particularly learning ability and strategy coding.",
    "fetched_at": "2025-11-05T02:19:00.096417Z"
  },
  {
    "id": "2510.26603v1",
    "title": "Agentic AI Home Energy Management System: A Large Language Model   Framework for Residential Load Scheduling",
    "date": "2025-10-30",
    "tags": [
      "cs.AI",
      "AI",
      "cs.MA",
      "MA",
      "cs.SY",
      "SY",
      "eess.SY"
    ],
    "authors": [
      "Reda El Makroum",
      "Sebastian Zwickl-Bernhard",
      "Lukas Kranzl"
    ],
    "institution": "Google, MIT",
    "link": "http://arxiv.org/pdf/2510.26603v1",
    "abstract": "The electricity sector transition requires substantial increases in residential demand response capacity, yet Home Energy Management Systems (HEMS) adoption remains limited by user interaction barriers requiring translation of everyday preferences into technical parameters. While large language models have been applied to energy systems as code generators and parameter extractors, no existing implementation deploys LLMs as autonomous coordinators managing the complete workflow from natural language input to multi-appliance scheduling. This paper presents an agentic AI HEMS where LLMs autonomously coordinate multi-appliance scheduling from natural language requests to device control, achieving optimal scheduling without example demonstrations. A hierarchical architecture combining one orchestrator with three specialist agents uses the ReAct pattern for iterative reasoning, enabling dynamic coordination without hardcoded workflows while integrating Google Calendar for context-aware deadline extraction. Evaluation across three open-source models using real Austrian day-ahead electricity prices reveals substantial capability differences. Llama-3.3-70B successfully coordinates all appliances across all scenarios to match cost-optimal benchmarks computed via mixed-integer linear programming, while other models achieve perfect single-appliance performance but struggle to coordinate all appliances simultaneously. Progressive prompt engineering experiments demonstrate that analytical query handling without explicit guidance remains unreliable despite models' general reasoning capabilities. We open-source the complete system including orchestration logic, agent prompts, tools, and web interfaces to enable reproducibility, extension, and future research.",
    "fetched_at": "2025-11-05T02:19:00.096343Z"
  },
  {
    "id": "2510.26854v1",
    "title": "Inverse Knowledge Search over Verifiable Reasoning: Synthesizing a   Scientific Encyclopedia from a Long Chains-of-Thought Knowledge Base",
    "date": "2025-10-30",
    "tags": [
      "cs.AI",
      "AI",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Yu Li",
      "Yuan Huang",
      "Tao Wang",
      "Caiyu Fan",
      "Xiansheng Cai",
      "Sihan Hu",
      "Xinzijian Liu",
      "Cheng Shi",
      "Mingjun Xu",
      "Zhen Wang",
      "Yan Wang",
      "Xiangqi Jin",
      "Tianhan Zhang",
      "Linfeng Zhang",
      "Lei Wang",
      "Youjin Deng",
      "Pan Zhang",
      "Weijie Sun",
      "Xingyu Li",
      "Weinan E",
      "Linfeng Zhang",
      "Zhiyuan Yao",
      "Kun Chen"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.26854v1",
    "abstract": "Most scientific materials compress reasoning, presenting conclusions while omitting the derivational chains that justify them. This compression hinders verification by lacking explicit, step-wise justifications and inhibits cross-domain links by collapsing the very pathways that establish the logical and causal connections between concepts. We introduce a scalable framework that decompresses scientific reasoning, constructing a verifiable Long Chain-of-Thought (LCoT) knowledge base and projecting it into an emergent encyclopedia, SciencePedia. Our pipeline operationalizes an endpoint-driven, reductionist strategy: a Socratic agent, guided by a curriculum of around 200 courses, generates approximately 3 million first-principles questions. To ensure high fidelity, multiple independent solver models generate LCoTs, which are then rigorously filtered by prompt sanitization and cross-model answer consensus, retaining only those with verifiable endpoints. This verified corpus powers the Brainstorm Search Engine, which performs inverse knowledge search -- retrieving diverse, first-principles derivations that culminate in a target concept. This engine, in turn, feeds the Plato synthesizer, which narrates these verified chains into coherent articles. The initial SciencePedia comprises approximately 200,000 fine-grained entries spanning mathematics, physics, chemistry, biology, engineering, and computation. In evaluations across six disciplines, Plato-synthesized articles (conditioned on retrieved LCoTs) exhibit substantially higher knowledge-point density and significantly lower factual error rates than an equally-prompted baseline without retrieval (as judged by an external LLM). Built on this verifiable LCoT knowledge base, this reasoning-centric approach enables trustworthy, cross-domain scientific synthesis at scale and establishes the foundation for an ever-expanding encyclopedia.",
    "fetched_at": "2025-11-05T02:19:00.096292Z"
  },
  {
    "id": "2510.26615v2",
    "title": "SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual   Document Understanding",
    "date": "2025-10-30",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Yiqiao Jin",
      "Rachneet Kaur",
      "Zhen Zeng",
      "Sumitra Ganesh",
      "Srijan Kumar"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26615v2",
    "abstract": "Multi-page visual documents such as manuals, brochures, presentations, and posters convey key information through layout, colors, icons, and cross-slide references. While large language models (LLMs) offer opportunities in document understanding, current systems struggle with complex, multi-page visual documents, particularly in fine-grained reasoning over elements and pages. We introduce SlideAgent, a versatile agentic framework for understanding multi-modal, multi-page, and multi-layout documents, especially slide decks. SlideAgent employs specialized agents and decomposes reasoning into three specialized levels-global, page, and element-to construct a structured, query-agnostic representation that captures both overarching themes and detailed visual or textual cues. During inference, SlideAgent selectively activates specialized agents for multi-level reasoning and integrates their outputs into coherent, context-aware answers. Extensive experiments show that SlideAgent achieves significant improvement over both proprietary (+7.9 overall) and open-source models (+9.8 overall).",
    "fetched_at": "2025-11-05T02:19:00.096174Z"
  },
  {
    "id": "2510.26658v1",
    "title": "The Era of Agentic Organization: Learning to Organize with Language   Models",
    "date": "2025-10-30",
    "tags": [
      "cs.AI",
      "AI",
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Zewen Chi",
      "Li Dong",
      "Qingxiu Dong",
      "Yaru Hao",
      "Xun Wu",
      "Shaohan Huang",
      "Furu Wei"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26658v1",
    "abstract": "We envision a new era of AI, termed agentic organization, where agents solve complex problems by working collaboratively and concurrently, enabling outcomes beyond individual intelligence. To realize this vision, we introduce asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large language models, which organizes the internal thinking process into concurrently executable structures. Specifically, we propose a thinking protocol where an organizer dynamically assigns sub-queries to workers, merges intermediate knowledge, and produces coherent solutions. More importantly, the thinking structure in this protocol can be further optimized through reinforcement learning. Experiments demonstrate that AsyncThink achieves 28% lower inference latency compared to parallel thinking while improving accuracy on mathematical reasoning. Moreover, AsyncThink generalizes its learned asynchronous thinking capabilities, effectively tackling unseen tasks without additional training.",
    "fetched_at": "2025-11-05T02:19:00.096124Z"
  },
  {
    "id": "2510.26699v1",
    "title": "Using Copilot Agent Mode to Automate Library Migration: A Quantitative   Assessment",
    "date": "2025-10-30",
    "tags": [
      "cs.SE",
      "SE"
    ],
    "authors": [
      "Aylton Almeida",
      "Laerte Xavier",
      "Marco Tulio Valente"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26699v1",
    "abstract": "Keeping software systems up to date is essential to avoid technical debt, security vulnerabilities, and the rigidity typical of legacy systems. However, updating libraries and frameworks remains a time consuming and error-prone process. Recent advances in Large Language Models (LLMs) and agentic coding systems offer new opportunities for automating such maintenance tasks. In this paper, we evaluate the update of a well-known Python library, SQLAlchemy, across a dataset of ten client applications. For this task, we use the Github's Copilot Agent Mode, an autonomous AI systema capable of planning and executing multi-step migration workflows. To assess the effectiveness of the automated migration, we also introduce Migration Coverage, a metric that quantifies the proportion of API usage points correctly migrated. The results of our study show that the LLM agent was capable of migrating functionalities and API usages between SQLAlchemy versions (migration coverage: 100%, median), but failed to maintain the application functionality, leading to a low test-pass rate (39.75%, median).",
    "fetched_at": "2025-11-05T02:19:00.096064Z"
  },
  {
    "id": "2510.26702v1",
    "title": "Delegated Authorization for Agents Constrained to Semantic Task-to-Scope   Matching",
    "date": "2025-10-30",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Majed El Helou",
      "Chiara Troiani",
      "Benjamin Ryder",
      "Jean Diaconu",
      "Hervé Muyal",
      "Marcelo Yannuzzi"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.26702v1",
    "abstract": "Authorizing Large Language Model driven agents to dynamically invoke tools and access protected resources introduces significant risks, since current methods for delegating authorization grant overly broad permissions and give access to tools allowing agents to operate beyond the intended task scope. We introduce and assess a delegated authorization model enabling authorization servers to semantically inspect access requests to protected resources, and issue access tokens constrained to the minimal set of scopes necessary for the agents' assigned tasks. Given the unavailability of datasets centered on delegated authorization flows, particularly including both semantically appropriate and inappropriate scope requests for a given task, we introduce ASTRA, a dataset and data generation pipeline for benchmarking semantic matching between tasks and scopes. Our experiments show both the potential and current limitations of model-based matching, particularly as the number of scopes needed for task completion increases. Our results highlight the need for further research into semantic matching techniques enabling intent-aware authorization for multi-agent and tool-augmented applications, including fine-grained control, such as Task-Based Access Control (TBAC).",
    "fetched_at": "2025-11-05T02:19:00.096020Z"
  },
  {
    "id": "2510.26790v1",
    "title": "Gistify! Codebase-Level Understanding via Runtime Execution",
    "date": "2025-10-30",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Hyunji Lee",
      "Minseon Kim",
      "Chinmay Singh",
      "Matheus Pereira",
      "Atharv Sonwane",
      "Isadora White",
      "Elias Stengel-Eskin",
      "Mohit Bansal",
      "Zhengyan Shi",
      "Alessandro Sordoni",
      "Marc-Alexandre Côté",
      "Xingdi Yuan",
      "Lucas Caccia"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26790v1",
    "abstract": "As coding agents are increasingly deployed in large codebases, the need to automatically design challenging, codebase-level evaluation is central. We propose Gistify, a task where a coding LLM must create a single, minimal, self-contained file that can reproduce a specific functionality of a codebase. The coding LLM is given full access to a codebase along with a specific entrypoint (e.g., a python command), and the generated file must replicate the output of the same command ran under the full codebase, while containing only the essential components necessary to execute the provided command. Success on Gistify requires both structural understanding of the codebase, accurate modeling of its execution flow as well as the ability to produce potentially large code patches. Our findings show that current state-of-the-art models struggle to reliably solve Gistify tasks, especially ones with long executions traces.",
    "fetched_at": "2025-11-05T02:19:00.095952Z"
  },
  {
    "id": "2510.26913v1",
    "title": "FlowMesh: A Service Fabric for Composable LLM Workflows",
    "date": "2025-10-30",
    "tags": [
      "cs.DC",
      "DC"
    ],
    "authors": [
      "Junyi Shen",
      "Noppanat Wadlom",
      "Lingfeng Zhou",
      "Dequan Wang",
      "Xu Miao",
      "Lei Fang",
      "Yao Lu"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26913v1",
    "abstract": "AI deployment increasingly resembles a pipeline of data transformation, fine-tuning, and agent interactions rather than a monolithic LLM job; recent examples include RLHF/RLAIF training and agentic workflows. To cope with this shift, we propose FlowMesh, a multi-tenant service fabric that executes and optimizes these workloads as one shared service instead of isolated pipelines. It decomposes workflows into fine-grained operators with recorded lineage, enabling de-duplication of work across users and batching requests on the same hardware while preserving per-workflow provenance. A global control plane maintains a cluster-wide pool of ready operators and uses a single utility function to pick both the batch and the worker, balancing throughput, cost, and data locality on heterogeneous GPUs. The data plane is an elastic fleet of stateless workers backed by a content-addressable store, enabling rapid, automatic scale-out, safe retry after preemption, and portability across managed clusters such as Kubernetes and geo-distributed GPU marketplaces such as Vast.ai. Compared with baseline solutions, FlowMesh achieves up to 3.8x cost reduction and 2.0x lower energy usage, provides a similar or better latency profile, and remains efficient under dynamic and failure-prone conditions.",
    "fetched_at": "2025-11-05T02:19:00.095874Z"
  },
  {
    "id": "2510.27016v1",
    "title": "Semantically-Aware LLM Agent to Enhance Privacy in Conversational AI   Services",
    "date": "2025-10-30",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Jayden Serenari",
      "Stephen Lee"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.27016v1",
    "abstract": "With the increasing use of conversational AI systems, there is growing concern over privacy leaks, especially when users share sensitive personal data in interactions with Large Language Models (LLMs). Conversations shared with these models may contain Personally Identifiable Information (PII), which, if exposed, could lead to security breaches or identity theft. To address this challenge, we present the Local Optimizations for Pseudonymization with Semantic Integrity Directed Entity Detection (LOPSIDED) framework, a semantically-aware privacy agent designed to safeguard sensitive PII data when using remote LLMs. Unlike prior work that often degrade response quality, our approach dynamically replaces sensitive PII entities in user prompts with semantically consistent pseudonyms, preserving the contextual integrity of conversations. Once the model generates its response, the pseudonyms are automatically depseudonymized, ensuring the user receives an accurate, privacy-preserving output. We evaluate our approach using real-world conversations sourced from ShareGPT, which we further augment and annotate to assess whether named entities are contextually relevant to the model's response. Our results show that LOPSIDED reduces semantic utility errors by a factor of 5 compared to baseline techniques, all while enhancing privacy.",
    "fetched_at": "2025-11-05T02:19:00.095815Z"
  },
  {
    "id": "2510.25232v1",
    "title": "From Medical Records to Diagnostic Dialogues: A Clinical-Grounded   Approach and Dataset for Psychiatric Comorbidity",
    "date": "2025-10-29",
    "tags": [
      "cs.AI",
      "AI",
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Tianxi Wan",
      "Jiaming Luo",
      "Siyuan Chen",
      "Kunyao Lan",
      "Jianhua Chen",
      "Haiyang Geng",
      "Mengyue Wu"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25232v1",
    "abstract": "Psychiatric comorbidity is clinically significant yet challenging due to the complexity of multiple co-occurring disorders. To address this, we develop a novel approach integrating synthetic patient electronic medical record (EMR) construction and multi-agent diagnostic dialogue generation. We create 502 synthetic EMRs for common comorbid conditions using a pipeline that ensures clinical relevance and diversity. Our multi-agent framework transfers the clinical interview protocol into a hierarchical state machine and context tree, supporting over 130 diagnostic states while maintaining clinical standards. Through this rigorous process, we construct PsyCoTalk, the first large-scale dialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic dialogues validated by psychiatrists. This dataset enhances diagnostic accuracy and treatment planning, offering a valuable resource for psychiatric comorbidity research. Compared to real-world clinical transcripts, PsyCoTalk exhibits high structural and linguistic fidelity in terms of dialogue length, token distribution, and diagnostic reasoning strategies. Licensed psychiatrists confirm the realism and diagnostic validity of the dialogues. This dataset enables the development and evaluation of models capable of multi-disorder psychiatric screening in a single conversational pass.",
    "fetched_at": "2025-11-05T02:19:02.085714Z"
  },
  {
    "id": "2510.25269v1",
    "title": "The influence of the random numbers quality on the results in stochastic   simulations and machine learning",
    "date": "2025-10-29",
    "tags": [
      "cs.PF",
      "PF"
    ],
    "authors": [
      "Benjamin A. Antunes"
    ],
    "institution": "LIRMM | DALI",
    "link": "http://arxiv.org/pdf/2510.25269v1",
    "abstract": "Pseudorandom number generators (PRNGs) are ubiquitous in stochastic simulations and machine learning (ML), where they drive sampling, parameter initialization, regularization, and data shuffling. While widely used, the potential impact of PRNG statistical quality on computational results remains underexplored. In this study, we investigate whether differences in PRNG quality, as measured by standard statistical test suites, can influence outcomes in representative stochastic applications. Seven PRNGs were evaluated, ranging from low-quality linear congruential generators (LCGs) with known statistical deficiencies to high-quality generators such as Mersenne Twister, PCG, and Philox. We applied these PRNGs to four distinct tasks: an epidemiological agent-based model (ABM), two independent from-scratch MNIST classification implementations (Python/NumPy and C++), and a reinforcement learning (RL) CartPole environment. Each experiment was repeated 30 times per generator using fixed seeds to ensure reproducibility, and outputs were compared using appropriate statistical analyses. Results show that very poor statistical quality, as in the ''bad'' LCG failing 125 TestU01 Crush tests, produces significant deviations in ABM epidemic dynamics, reduces MNIST classification accuracy, and severely degrades RL performance. In contrast, mid-and good-quality LCGs-despite failing a limited number of Crush or BigCrush tests-performed comparably to top-tier PRNGs in most tasks, with the RL experiment being the primary exception where performance scaled with statistical quality. Our findings indicate that, once a generator meets a sufficient statistical robustness threshold, its family or design has negligible impact on outcomes for most workloads, allowing selection to be guided by performance and implementation considerations. However, the use of low-quality PRNGs in sensitive stochastic computations can introduce substantial and systematic errors.",
    "fetched_at": "2025-11-05T02:19:02.085653Z"
  },
  {
    "id": "2510.25271v1",
    "title": "Adaptive Design of mmWave Initial Access Codebooks using Reinforcement   Learning",
    "date": "2025-10-29",
    "tags": [
      "cs.NI",
      "NI"
    ],
    "authors": [
      "Sabrine Aroua",
      "Christos Anastasios Bovolis",
      "Bo Göransson",
      "Anastasios Giovanidis",
      "Mathieu Leconte",
      "Apostolos Destounis"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.25271v1",
    "abstract": "Initial access (IA) is the process by which user equipment (UE) establishes its first connection with a base station. In 5G systems, particularly at millimeter-wave frequencies, IA integrates beam management to support highly directional transmissions. The base station employs a codebook of beams for the transmission of Synchronization Signal Blocks (SSBs), which are periodically swept to detect and connect users. The design of this SSB codebook is critical for ensuring reliable, wide-area coverage. In current networks, SSB codebooks are meticulously engineered by domain experts. While these expert-defined codebooks provide a robust baseline, they lack flexibility in dynamic or heterogeneous environments where user distributions vary, limiting their overall effectiveness. This paper proposes a hybrid Reinforcement Learning (RL) framework for adaptive SSB codebook design. Building on top of expert knowledge, the RL agent leverages a pool of expert-designed SSB beams and learns to adaptively select or combine them based on real-time feedback. This enables the agent to dynamically tailor codebooks to the actual environment, without requiring explicit user location information, while always respecting practical beam constraints. Simulation results demonstrate that, on average, the proposed approach improves user connectivity by 10.8$\\%$ compared to static expert configurations. These findings highlight the potential of combining expert knowledge with data-driven optimization to achieve more intelligent, flexible, and resilient beam management in next-generation wireless networks.",
    "fetched_at": "2025-11-05T02:19:02.085628Z"
  },
  {
    "id": "2510.25340v1",
    "title": "Multi-party Agent Relation Sampling for Multi-party Ad Hoc Teamwork",
    "date": "2025-10-29",
    "tags": [
      "cs.MA",
      "MA",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Beiwen Zhang",
      "Yongheng Liang",
      "Hejun Wu"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25340v1",
    "abstract": "Multi-agent reinforcement learning (MARl) has achieved strong results in cooperative tasks but typically assumes fixed, fully controlled teams. Ad hoc teamwork (AHT) relaxes this by allowing collaboration with unknown partners, yet existing variants still presume shared conventions. We introduce Multil-party Ad Hoc Teamwork (MAHT), where controlled agents must coordinate with multiple mutually unfamiliar groups of uncontrolled teammates. To address this, we propose MARs, which builds a sparse skeleton graph and applies relational modeling to capture cross-group dvnamics. Experiments on MPE and starCralt ll show that MARs outperforms MARL and AHT baselines while converging faster.",
    "fetched_at": "2025-11-05T02:19:02.085448Z"
  },
  {
    "id": "2510.25445v1",
    "title": "Agentic AI: A Comprehensive Survey of Architectures, Applications, and   Future Directions",
    "date": "2025-10-29",
    "tags": [
      "cs.AI",
      "AI",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Mohamad Abou Ali",
      "Fadi Dornaika"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.25445v1",
    "abstract": "Agentic AI represents a transformative shift in artificial intelligence, but its rapid advancement has led to a fragmented understanding, often conflating modern neural systems with outdated symbolic models -- a practice known as conceptual retrofitting. This survey cuts through this confusion by introducing a novel dual-paradigm framework that categorizes agentic systems into two distinct lineages: the Symbolic/Classical (relying on algorithmic planning and persistent state) and the Neural/Generative (leveraging stochastic generation and prompt-driven orchestration). Through a systematic PRISMA-based review of 90 studies (2018--2025), we provide a comprehensive analysis structured around this framework across three dimensions: (1) the theoretical foundations and architectural principles defining each paradigm; (2) domain-specific implementations in healthcare, finance, and robotics, demonstrating how application constraints dictate paradigm selection; and (3) paradigm-specific ethical and governance challenges, revealing divergent risks and mitigation strategies. Our analysis reveals that the choice of paradigm is strategic: symbolic systems dominate safety-critical domains (e.g., healthcare), while neural systems prevail in adaptive, data-rich environments (e.g., finance). Furthermore, we identify critical research gaps, including a significant deficit in governance models for symbolic systems and a pressing need for hybrid neuro-symbolic architectures. The findings culminate in a strategic roadmap arguing that the future of Agentic AI lies not in the dominance of one paradigm, but in their intentional integration to create systems that are both adaptable and reliable. This work provides the essential conceptual toolkit to guide future research, development, and policy toward robust and trustworthy hybrid intelligent systems.",
    "fetched_at": "2025-11-05T02:19:02.085293Z"
  },
  {
    "id": "2510.25496v1",
    "title": "Dynamic Beamforming and Power Allocation in ISAC via Deep Reinforcement   Learning",
    "date": "2025-10-29",
    "tags": [
      "eess.SP",
      "SP"
    ],
    "authors": [
      "Duc Nguyen Dao",
      "André B. J. Kokkeler",
      "Haibin Zhang",
      "Yang Miao"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25496v1",
    "abstract": "Integrated Sensing and Communication (ISAC) is a key enabler in 6G networks, where sensing and communication capabilities are designed to complement and enhance each other. One of the main challenges in ISAC lies in resource allocation, which becomes computationally demanding in dynamic environments requiring real-time adaptation. In this paper, we propose a Deep Reinforcement Learning (DRL)-based approach for dynamic beamforming and power allocation in ISAC systems. The DRL agent interacts with the environment and learns optimal strategies through trial and error, guided by predefined rewards. Simulation results show that the DRL-based solution converges within 2000 episodes and achieves up to 80\\% of the spectral efficiency of a semidefinite relaxation (SDR) benchmark. More importantly, it offers a significant improvement in runtime performance, achieving decision times of around 20 ms compared to 4500 ms for the SDR method. Furthermore, compared with a Deep Q-Network (DQN) benchmark employing discrete beamforming, the proposed approach achieves approximately 30\\% higher sum-rate with comparable runtime. These results highlight the potential of DRL for enabling real-time, high-performance ISAC in dynamic scenarios.",
    "fetched_at": "2025-11-05T02:19:02.085241Z"
  },
  {
    "id": "2510.25529v1",
    "title": "Off-policy Reinforcement Learning with Model-based Exploration   Augmentation",
    "date": "2025-10-29",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Likun Wang",
      "Xiangteng Zhang",
      "Yinuo Wang",
      "Guojian Zhan",
      "Wenxuan Wang",
      "Haoyu Gao",
      "Jingliang Duan",
      "Shengbo Eben Li"
    ],
    "institution": "DeepMind, OpenAI, MIT",
    "link": "http://arxiv.org/pdf/2510.25529v1",
    "abstract": "Exploration is fundamental to reinforcement learning (RL), as it determines how effectively an agent discovers and exploits the underlying structure of its environment to achieve optimal performance. Existing exploration methods generally fall into two categories: active exploration and passive exploration. The former introduces stochasticity into the policy but struggles in high-dimensional environments, while the latter adaptively prioritizes transitions in the replay buffer to enhance exploration, yet remains constrained by limited sample diversity. To address the limitation in passive exploration, we propose Modelic Generative Exploration (MoGE), which augments exploration through the generation of under-explored critical states and synthesis of dynamics-consistent experiences through transition models. MoGE is composed of two components: (1) a diffusion-based generator that synthesizes critical states under the guidance of a utility function evaluating each state's potential influence on policy exploration, and (2) a one-step imagination world model for constructing critical transitions based on the critical states for agent learning. Our method adopts a modular formulation that aligns with the principles of off-policy learning, allowing seamless integration with existing algorithms to improve exploration without altering their core structures. Empirical results on OpenAI Gym and DeepMind Control Suite reveal that MoGE effectively bridges exploration and policy learning, leading to remarkable gains in both sample efficiency and performance across complex control tasks.",
    "fetched_at": "2025-11-05T02:19:02.085195Z"
  },
  {
    "id": "2510.25572v2",
    "title": "On the instability of local learning algorithms: Q-learning can fail in   infinite state spaces",
    "date": "2025-10-29",
    "tags": [
      "math.PR",
      "PR"
    ],
    "authors": [
      "Urtzi Ayesta",
      "Sergey Foss",
      "Matthieu Jonckheere",
      "Vittorio Puricelli"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.25572v2",
    "abstract": "We investigate the challenges of applying model-free reinforcement learning algorithms, like online Q-learning, to infinite state space Markov Decision Processes (MDPs). We first introduce the notion of Local Learning Processes (LLPs), where agents make decisions based solely on local information, and we show that Q-learning can be seen as a specific instance of an LLP. Using renewal techniques, we analyze LLPs and demonstrate their instability under certain drift and initial conditions, revealing fundamental limitations in infinite state spaces. In particular, we show that while asymptotically optimal in finite settings, Q-learning can face instability and strict sub-optimality in infinite spaces. Our findings are illustrated through queueing system examples drawn from load balancing and server allocation. The study underscores the need for new theoretical frameworks and suggests future research into nonlocal Q-learning variants.",
    "fetched_at": "2025-11-05T02:19:02.085132Z"
  },
  {
    "id": "2510.25650v1",
    "title": "Collision avoidance and path finding in a robotic mobile fulfillment   system using multi-objective meta-heuristics",
    "date": "2025-10-29",
    "tags": [
      "cs.RO",
      "RO",
      "math.OC",
      "OC"
    ],
    "authors": [
      "Ahmad Kokhahi",
      "Mary Kurz"
    ],
    "institution": "Meta",
    "link": "http://arxiv.org/pdf/2510.25650v1",
    "abstract": "Multi-Agent Path Finding (MAPF) has gained significant attention, with most research focusing on minimizing collisions and travel time. This paper also considers energy consumption in the path planning of automated guided vehicles (AGVs). It addresses two main challenges: i) resolving collisions between AGVs and ii) assigning tasks to AGVs. We propose a new collision avoidance strategy that takes both energy use and travel time into account. For task assignment, we present two multi-objective algorithms: Non-Dominated Sorting Genetic Algorithm (NSGA) and Adaptive Large Neighborhood Search (ALNS). Comparative evaluations show that these proposed methods perform better than existing approaches in both collision avoidance and task assignment.",
    "fetched_at": "2025-11-05T02:19:02.084959Z"
  },
  {
    "id": "2510.25679v1",
    "title": "Navigation in a Three-Dimensional Urban Flow using Deep Reinforcement   Learning",
    "date": "2025-10-29",
    "tags": [
      "cs.AI",
      "AI",
      "physics.flu-dyn",
      "flu-dyn"
    ],
    "authors": [
      "Federica Tonti",
      "Ricardo Vinuesa"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25679v1",
    "abstract": "Unmanned Aerial Vehicles (UAVs) are increasingly populating urban areas for delivery and surveillance purposes. In this work, we develop an optimal navigation strategy based on Deep Reinforcement Learning. The environment is represented by a three-dimensional high-fidelity simulation of an urban flow, characterized by turbulence and recirculation zones. The algorithm presented here is a flow-aware Proximal Policy Optimization (PPO) combined with a Gated Transformer eXtra Large (GTrXL) architecture, giving the agent richer information about the turbulent flow field in which it navigates. The results are compared with a PPO+GTrXL without the secondary prediction tasks, a PPO combined with Long Short Term Memory (LSTM) cells and a traditional navigation algorithm. The obtained results show a significant increase in the success rate (SR) and a lower crash rate (CR) compared to a PPO+LSTM, PPO+GTrXL and the classical Zermelo's navigation algorithm, paving the way to a completely reimagined UAV landscape in complex urban environments.",
    "fetched_at": "2025-11-05T02:19:02.084864Z"
  },
  {
    "id": "2510.25889v1",
    "title": "$π_\\texttt{RL}$: Online RL Fine-tuning for Flow-based   Vision-Language-Action Models",
    "date": "2025-10-29",
    "tags": [
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Kang Chen",
      "Zhihao Liu",
      "Tonghe Zhang",
      "Zhen Guo",
      "Si Xu",
      "Hao Lin",
      "Hongzhi Zang",
      "Quanlu Zhang",
      "Zhaofei Yu",
      "Guoliang Fan",
      "Tiejun Huang",
      "Yu Wang",
      "Chao Yu"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25889v1",
    "abstract": "Vision-Language-Action (VLA) models enable robots to understand and perform complex tasks from multimodal input. Although recent work explores using reinforcement learning (RL) to automate the laborious data collection process in scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based VLAs (e.g., $\\pi_0$, $\\pi_{0.5}$) remains challenging due to intractable action log-likelihoods from iterative denoising.   We address this challenge with $\\pi_{\\text{RL}}$, an open-source framework for training flow-based VLAs in parallel simulation. $\\pi_{\\text{RL}}$ implements two RL algorithms: (1) {Flow-Noise} models the denoising process as a discrete-time MDP with a learnable noise network for exact log-likelihood computation. (2) {Flow-SDE} integrates denoising with agent-environment interaction, formulating a two-layer MDP that employs ODE-to-SDE conversion for efficient RL exploration.   We evaluate $\\pi_{\\text{RL}}$ on LIBERO and ManiSkill benchmarks. On LIBERO, $\\pi_{\\text{RL}}$ boosts few-shot SFT models $\\pi_0$ and $\\pi_{0.5}$ from 57.6% to 97.6% and from 77.1% to 98.3%, respectively. In ManiSkill, we train $\\pi_{\\text{RL}}$ in 320 parallel environments, improving $\\pi_0$ from 41.6% to 85.7% and $\\pi_{0.5}$ from 40.0% to 84.8% across 4352 pick-and-place tasks, demonstrating scalable multitask RL under heterogeneous simulation.   Overall, $\\pi_{\\text{RL}}$ achieves significant performance gains and stronger generalization over SFT-models, validating the effectiveness of online RL for flow-based VLAs.",
    "fetched_at": "2025-11-05T02:19:02.084653Z"
  },
  {
    "id": "2510.25929v1",
    "title": "Multi-Agent Reinforcement Learning for Market Making: Competition   without Collusion",
    "date": "2025-10-29",
    "tags": [
      "cs.MA",
      "MA",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Ziyi Wang",
      "Carmine Ventre",
      "Maria Polukarov"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25929v1",
    "abstract": "Algorithmic collusion has emerged as a central question in AI: Will the interaction between different AI agents deployed in markets lead to collusion? More generally, understanding how emergent behavior, be it a cartel or market dominance from more advanced bots, affects the market overall is an important research question.   We propose a hierarchical multi-agent reinforcement learning framework to study algorithmic collusion in market making. The framework includes a self-interested market maker (Agent~A), which is trained in an uncertain environment shaped by an adversary, and three bottom-layer competitors: the self-interested Agent~B1 (whose objective is to maximize its own PnL), the competitive Agent~B2 (whose objective is to minimize the PnL of its opponent), and the hybrid Agent~B$^\\star$, which can modulate between the behavior of the other two. To analyze how these agents shape the behavior of each other and affect market outcomes, we propose interaction-level metrics that quantify behavioral asymmetry and system-level dynamics, while providing signals potentially indicative of emergent interaction patterns.   Experimental results show that Agent~B2 secures dominant performance in a zero-sum setting against B1, aggressively capturing order flow while tightening average spreads, thus improving market execution efficiency. In contrast, Agent~B$^\\star$ exhibits a self-interested inclination when co-existing with other profit-seeking agents, securing dominant market share through adaptive quoting, yet exerting a milder adverse impact on the rewards of Agents~A and B1 compared to B2. These findings suggest that adaptive incentive control supports more sustainable strategic co-existence in heterogeneous agent environments and offers a structured lens for evaluating behavioral design in algorithmic trading systems.",
    "fetched_at": "2025-11-05T02:19:02.084512Z"
  },
  {
    "id": "2510.25951v1",
    "title": "Estimating cognitive biases with attention-aware inverse planning",
    "date": "2025-10-29",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Sounak Banerjee",
      "Daphne Cornelisse",
      "Deepak Gopinath",
      "Emily Sumner",
      "Jonathan DeCastro",
      "Guy Rosman",
      "Eugene Vinitsky",
      "Mark K. Ho"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25951v1",
    "abstract": "People's goal-directed behaviors are influenced by their cognitive biases, and autonomous systems that interact with people should be aware of this. For example, people's attention to objects in their environment will be biased in a way that systematically affects how they perform everyday tasks such as driving to work. Here, building on recent work in computational cognitive science, we formally articulate the attention-aware inverse planning problem, in which the goal is to estimate a person's attentional biases from their actions. We demonstrate how attention-aware inverse planning systematically differs from standard inverse reinforcement learning and how cognitive biases can be inferred from behavior. Finally, we present an approach to attention-aware inverse planning that combines deep reinforcement learning with computational cognitive modeling. We use this approach to infer the attentional strategies of RL agents in real-life driving scenarios selected from the Waymo Open Dataset, demonstrating the scalability of estimating cognitive biases with attention-aware inverse planning.",
    "fetched_at": "2025-11-05T02:19:02.084462Z"
  },
  {
    "id": "2510.25092v1",
    "title": "SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In   Text-only LLMs",
    "date": "2025-10-29",
    "tags": [
      "cs.MA",
      "MA"
    ],
    "authors": [
      "Weijia Zhang",
      "Zijia Liu",
      "Haoru Li",
      "Haoqi Chen",
      "Jiaxuan You"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.25092v1",
    "abstract": "Recent advances in text-only large language models (LLMs), such as DeepSeek-R1, demonstrate remarkable reasoning ability. However, these models remain fragile or entirely incapable when extended to multi-modal tasks. Existing approaches largely rely on single-form captions, which lack diversity and often fail to adapt across different types of Visual Question Answering (VQA) benchmarks. As a result, they provide no principled or efficient channel for transmitting fine-grained visual information. We introduce Seeing Eye, a modular framework that unlocks multimodal reasoning in text-only LLMs through an agent-based small VLM translator. This translator acts as a perception agent: it can invoke specialized tools (e.g., OCR and crop) and iteratively distill multimodal inputs into structured intermediate representations (SIRs) tailored to the question. These SIRs are then passed to the text-only LLM, which serves as a reasoning agent. Crucially, the translator and reasoner engage in multi-round feedback and interaction, enabling the extraction of targeted visual details and yielding more confident answers. Experiments on knowledge-intensive VQA benchmarks, including MMMU and MIA-Bench, demonstrate that Seeing Eye not only reduces inference cost but also surpasses much larger end-to-end VLMs. For example, an instantiation combining a 3B-parameter vision translator with an 8B-parameter language reasoner outperforms a monolithic 32B VLM on challenging knowledge-based questions. Our results highlight that decoupling perception from reasoning via agent information flow offers a scalable and plug-and-play pathway to multimodal reasoning, allowing strong text-only LLMs to fully leverage their reasoning capabilities. Code is available at: https://github.com/ulab-uiuc/SeeingEye",
    "fetched_at": "2025-11-05T02:19:00.099233Z"
  },
  {
    "id": "2510.25101v1",
    "title": "KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome   Supervision for KBQA",
    "date": "2025-10-29",
    "tags": [
      "cs.AI",
      "AI",
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Zhuo Chen",
      "Fei Wang",
      "Zixuan Li",
      "Zhao Zhang",
      "Weiwei Ding",
      "Chuanguang Yang",
      "Yongjun Xu",
      "Xiaolong Jin",
      "Jiafeng Guo"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25101v1",
    "abstract": "Knowledge Base Question Answering (KBQA) aims to answer natural-language questions over a structured Knowledge Base (KB). Recent work improves KBQA by adopting an agentic reasoning paradigm, in which Large Language Models (LLMs) iteratively decompose a question, generate its corresponding logical queries, and interact with the KB to derive the answer. However, these methods typically fine-tune LLMs on reasoning trajectories synthesized via process supervision, which offers weak incentives for exploration and thus fails to strengthen the agentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that can autonomously perform agentic reasoning on KBs to obtain answers. To incentivize autonomous exploration, KnowCoder-A1 trains the LLM under outcome-only supervision via a multi-stage curriculum reinforcement learning with an easy-to-hard curriculum. To establish foundational agentic capabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of high-quality trajectories obtained through outcome-based rejection sampling. Then, to alleviate the reward sparsity inherent in outcome-only supervision, it applies multi-stage curriculum RL with reward schedules that progress from easy to hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful reasoning behaviors and consistently outperforms prior approaches across three mainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1 achieves up to an 11.1% relative improvement while using only one-twelfth of the training data, demonstrating strong agentic reasoning capabilities.",
    "fetched_at": "2025-11-05T02:19:00.099178Z"
  },
  {
    "id": "2510.25110v1",
    "title": "DEBATE: A Large-Scale Benchmark for Role-Playing LLM Agents in   Multi-Agent, Long-Form Debates",
    "date": "2025-10-29",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Yun-Shiuan Chuang",
      "Ruixuan Tu",
      "Chengtao Dai",
      "Smit Vasani",
      "Binwei Yao",
      "Michael Henry Tessler",
      "Sijia Yang",
      "Dhavan Shah",
      "Robert Hawkins",
      "Junjie Hu",
      "Timothy T. Rogers"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.25110v1",
    "abstract": "Accurately modeling opinion change through social interactions is crucial for addressing issues like misinformation and polarization. While role-playing large language models (LLMs) offer a promising way to simulate human-like interactions, existing research shows that single-agent alignment does not guarantee authentic multi-agent group dynamics. Current LLM role-play setups often produce unnatural dynamics (e.g., premature convergence), without an empirical benchmark to measure authentic human opinion trajectories. To bridge this gap, we introduce DEBATE, the first large-scale empirical benchmark explicitly designed to evaluate the authenticity of the interaction between multi-agent role-playing LLMs. DEBATE contains 29,417 messages from multi-round debate conversations among over 2,792 U.S.-based participants discussing 107 controversial topics, capturing both publicly-expressed messages and privately-reported opinions. Using DEBATE, we systematically evaluate and identify critical discrepancies between simulated and authentic group dynamics. We further demonstrate DEBATE's utility for aligning LLMs with human behavior through supervised fine-tuning, achieving improvements in surface-level metrics (e.g., ROUGE-L and message length) while highlighting limitations in deeper semantic alignment (e.g., semantic similarity). Our findings highlight both the potential and current limitations of role-playing LLM agents for realistically simulating human-like social dynamics.",
    "fetched_at": "2025-11-05T02:19:00.099107Z"
  },
  {
    "id": "2510.25160v2",
    "title": "Model-Document Protocol for AI Search",
    "date": "2025-10-29",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI",
      "cs.IR",
      "IR"
    ],
    "authors": [
      "Hongjin Qian",
      "Zheng Liu"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25160v2",
    "abstract": "AI search depends on linking large language models (LLMs) with vast external knowledge sources. Yet web pages, PDF files, and other raw documents are not inherently LLM-ready: they are long, noisy, and unstructured. Conventional retrieval methods treat these documents as verbatim text and return raw passages, leaving the burden of fragment assembly and contextual reasoning to the LLM. This gap underscores the need for a new retrieval paradigm that redefines how models interact with documents.   We introduce the Model-Document Protocol (MDP), a general framework that formalizes how raw text is bridged to LLMs through consumable knowledge representations. Rather than treating retrieval as passage fetching, MDP defines multiple pathways that transform unstructured documents into task-specific, LLM-ready inputs. These include agentic reasoning, which curates raw evidence into coherent context; memory grounding, which accumulates reusable notes to enrich reasoning; and structured leveraging, which encodes documents into formal representations such as graphs or key-value caches. All three pathways share the same goal: ensuring that what reaches the LLM is not raw fragments but compact, structured knowledge directly consumable for reasoning.   As an instantiation, we present MDP-Agent, which realizes the protocol through an agentic process: constructing document-level gist memories for global coverage, performing diffusion-based exploration with vertical exploitation to uncover layered dependencies, and applying map-reduce style synthesis to integrate large-scale evidence into compact yet sufficient context. Experiments on information-seeking benchmarks demonstrate that MDP-Agent outperforms baselines, validating both the soundness of the MDP framework and the effectiveness of its agentic instantiation.",
    "fetched_at": "2025-11-05T02:19:00.099029Z"
  },
  {
    "id": "2510.25179v1",
    "title": "Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models",
    "date": "2025-10-29",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Juan Ren",
      "Mark Dras",
      "Usman Naseem"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25179v1",
    "abstract": "Agentic methods have emerged as a powerful and autonomous paradigm that enhances reasoning, collaboration, and adaptive control, enabling systems to coordinate and independently solve complex tasks. We extend this paradigm to safety alignment by introducing Agentic Moderation, a model-agnostic framework that leverages specialised agents to defend multimodal systems against jailbreak attacks. Unlike prior approaches that apply as a static layer over inputs or outputs and provide only binary classifications (safe or unsafe), our method integrates dynamic, cooperative agents, including Shield, Responder, Evaluator, and Reflector, to achieve context-aware and interpretable moderation. Extensive experiments across five datasets and four representative Large Vision-Language Models (LVLMs) demonstrate that our approach reduces the Attack Success Rate (ASR) by 7-19%, maintains a stable Non-Following Rate (NF), and improves the Refusal Rate (RR) by 4-20%, achieving robust, interpretable, and well-balanced safety performance. By harnessing the flexibility and reasoning capacity of agentic architectures, Agentic Moderation provides modular, scalable, and fine-grained safety enforcement, highlighting the broader potential of agentic systems as a foundation for automated safety governance.",
    "fetched_at": "2025-11-05T02:19:00.098967Z"
  },
  {
    "id": "2510.25189v1",
    "title": "AgentCyTE: Leveraging Agentic AI to Generate Cybersecurity Training &   Experimentation Scenarios",
    "date": "2025-10-29",
    "tags": [
      "cs.CR",
      "CR"
    ],
    "authors": [
      "Ana M. Rodriguez",
      "Jaime Acosta",
      "Anantaa Kotal",
      "Aritran Piplai"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25189v1",
    "abstract": "Designing realistic and adaptive networked threat scenarios remains a core challenge in cybersecurity research and training, still requiring substantial manual effort. While large language models (LLMs) show promise for automated synthesis, unconstrained generation often yields configurations that fail validation or execution. We present AgentCyTE, a framework integrating LLM-based reasoning with deterministic, schema-constrained network emulation to generate and refine executable threat environments. Through an agentic feedback loop, AgentCyTE observes scenario outcomes, validates correctness, and iteratively enhances realism and consistency. This hybrid approach preserves LLM flexibility while enforcing structural validity, enabling scalable, data-driven experimentation and reliable scenario generation for threat modeling and adaptive cybersecurity training. Our framework can be accessed at: https://github.com/AnantaaKotal/AgentCyTE",
    "fetched_at": "2025-11-05T02:19:00.098923Z"
  },
  {
    "id": "2510.25223v2",
    "title": "FELA: A Multi-Agent Evolutionary System for Feature Engineering of   Industrial Event Log Data",
    "date": "2025-10-29",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Kun Ouyang",
      "Haoyu Wang",
      "Dong Fang"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.25223v2",
    "abstract": "Event log data, recording fine-grained user actions and system events, represent one of the most valuable assets for modern digital services. However, the complexity and heterogeneity of industrial event logs--characterized by large scale, high dimensionality, diverse data types, and intricate temporal or relational structures--make feature engineering extremely challenging. Existing automatic feature engineering approaches, such as AutoML or genetic methods, often suffer from limited explainability, rigid predefined operations, and poor adaptability to complicated heterogeneous data. In this paper, we propose FELA (Feature Engineering LLM Agents), a multi-agent evolutionary system that autonomously extracts meaningful and high-performing features from complex industrial event log data. FELA integrates the reasoning and coding capabilities of large language models (LLMs) with an insight-guided self-evolution paradigm. Specifically, FELA employs specialized agents--Idea Agents, Code Agents, and Critic Agents--to collaboratively generate, validate, and implement novel feature ideas. An Evaluation Agent summarizes feedback and updates a hierarchical knowledge base and dual-memory system to enable continual improvement. Moreover, FELA introduces an agentic evolution algorithm, combining reinforcement learning and genetic algorithm principles to balance exploration and exploitation across the idea space. Extensive experiments on real industrial datasets demonstrate that FELA can generate explainable, domain-relevant features that significantly improve model performance while reducing manual effort. Our results highlight the potential of LLM-based multi-agent systems as a general framework for automated, interpretable, and adaptive feature engineering in complex real-world environments.",
    "fetched_at": "2025-11-05T02:19:00.098879Z"
  },
  {
    "id": "2510.25224v1",
    "title": "ProMediate: A Socio-cognitive framework for evaluating proactive agents   in multi-party negotiation",
    "date": "2025-10-29",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Ziyi Liu",
      "Bahar Sarrafzadeh",
      "Pei Zhou",
      "Longqi Yang",
      "Jieyu Zhao",
      "Ashish Sharma"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.25224v1",
    "abstract": "While Large Language Models (LLMs) are increasingly used in agentic frameworks to assist individual users, there is a growing need for agents that can proactively manage complex, multi-party collaboration. Systematic evaluation methods for such proactive agents remain scarce, limiting progress in developing AI that can effectively support multiple people together. Negotiation offers a demanding testbed for this challenge, requiring socio-cognitive intelligence to navigate conflicting interests between multiple participants and multiple topics and build consensus. Here, we present ProMediate, the first framework for evaluating proactive AI mediator agents in complex, multi-topic, multi-party negotiations. ProMediate consists of two core components: (i) a simulation testbed based on realistic negotiation cases and theory-driven difficulty levels (ProMediate-Easy, ProMediate-Medium, and ProMediate-Hard), with a plug-and-play proactive AI mediator grounded in socio-cognitive mediation theories, capable of flexibly deciding when and how to intervene; and (ii) a socio-cognitive evaluation framework with a new suite of metrics to measure consensus changes, intervention latency, mediator effectiveness, and intelligence. Together, these components establish a systematic framework for assessing the socio-cognitive intelligence of proactive AI agents in multi-party settings. Our results show that a socially intelligent mediator agent outperforms a generic baseline, via faster, better-targeted interventions. In the ProMediate-Hard setting, our social mediator increases consensus change by 3.6 percentage points compared to the generic baseline (10.65\\% vs 7.01\\%) while being 77\\% faster in response (15.98s vs. 3.71s). In conclusion, ProMediate provides a rigorous, theory-grounded testbed to advance the development of proactive, socially intelligent agents.",
    "fetched_at": "2025-11-05T02:19:00.098829Z"
  },
  {
    "id": "2510.25320v1",
    "title": "GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement   Learning",
    "date": "2025-10-29",
    "tags": [
      "cs.AI",
      "AI",
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Jiaqi Wu",
      "Qinlao Zhao",
      "Zefeng Chen",
      "Kai Qin",
      "Yifei Zhao",
      "Xueqian Wang",
      "Yuhang Yao"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25320v1",
    "abstract": "Autonomous agents powered by large language models (LLMs) have shown impressive capabilities in tool manipulation for complex task-solving. However, existing paradigms such as ReAct rely on sequential reasoning and execution, failing to exploit the inherent parallelism among independent sub-tasks. This sequential bottleneck leads to inefficient tool utilization and suboptimal performance in multi-step reasoning scenarios. We introduce Graph-based Agent Planning (GAP), a novel framework that explicitly models inter-task dependencies through graph-based planning to enable adaptive parallel and serial tool execution. Our approach trains agent foundation models to decompose complex tasks into dependency-aware sub-task graphs, autonomously determining which tools can be executed in parallel and which must follow sequential dependencies. This dependency-aware orchestration achieves substantial improvements in both execution efficiency and task accuracy. To train GAP, we construct a high-quality dataset of graph-based planning traces derived from the Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage training strategy: supervised fine-tuning (SFT) on the curated dataset, followed by reinforcement learning (RL) with a correctness-based reward function on strategically sampled queries where tool-based reasoning provides maximum value. Experimental results on MHQA datasets demonstrate that GAP significantly outperforms traditional ReAct baselines, particularly on multi-step retrieval tasks, while achieving dramatic improvements in tool invocation efficiency through intelligent parallelization. The project page is available at: https://github.com/WJQ7777/Graph-Agent-Planning.",
    "fetched_at": "2025-11-05T02:19:00.098764Z"
  },
  {
    "id": "2510.25333v1",
    "title": "CRMWeaver: Building Powerful Business Agent via Agentic RL and Shared   Memories",
    "date": "2025-10-29",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Yilong Lai",
      "Yipin Yang",
      "Jialong Wu",
      "Fengran Mo",
      "Zhenglin Wang",
      "Ting Liang",
      "Jianguo Lin",
      "Keping Yang"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25333v1",
    "abstract": "Recent years have witnessed the rapid development of LLM-based agents, which shed light on using language agents to solve complex real-world problems. A prominent application lies in business agents, which interact with databases and internal knowledge bases via tool calls to fulfill diverse user requirements. However, this domain is characterized by intricate data relationships and a wide range of heterogeneous tasks, from statistical data queries to knowledge-based question-answering. To address these challenges, we propose CRMWeaver, a novel approach that enhances business agents in such complex settings. To acclimate the agentic model to intricate business environments, we employ a synthesis data generation and RL-based paradigm during training, which significantly improves the model's ability to handle complex data and varied tasks. During inference, a shared memories mechanism is introduced, prompting the agent to learn from task guidelines in similar problems, thereby further boosting its effectiveness and generalization, especially in unseen scenarios. We validate the efficacy of our approach on the CRMArena-Pro dataset, where our lightweight model achieves competitive results in both B2B and B2C business scenarios, underscoring its practical value for real-world applications.",
    "fetched_at": "2025-11-05T02:19:00.098702Z"
  },
  {
    "id": "2510.25381v2",
    "title": "CGM-Led Multimodal Tracking with Chatbot Support: An Autoethnography in   Sub-Health",
    "date": "2025-10-29",
    "tags": [
      "cs.HC",
      "HC"
    ],
    "authors": [
      "Dongyijie Primo Pan",
      "Lan Luo",
      "Yike Wang",
      "Pan Hui"
    ],
    "institution": "Meta",
    "link": "http://arxiv.org/pdf/2510.25381v2",
    "abstract": "Metabolic disorders present a pressing global health challenge, with China carrying the world's largest burden. While continuous glucose monitoring (CGM) has transformed diabetes care, its potential for supporting sub-health populations -- such as individuals who are overweight, prediabetic, or anxious -- remains underexplored. At the same time, large language models (LLMs) are increasingly used in health coaching, yet CGM is rarely incorporated as a first-class signal. To address this gap, we conducted a six-week autoethnography, combining CGM with multimodal indicators captured via common digital devices and a chatbot that offered personalized reflections and explanations of glucose fluctuations. Our findings show how CGM-led, data-first multimodal tracking, coupled with conversational support, shaped everyday practices of diet, activity, stress, and wellbeing. This work contributes to HCI by extending CGM research beyond clinical diabetes and demonstrating how LLM-driven agents can support preventive health and reflection in at-risk populations.",
    "fetched_at": "2025-11-05T02:19:00.098641Z"
  },
  {
    "id": "2510.25421v1",
    "title": "Small Talk, Big Impact? LLM-based Conversational Agents to Mitigate   Passive Fatigue in Conditional Automated Driving",
    "date": "2025-10-29",
    "tags": [
      "cs.HC",
      "HC"
    ],
    "authors": [
      "Lewis Cockram",
      "Yueteng Yu",
      "Jorge Pardo",
      "Xiaomeng Li",
      "Andry Rakotonirainy",
      "Jonny Kuo",
      "Sebastien Demmel",
      "Mike Lenné",
      "Ronald Schroeter"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.25421v1",
    "abstract": "Passive fatigue during conditional automated driving can compromise driver readiness and safety. This paper presents findings from a test-track study with 40 participants in a real-world rural automated driving scenario. In this scenario, a Large Language Model (LLM) based conversational agent (CA) was designed to check in with drivers and re-engage them with their surroundings. Drawing on in-car video recordings, sleepiness ratings and interviews, we analysed how drivers interacted with the agent and how these interactions shaped alertness. Users found the CA helpful for supporting vigilance during passive fatigue. Thematic analysis of acceptability further revealed three user preference profiles that implicate future intention to use CAs. Positioning empirically observed profiles within existing CA archetype frameworks highlights the need for adaptive design sensitive to diverse user groups. This work underscores the potential of CAs as proactive Human-Machine Interface (HMI) interventions, demonstrating how natural language can support context-aware interaction during automated driving.",
    "fetched_at": "2025-11-05T02:19:00.098596Z"
  },
  {
    "id": "2510.25423v1",
    "title": "What Challenges Do Developers Face in AI Agent Systems? An Empirical   Study on Stack Overflow",
    "date": "2025-10-29",
    "tags": [
      "cs.SE",
      "SE"
    ],
    "authors": [
      "Ali Asgari",
      "Annibale Panichella",
      "Pouria Derakhshanfar",
      "Mitchell Olsthoorn"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25423v1",
    "abstract": "AI agents have rapidly gained popularity across research and industry as systems that extend large language models with additional capabilities to plan, use tools, remember, and act toward specific goals. Yet despite their promise, developers face persistent and often underexplored challenges when building, deploying, and maintaining these emerging systems. To identify these challenges, we study developer discussions on Stack Overflow, the world's largest developer-focused Q and A platform with about 60 million questions and answers and 30 million users. We construct a taxonomy of developer challenges through tag expansion and filtering, apply LDA-MALLET for topic modeling, and manually validate and label the resulting themes. Our analysis reveals seven major areas of recurring issues encompassing 77 distinct technical challenges related to runtime integration, dependency management, orchestration complexity, and evaluation reliability. We further quantify topic popularity and difficulty to identify which issues are most common and hardest to resolve, map the tools and programming languages used in agent development, and track their evolution from 2021 to 2025 in relation to major AI model and framework releases. Finally, we present the implications of our results, offering concrete guidance for practitioners, researchers, and educators on agent reliability and developer support.",
    "fetched_at": "2025-11-05T02:19:00.098533Z"
  },
  {
    "id": "2510.25441v1",
    "title": "Grounded in Reality: Learning and Deploying Proactive LLM from Offline   Logs",
    "date": "2025-10-29",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Fei Wei",
      "Daoyuan Chen",
      "Ce Wang",
      "Yilun Huang",
      "Yushuo Chen",
      "Xuchen Pan",
      "Yaliang Li",
      "Bolin Ding"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25441v1",
    "abstract": "Large Language Models (LLMs) excel as passive responders, but teaching them to be proactive, goal-oriented partners, a critical capability in high-stakes domains, remains a major challenge. Current paradigms either myopically optimize single-turn attributes or rely on brittle, high-cost user simulators, creating a persistent ``reality gap''. To bridge this gap, we introduce \\texttt{Learn-to-Ask}, a general, simulator-free framework for learning and deploying proactive dialogue agents \\textit{directly from offline expert data}, bypassing the need to model complex user dynamics. Our key insight is to reframe the offline policy learning problem by leveraging the \\textbf{observed future} of each expert trajectory. This allows us to infer a dense, turn-by-turn reward signal grounded in the expert's revealed strategy, decomposing the intractable long-horizon problem into a series of supervised learning tasks, and training a policy to output a structured \\texttt{(action, state_assessment)} tuple, governing both \\textbf{what to ask} and, crucially, \\textbf{when to stop}. To ensure reward fidelity, our Automated Grader Calibration pipeline systematically purges noise from the LLM-based reward model with minimal human supervision. Empirically, we demonstrate the efficacy of \\texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying sizes up to 32B. Our approach culminates in the successful deployment of LLMs into a live, large-scale online AI service. In rigorous in-house evaluations, our model was launched and achieved performance even superior to human experts, proving our framework's ability to translate offline data into tangible, real-world impact. We hope this work provides a practical and economically viable blueprint for transforming passive LLMs into proactive, goal-oriented LLM applications.",
    "fetched_at": "2025-11-05T02:19:00.098484Z"
  },
  {
    "id": "2510.25588v1",
    "title": "Standardization of Psychiatric Diagnoses -- Role of Fine-tuned LLM   Consortium and OpenAI-gpt-oss Reasoning LLM Enabled Decision Support System",
    "date": "2025-10-29",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Eranga Bandara",
      "Ross Gore",
      "Atmaram Yarlagadda",
      "Anita H. Clayton",
      "Preston Samuel",
      "Christopher K. Rhea",
      "Sachin Shetty"
    ],
    "institution": "OpenAI",
    "link": "http://arxiv.org/pdf/2510.25588v1",
    "abstract": "The diagnosis of most mental disorders, including psychiatric evaluations, primarily depends on dialogues between psychiatrists and patients. This subjective process can lead to variability in diagnoses across clinicians and patients, resulting in inconsistencies and challenges in achieving reliable outcomes. To address these issues and standardize psychiatric diagnoses, we propose a Fine-Tuned Large Language Model (LLM) Consortium and OpenAI-gpt-oss Reasoning LLM-enabled Decision Support System for the clinical diagnosis of mental disorders. Our approach leverages fine-tuned LLMs trained on conversational datasets involving psychiatrist-patient interactions focused on mental health conditions (e.g., depression). The diagnostic predictions from individual models are aggregated through a consensus-based decision-making process, refined by the OpenAI-gpt-oss reasoning LLM. We propose a novel method for deploying LLM agents that orchestrate communication between the LLM consortium and the reasoning LLM, ensuring transparency, reliability, and responsible AI across the entire diagnostic workflow. Experimental results demonstrate the transformative potential of combining fine-tuned LLMs with a reasoning model to create a robust and highly accurate diagnostic system for mental health assessment. A prototype of the proposed platform, integrating three fine-tuned LLMs with the OpenAI-gpt-oss reasoning LLM, was developed in collaboration with the U.S. Army Medical Research Team in Norfolk, Virginia, USA. To the best of our knowledge, this work represents the first application of a fine-tuned LLM consortium integrated with a reasoning LLM for clinical mental health diagnosis paving the way for next-generation AI-powered eHealth systems aimed at standardizing psychiatric diagnoses.",
    "fetched_at": "2025-11-05T02:19:00.098417Z"
  },
  {
    "id": "2510.25595v1",
    "title": "Communication and Verification in LLM Agents towards Collaboration under   Information Asymmetry",
    "date": "2025-10-29",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Run Peng",
      "Ziqiao Ma",
      "Amy Pang",
      "Sikai Li",
      "Zhang Xi-Jia",
      "Yingzhuo Yu",
      "Cristian-Paul Bara",
      "Joyce Chai"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.25595v1",
    "abstract": "While Large Language Model (LLM) agents are often approached from the angle of action planning/generation to accomplish a goal (e.g., given by language descriptions), their abilities to collaborate with each other to achieve a joint goal are not well explored. To address this limitation, this paper studies LLM agents in task collaboration, particularly under the condition of information asymmetry, where agents have disparities in their knowledge and skills and need to work together to complete a shared task. We extend Einstein Puzzles, a classical symbolic puzzle, to a table-top game. In this game, two LLM agents must reason, communicate, and act to satisfy spatial and relational constraints required to solve the puzzle. We apply a fine-tuning-plus-verifier framework in which LLM agents are equipped with various communication strategies and verification signals from the environment. Empirical results highlight the critical importance of aligned communication, especially when agents possess both information-seeking and -providing capabilities. Interestingly, agents without communication can still achieve high task performance; however, further analysis reveals a lack of true rule understanding and lower trust from human evaluators. Instead, by integrating an environment-based verifier, we enhance agents' ability to comprehend task rules and complete tasks, promoting both safer and more interpretable collaboration in AI systems. https://github.com/Roihn/EinsteinPuzzles",
    "fetched_at": "2025-11-05T02:19:00.098355Z"
  },
  {
    "id": "2510.25612v1",
    "title": "Counterfactual-based Agent Influence Ranker for Agentic AI Workflows",
    "date": "2025-10-29",
    "tags": [
      "cs.AI",
      "AI",
      "cs.MA",
      "MA"
    ],
    "authors": [
      "Amit Giloni",
      "Chiara Picardi",
      "Roy Betser",
      "Shamik Bose",
      "Aishvariya Priya Rathina Sabapathy",
      "Roman Vainshtein"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25612v1",
    "abstract": "An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system, is an autonomous system that assembles several LLM-based agents to work collaboratively towards a shared goal. The high autonomy, widespread adoption, and growing interest in such AAWs highlight the need for a deeper understanding of their operations, from both quality and security aspects. To this day, there are no existing methods to assess the influence of each agent on the AAW's final output. Adopting techniques from related fields is not feasible since existing methods perform only static structural analysis, which is unsuitable for inference time execution. We present Counterfactual-based Agent Influence Ranker (CAIR) - the first method for assessing the influence level of each agent on the AAW's output and determining which agents are the most influential. By performing counterfactual analysis, CAIR provides a task-agnostic analysis that can be used both offline and at inference time. We evaluate CAIR using an AAWs dataset of our creation, containing 30 different use cases with 230 different functionalities. Our evaluation showed that CAIR produces consistent rankings, outperforms baseline methods, and can easily enhance the effectiveness and relevancy of downstream tasks.",
    "fetched_at": "2025-11-05T02:19:00.098292Z"
  },
  {
    "id": "2510.25616v1",
    "title": "Don't Blind Your VLA: Aligning Visual Representations for OOD   Generalization",
    "date": "2025-10-29",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI",
      "cs.RO",
      "RO"
    ],
    "authors": [
      "Nikita Kachaev",
      "Mikhail Kolosov",
      "Daniil Zelezetsky",
      "Alexey K. Kovalev",
      "Aleksandr I. Panov"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.25616v1",
    "abstract": "The growing success of Vision-Language-Action (VLA) models stems from the promise that pretrained Vision-Language Models (VLMs) can endow agents with transferable world knowledge and vision-language (VL) grounding, laying a foundation for action models with broader generalization. Yet when these VLMs are adapted to the action modality, it remains unclear to what extent their original VL representations and knowledge are preserved. In this work, we conduct a systematic study of representation retention during VLA fine-tuning, showing that naive action fine-tuning leads to degradation of visual representations. To characterize and measure these effects, we probe VLA's hidden representations and analyze attention maps, further, we design a set of targeted tasks and methods that contrast VLA models with their counterpart VLMs, isolating changes in VL capabilities induced by action fine-tuning. We further evaluate a range of strategies for aligning visual representations and introduce a simple yet effective method that mitigates degradation and yields improved generalization to out-of-distribution (OOD) scenarios. Taken together, our analysis clarifies the trade-off between action fine-tuning and the degradation of VL representations and highlights practical approaches to recover inherited VL capabilities. Code is publicly available: https://blind-vla-paper.github.io",
    "fetched_at": "2025-11-05T02:19:00.098233Z"
  },
  {
    "id": "2510.25668v1",
    "title": "ALDEN: Reinforcement Learning for Active Navigation and Evidence   Gathering in Long Documents",
    "date": "2025-10-29",
    "tags": [
      "cs.AI",
      "AI",
      "cs.MM",
      "MM"
    ],
    "authors": [
      "Tianyu Yang",
      "Terry Ruas",
      "Yijun Tian",
      "Jan Philip Wahle",
      "Daniel Kurzawe",
      "Bela Gipp"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25668v1",
    "abstract": "Vision-language models (VLMs) excel at interpreting text-rich images but struggle with long, visually complex documents that demand analysis and integration of information spread across multiple pages. Existing approaches typically rely on fixed reasoning templates or rigid pipelines, which force VLMs into a passive role and hinder both efficiency and generalization. We present Active Long-DocumEnt Navigation (ALDEN), a multi-turn reinforcement learning framework that fine-tunes VLMs as interactive agents capable of actively navigating long, visually rich documents. ALDEN introduces a novel fetch action that directly accesses the page by index, complementing the classic search action and better exploiting document structure. For dense process supervision and efficient training, we propose a rule-based cross-level reward that provides both turn- and token-level signals. To address the empirically observed training instability caused by numerous visual tokens from long documents, we further propose a visual-semantic anchoring mechanism that applies a dual-path KL-divergence constraint to stabilize visual and textual representations separately during training. Trained on a corpus constructed from three open-source datasets, ALDEN achieves state-of-the-art performance on five long-document benchmarks. Overall, ALDEN marks a step beyond passive document reading toward agents that autonomously navigate and reason across long, visually rich documents, offering a robust path to more accurate and efficient long-document understanding.",
    "fetched_at": "2025-11-05T02:19:00.098179Z"
  },
  {
    "id": "2510.25694v1",
    "title": "Process-Level Trajectory Evaluation for Environment Configuration in   Software Engineering Agents",
    "date": "2025-10-29",
    "tags": [
      "cs.SE",
      "SE",
      "cs.AI",
      "AI",
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Jiayi Kuang",
      "Yinghui Li",
      "Xin Zhang",
      "Yangning Li",
      "Di Yin",
      "Xing Sun",
      "Ying Shen",
      "Philip S. Yu"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.25694v1",
    "abstract": "Large language model-based agents show promise for software engineering, but environment configuration remains a bottleneck due to heavy manual effort and scarce large-scale, high-quality datasets. Existing benchmarks assess only end-to-end build/test success, obscuring where and why agents succeed or fail. We introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench, which provides process-level trajectory assessment of fine-grained agent capabilities during environment setup-planning, perception-driven error diagnosis, feedback-driven repair, and action to execute final environment configuration. Our task instances are automatically constructed by injecting realistic README errors and are validated in Docker for scalable, high-quality evaluation. Enconda-bench combines process-level analysis with end-to-end executability to enable capability assessments beyond aggregate success rates. Evaluations across state-of-the-art LLMs and agent frameworks show that while agents can localize errors, they struggle to translate feedback into effective corrections, limiting end-to-end performance. To our knowledge, Enconda-bench is the first framework to provide process-level internal capability assessment for environment configuration, offering actionable insights for improving software engineering agents.",
    "fetched_at": "2025-11-05T02:19:00.098119Z"
  },
  {
    "id": "2510.26832v1",
    "title": "Simulating hashtag dynamics with networked groups of generative agents",
    "date": "2025-10-29",
    "tags": [
      "cs.SI",
      "SI"
    ],
    "authors": [
      "Abha Jha",
      "J. Hunter Priniski",
      "Carolyn Steinle",
      "Fred Morstatter"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.26832v1",
    "abstract": "Networked environments shape how information embedded in narratives influences individual and group beliefs and behavior. This raises key questions about how group communication around narrative media impacts belief formation and how such mechanisms contribute to the emergence of consensus or polarization. Language data from generative agents offer insight into how naturalistic forms of narrative interactions (such as hashtag generation) evolve in response to social rewards within networked communication settings. To investigate this, we developed an agent-based modeling and simulation framework composed of networks of interacting Large Language Model (LLM) agents. We benchmarked the simulations of four state-of-the-art LLMs against human group behaviors observed in a prior network experiment (Study 1) and against naturally occurring hashtags from Twitter (Study 2). Quantitative metrics of network coherence (e.g., entropy of a group's responses) reveal that while LLMs can approximate human-like coherence in sanitized domains (Study 1's experimental data), effective integration of background knowledge and social context in more complex or politically sensitive narratives likely requires careful and structured prompting.",
    "fetched_at": "2025-11-05T02:19:00.098050Z"
  },
  {
    "id": "2510.25743v1",
    "title": "Agentic Economic Modeling",
    "date": "2025-10-29",
    "tags": [
      "econ.EM",
      "EM"
    ],
    "authors": [
      "Bohan Zhang",
      "Jiaxuan Li",
      "Ali Hortaçsu",
      "Xiaoyang Ye",
      "Victor Chernozhukov",
      "Angelo Ni",
      "Edward Huang"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25743v1",
    "abstract": "We introduce Agentic Economic Modeling (AEM), a framework that aligns synthetic LLM choices with small-sample human evidence for reliable econometric inference. AEM first generates task-conditioned synthetic choices via LLMs, then learns a bias-correction mapping from task features and raw LLM choices to human-aligned choices, upon which standard econometric estimators perform inference to recover demand elasticities and treatment effects.We validate AEM in two experiments. In a large scale conjoint study with millions of observations, using only 10% of the original data to fit the correction model lowers the error of the demand-parameter estimates, while uncorrected LLM choices even increase the errors. In a regional field experiment, a mixture model calibrated on 10% of geographic regions estimates an out-of-domain treatment effect of -65\\pm10 bps, closely matching the full human experiment (-60\\pm8 bps).Under time-wise extrapolation, training with only day-one human data yields -24 bps (95% CI: [-26, -22], p<1e-5),improving over the human-only day-one baseline (-17 bps, 95% CI: [-43, +9], p=0.2049).These results demonstrate AEM's potential to improve RCT efficiency and establish a foundation method for LLM-based counterfactual generation.",
    "fetched_at": "2025-11-05T02:19:00.097964Z"
  },
  {
    "id": "2510.25758v1",
    "title": "TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological   Counseling",
    "date": "2025-10-29",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "He Hu",
      "Yucheng Zhou",
      "Chiyuan Ma",
      "Qianning Wang",
      "Zheng Zhang",
      "Fei Ma",
      "Laizhong Cui",
      "Qi Tian"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25758v1",
    "abstract": "Large language models (LLMs) in psychological counseling have attracted increasing attention. However, existing approaches often lack emotional understanding, adaptive strategies, and the use of therapeutic methods across multiple sessions with long-term memory, leaving them far from real clinical practice. To address these critical gaps, we introduce TheraMind, a strategic and adaptive agent for longitudinal psychological counseling. The cornerstone of TheraMind is a novel dual-loop architecture that decouples the complex counseling process into an Intra-Session Loop for tactical dialogue management and a Cross-Session Loop for strategic therapeutic planning. The Intra-Session Loop perceives the patient's emotional state to dynamically select response strategies while leveraging cross-session memory to ensure continuity. Crucially, the Cross-Session Loop empowers the agent with long-term adaptability by evaluating the efficacy of the applied therapy after each session and adjusting the method for subsequent interactions. We validate our approach in a high-fidelity simulation environment grounded in real clinical cases. Extensive evaluations show that TheraMind outperforms other methods, especially on multi-session metrics like Coherence, Flexibility, and Therapeutic Attunement, validating the effectiveness of its dual-loop design in emulating strategic, adaptive, and longitudinal therapeutic behavior. The code is publicly available at https://0mwwm0.github.io/TheraMind/.",
    "fetched_at": "2025-11-05T02:19:00.097907Z"
  },
  {
    "id": "2510.25850v1",
    "title": "Debate2Create: Robot Co-design via Large Language Model Debates",
    "date": "2025-10-29",
    "tags": [
      "cs.RO",
      "RO",
      "cs.LG",
      "LG",
      "cs.MA",
      "MA"
    ],
    "authors": [
      "Kevin Qiu",
      "Marek Cygan"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25850v1",
    "abstract": "Automating the co-design of a robot's morphology and control is a long-standing challenge due to the vast design space and the tight coupling between body and behavior. We introduce Debate2Create (D2C), a framework in which large language model (LLM) agents engage in a structured dialectical debate to jointly optimize a robot's design and its reward function. In each round, a design agent proposes targeted morphological modifications, and a control agent devises a reward function tailored to exploit the new design. A panel of pluralistic judges then evaluates the design-control pair in simulation and provides feedback that guides the next round of debate. Through iterative debates, the agents progressively refine their proposals, producing increasingly effective robot designs. Notably, D2C yields diverse and specialized morphologies despite no explicit diversity objective. On a quadruped locomotion benchmark, D2C discovers designs that travel 73% farther than the default, demonstrating that structured LLM-based debate can serve as a powerful mechanism for emergent robot co-design. Our results suggest that multi-agent debate, when coupled with physics-grounded feedback, is a promising new paradigm for automated robot design.",
    "fetched_at": "2025-11-05T02:19:00.097843Z"
  },
  {
    "id": "2510.25863v2",
    "title": "AAGATE: A NIST AI RMF-Aligned Governance Platform for Agentic AI",
    "date": "2025-10-29",
    "tags": [
      "cs.CR",
      "CR",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Ken Huang",
      "Kyriakos Rock Lambros",
      "Jerry Huang",
      "Yasir Mehmood",
      "Hammad Atta",
      "Joshua Beck",
      "Vineeth Sai Narajala",
      "Muhammad Zeeshan Baig",
      "Muhammad Aziz Ul Haq",
      "Nadeem Shahzad",
      "Bhavya Gupta"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.25863v2",
    "abstract": "This paper introduces the Agentic AI Governance Assurance & Trust Engine (AAGATE), a Kubernetes-native control plane designed to address the unique security and governance challenges posed by autonomous, language-model-driven agents in production. Recognizing the limitations of traditional Application Security (AppSec) tooling for improvisational, machine-speed systems, AAGATE operationalizes the NIST AI Risk Management Framework (AI RMF). It integrates specialized security frameworks for each RMF function: the Agentic AI Threat Modeling MAESTRO framework for Map, a hybrid of OWASP's AIVSS and SEI's SSVC for Measure, and the Cloud Security Alliance's Agentic AI Red Teaming Guide for Manage. By incorporating a zero-trust service mesh, an explainable policy engine, behavioral analytics, and decentralized accountability hooks, AAGATE provides a continuous, verifiable governance solution for agentic AI, enabling safe, accountable, and scalable deployment. The framework is further extended with DIRF for digital identity rights, LPCI defenses for logic-layer injection, and QSAF monitors for cognitive degradation, ensuring governance spans systemic, adversarial, and ethical risks.",
    "fetched_at": "2025-11-05T02:19:00.097802Z"
  },
  {
    "id": "2510.25914v1",
    "title": "FinOps Agent -- A Use-Case for IT Infrastructure and Cost Optimization",
    "date": "2025-10-29",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Ngoc Phuoc An Vo",
      "Manish Kesarwani",
      "Ruchi Mahindru",
      "Chandrasekhar Narayanaswami"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25914v1",
    "abstract": "FinOps (Finance + Operations) represents an operational framework and cultural practice which maximizes cloud business value through collaborative financial accountability across engineering, finance, and business teams. FinOps practitioners face a fundamental challenge: billing data arrives in heterogeneous formats, taxonomies, and metrics from multiple cloud providers and internal systems which eventually lead to synthesizing actionable insights, and making time-sensitive decisions. To address this challenge, we propose leveraging autonomous, goal-driven AI agents for FinOps automation. In this paper, we built a FinOps agent for a typical use-case for IT infrastructure and cost optimization. We built a system simulating a realistic end-to-end industry process starting with retrieving data from various sources to consolidating and analyzing the data to generate recommendations for optimization. We defined a set of metrics to evaluate our agent using several open-source and close-source language models and it shows that the agent was able to understand, plan, and execute tasks as well as an actual FinOps practitioner.",
    "fetched_at": "2025-11-05T02:19:00.097728Z"
  },
  {
    "id": "2510.25941v1",
    "title": "RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic   Pipeline",
    "date": "2025-10-29",
    "tags": [
      "cs.CL",
      "CL",
      "I.2",
      "2"
    ],
    "authors": [
      "André V. Duarte",
      "Xuying li",
      "Bin Zeng",
      "Arlindo L. Oliveira",
      "Lei Li",
      "Zhuo Li"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25941v1",
    "abstract": "If we cannot inspect the training data of a large language model (LLM), how can we ever know what it has seen? We believe the most compelling evidence arises when the model itself freely reproduces the target content. As such, we propose RECAP, an agentic pipeline designed to elicit and verify memorized training data from LLM outputs. At the heart of RECAP is a feedback-driven loop, where an initial extraction attempt is evaluated by a secondary language model, which compares the output against a reference passage and identifies discrepancies. These are then translated into minimal correction hints, which are fed back into the target model to guide subsequent generations. In addition, to address alignment-induced refusals, RECAP includes a jailbreaking module that detects and overcomes such barriers. We evaluate RECAP on EchoTrace, a new benchmark spanning over 30 full books, and the results show that RECAP leads to substantial gains over single-iteration approaches. For instance, with GPT-4.1, the average ROUGE-L score for the copyrighted text extraction improved from 0.38 to 0.47 - a nearly 24% increase.",
    "fetched_at": "2025-11-05T02:19:00.097678Z"
  },
  {
    "id": "2510.25992v1",
    "title": "Supervised Reinforcement Learning: From Expert Trajectories to Step-wise   Reasoning",
    "date": "2025-10-29",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Yihe Deng",
      "I-Hung Hsu",
      "Jun Yan",
      "Zifeng Wang",
      "Rujun Han",
      "Gufeng Zhang",
      "Yanfei Chen",
      "Wei Wang",
      "Tomas Pfister",
      "Chen-Yu Lee"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.25992v1",
    "abstract": "Large Language Models (LLMs) often struggle with problems that require multi-step reasoning. For small-scale open-source models, Reinforcement Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to overfit long demonstrations through rigid token-by-token imitation. To address this gap, we propose Supervised Reinforcement Learning (SRL), a framework that reformulates problem solving as generating a sequence of logical \"actions\". SRL trains the model to generate an internal reasoning monologue before committing to each action. It provides smoother rewards based on the similarity between the model's actions and expert actions extracted from the SFT dataset in a step-wise manner. This supervision offers richer learning signals even when all rollouts are incorrect, while encouraging flexible reasoning guided by expert demonstrations. As a result, SRL enables small models to learn challenging problems previously unlearnable by SFT or RLVR. Moreover, initializing training with SRL before refining with RLVR yields the strongest overall performance. Beyond reasoning benchmarks, SRL generalizes effectively to agentic software engineering tasks, establishing it as a robust and versatile training framework for reasoning-oriented LLMs.",
    "fetched_at": "2025-11-05T02:19:00.097624Z"
  },
  {
    "id": "2510.25997v1",
    "title": "From Queries to Insights: Agentic LLM Pipelines for Spatio-Temporal   Text-to-SQL",
    "date": "2025-10-29",
    "tags": [
      "cs.AI",
      "AI",
      "cs.SE",
      "SE"
    ],
    "authors": [
      "Manu Redd",
      "Tao Zhe",
      "Dongjie Wang"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.25997v1",
    "abstract": "Natural-language-to-SQL (NL-to-SQL) systems hold promise for democratizing access to structured data, allowing users to query databases without learning SQL. Yet existing systems struggle with realistic spatio-temporal queries, where success requires aligning vague user phrasing with schema-specific categories, handling temporal reasoning, and choosing appropriate outputs. We present an agentic pipeline that extends a naive text-to-SQL baseline (llama-3-sqlcoder-8b) with orchestration by a Mistral-based ReAct agent. The agent can plan, decompose, and adapt queries through schema inspection, SQL generation, execution, and visualization tools. We evaluate on 35 natural-language queries over the NYC and Tokyo check-in dataset, covering spatial, temporal, and multi-dataset reasoning. The agent achieves substantially higher accuracy than the naive baseline 91.4% vs. 28.6% and enhances usability through maps, plots, and structured natural-language summaries. Crucially, our design enables more natural human-database interaction, supporting users who lack SQL expertise, detailed schema knowledge, or prompting skill. We conclude that agentic orchestration, rather than stronger SQL generators alone, is a promising foundation for interactive geospatial assistants.",
    "fetched_at": "2025-11-05T02:19:00.097554Z"
  },
  {
    "id": "2510.24698v1",
    "title": "ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking",
    "date": "2025-10-28",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Baixuan Li",
      "Dingchu Zhang",
      "Jialong Wu",
      "Wenbiao Yin",
      "Zhengwei Tao",
      "Yida Zhao",
      "Liwen Zhang",
      "Haiyang Shen",
      "Runnan Fang",
      "Pengjun Xie",
      "Jingren Zhou",
      "Yong Jiang"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.24698v1",
    "abstract": "Parallel thinking expands exploration breadth, complementing the deep exploration of information-seeking (IS) agents to further enhance problem-solving capability. However, conventional parallel thinking faces two key challenges in this setting: inefficiency from repeatedly rolling out from scratch, and difficulty in integrating long-horizon reasoning trajectories during answer generation, as limited context capacity prevents full consideration of the reasoning process. To address these issues, we propose ParallelMuse, a two-stage paradigm designed for deep IS agents. The first stage, Functionality-Specified Partial Rollout, partitions generated sequences into functional regions and performs uncertainty-guided path reuse and branching to enhance exploration efficiency. The second stage, Compressed Reasoning Aggregation, exploits reasoning redundancy to losslessly compress information relevant to answer derivation and synthesize a coherent final answer. Experiments across multiple open-source agents and benchmarks demonstrate up to 62% performance improvement with a 10--30% reduction in exploratory token consumption.",
    "fetched_at": "2025-10-29T10:17:29.261795Z"
  },
  {
    "id": "2510.24700v1",
    "title": "Greedy Sampling Is Provably Efficient for RLHF",
    "date": "2025-10-28",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI",
      "cs.IT",
      "IT",
      "math.IT",
      "stat.ML",
      "ML"
    ],
    "authors": [
      "Di Wu",
      "Chengshuai Shi",
      "Jing Yang",
      "Cong Shen"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.24700v1",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique for post-training large language models. Despite its empirical success, the theoretical understanding of RLHF is still limited, as learning the KL-regularized target with only preference feedback poses additional challenges compared with canonical RL. Existing works mostly study the reward-based Bradley-Terry (BT) preference model, and extend classical designs utilizing optimism or pessimism. This work, instead, considers the general preference model (whose practical relevance has been observed recently) and obtains performance guarantees with major, order-wise improvements over existing ones. Surprisingly, these results are derived from algorithms that directly use the empirical estimates (i.e., greedy sampling), as opposed to constructing optimistic or pessimistic estimates in previous works. This insight has a deep root in the unique structural property of the optimal policy class under the KL-regularized target, and we further specialize it to the BT model, highlighting the surprising sufficiency of greedy sampling in RLHF.",
    "fetched_at": "2025-10-29T10:17:29.261725Z"
  },
  {
    "id": "2510.24706v1",
    "title": "ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality   Games?",
    "date": "2025-10-28",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI",
      "cs.HC",
      "HC",
      "cs.SE",
      "SE"
    ],
    "authors": [
      "Shuqing Li",
      "Jiayi Yan",
      "Chenyu Niu",
      "Jen-tse Huang",
      "Yun Peng",
      "Wenxuan Wang",
      "Yepang Liu",
      "Michael R. Lyu"
    ],
    "institution": "Google",
    "link": "http://arxiv.org/pdf/2510.24706v1",
    "abstract": "Virtual Reality (VR) games require players to translate high-level semantic actions into precise device manipulations using controllers and head-mounted displays (HMDs). While humans intuitively perform this translation based on common sense and embodied understanding, whether Large Language Models (LLMs) can effectively replicate this ability remains underexplored. This paper introduces a benchmark, ComboBench, evaluating LLMs' capability to translate semantic actions into VR device manipulation sequences across 262 scenarios from four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II, and Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o, Gemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against annotated ground truth and human performance. Our results reveal that while top-performing models like Gemini-1.5-Pro demonstrate strong task decomposition capabilities, they still struggle with procedural reasoning and spatial understanding compared to humans. Performance varies significantly across games, suggesting sensitivity to interaction complexity. Few-shot examples substantially improve performance, indicating potential for targeted enhancement of LLMs' VR manipulation capabilities. We release all materials at https://sites.google.com/view/combobench.",
    "fetched_at": "2025-10-29T10:17:29.261337Z"
  },
  {
    "id": "2510.24707v1",
    "title": "MetricX-25 and GemSpanEval: Google Translate Submissions to the WMT25   Evaluation Shared Task",
    "date": "2025-10-28",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Juraj Juraska",
      "Tobias Domhan",
      "Mara Finkelstein",
      "Tetsuji Nakagawa",
      "Geza Kovacs",
      "Daniel Deutsch",
      "Pidong Wang",
      "Markus Freitag"
    ],
    "institution": "Google",
    "link": "http://arxiv.org/pdf/2510.24707v1",
    "abstract": "In this paper, we present our submissions to the unified WMT25 Translation Evaluation Shared Task. For the Quality Score Prediction subtask, we create a new generation of MetricX with improvements in the input format and the training protocol, while for the Error Span Detection subtask we develop a new model, GemSpanEval, trained to predict error spans along with their severities and categories. Both systems are based on the state-of-the-art multilingual open-weights model Gemma 3, fine-tuned on publicly available WMT data. We demonstrate that MetricX-25, adapting Gemma 3 to an encoder-only architecture with a regression head on top, can be trained to effectively predict both MQM and ESA quality scores, and significantly outperforms its predecessor. Our decoder-only GemSpanEval model, on the other hand, we show to be competitive in error span detection with xCOMET, a strong encoder-only sequence-tagging baseline. With error span detection formulated as a generative task, we instruct the model to also output the context for each predicted error span, thus ensuring that error spans are identified unambiguously.",
    "fetched_at": "2025-10-29T10:17:29.261273Z"
  },
  {
    "id": "2510.24709v1",
    "title": "Does Object Binding Naturally Emerge in Large Pretrained Vision   Transformers?",
    "date": "2025-10-28",
    "tags": [
      "cs.CV",
      "CV",
      "cs.AI",
      "AI",
      "cs.LG",
      "LG",
      "q-bio.NC",
      "NC"
    ],
    "authors": [
      "Yihao Li",
      "Saeed Salehi",
      "Lyle Ungar",
      "Konrad P. Kording"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24709v1",
    "abstract": "Object binding, the brain's ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high-level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term IsSameObject. We decode IsSameObject from patch embeddings across ViT layers using a similarity probe, which reaches over 90% accuracy. Crucially, this object-binding capability emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker in ImageNet-supervised models, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that IsSameObject is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating IsSameObject from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of \"which parts belong together\" emerges naturally in a connectionist system.",
    "fetched_at": "2025-10-29T10:17:29.261210Z"
  },
  {
    "id": "2510.24710v1",
    "title": "A Single-Loop First-Order Algorithm for Linearly Constrained Bilevel   Optimization",
    "date": "2025-10-28",
    "tags": [
      "math.OC",
      "OC",
      "cs.IT",
      "IT",
      "cs.LG",
      "LG",
      "math.IT",
      "stat.ML",
      "ML"
    ],
    "authors": [
      "Wei Shen",
      "Jiawei Zhang",
      "Minhui Huang",
      "Cong Shen"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24710v1",
    "abstract": "We study bilevel optimization problems where the lower-level problems are strongly convex and have coupled linear constraints. To overcome the potential non-smoothness of the hyper-objective and the computational challenges associated with the Hessian matrix, we utilize penalty and augmented Lagrangian methods to reformulate the original problem as a single-level one. Especially, we establish a strong theoretical connection between the reformulated function and the original hyper-objective by characterizing the closeness of their values and derivatives. Based on this reformulation, we propose a single-loop, first-order algorithm for linearly constrained bilevel optimization (SFLCB). We provide rigorous analyses of its non-asymptotic convergence rates, showing an improvement over prior double-loop algorithms -- form $O(\\epsilon^{-3}\\log(\\epsilon^{-1}))$ to $O(\\epsilon^{-3})$. The experiments corroborate our theoretical findings and demonstrate the practical efficiency of the proposed SFLCB algorithm. Simulation code is provided at https://github.com/ShenGroup/SFLCB.",
    "fetched_at": "2025-10-29T10:17:29.261151Z"
  },
  {
    "id": "2510.24718v1",
    "title": "Generative View Stitching",
    "date": "2025-10-28",
    "tags": [
      "cs.CV",
      "CV",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Chonghyuk Song",
      "Michal Stary",
      "Boyuan Chen",
      "George Kopanas",
      "Vincent Sitzmann"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.24718v1",
    "abstract": "Autoregressive video diffusion models are capable of long rollouts that are stable and consistent with history, but they are unable to guide the current generation with conditioning from the future. In camera-guided video generation with a predefined camera trajectory, this limitation leads to collisions with the generated scene, after which autoregression quickly collapses. To address this, we propose Generative View Stitching (GVS), which samples the entire sequence in parallel such that the generated scene is faithful to every part of the predefined camera trajectory. Our main contribution is a sampling algorithm that extends prior work on diffusion stitching for robot planning to video generation. While such stitching methods usually require a specially trained model, GVS is compatible with any off-the-shelf video model trained with Diffusion Forcing, a prevalent sequence diffusion framework that we show already provides the affordances necessary for stitching. We then introduce Omni Guidance, a technique that enhances the temporal consistency in stitching by conditioning on both the past and future, and that enables our proposed loop-closing mechanism for delivering long-range coherence. Overall, GVS achieves camera-guided video generation that is stable, collision-free, frame-to-frame consistent, and closes loops for a variety of predefined camera paths, including Oscar Reutersv\\\"ard's Impossible Staircase. Results are best viewed as videos at https://andrewsonga.github.io/gvs.",
    "fetched_at": "2025-10-29T10:17:29.261086Z"
  },
  {
    "id": "2510.24151v1",
    "title": "BMGQ: A Bottom-up Method for Generating Complex Multi-hop Reasoning   Questions from Semi-structured Data",
    "date": "2025-10-28",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Bingsen Qiu",
      "Zijian Liu",
      "Xiao Liu",
      "Haoshen Yang",
      "Zeren Gao",
      "Bingjie Wang",
      "Feier Zhang",
      "Yixuan Qin",
      "Chunyan Li"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24151v1",
    "abstract": "Building training-ready multi-hop question answering (QA) datasets that truly stress a model's retrieval and reasoning abilities remains highly challenging recently. While there have been a few recent evaluation datasets that capture the characteristics of hard-to-search but easy-to-verify problems -- requiring the integration of ambiguous, indirect, and cross-domain cues -- these data resources remain scarce and are mostly designed for evaluation, making them unsuitable for supervised fine-tuning (SFT) or reinforcement learning (RL). Meanwhile, manually curating non-trivially retrievable questions -- where answers cannot be found through a single direct query but instead require multi-hop reasoning over oblique and loosely connected evidence -- incurs prohibitive human costs and fails to scale, creating a critical data bottleneck for training high-capability retrieval-and-reasoning agents.   To address this, we present an automated framework for generating high-difficulty, training-ready multi-hop questions from semi-structured knowledge sources. The system (i) grows diverse, logically labeled evidence clusters through Natural Language Inference (NLI)-based relation typing and diversity-aware expansion; (ii) applies reverse question construction to compose oblique cues so that isolated signals are underinformative but their combination uniquely identifies the target entity; and (iii) enforces quality with a two-step evaluation pipeline that combines multi-model consensus filtering with structured constraint decomposition and evidence-based matching. The result is a scalable process that yields complex, retrieval-resistant yet verifiable questions suitable for SFT/RL training as well as challenging evaluation, substantially reducing human curation effort while preserving the difficulty profile of strong evaluation benchmarks.",
    "fetched_at": "2025-10-29T10:17:26.629832Z"
  },
  {
    "id": "2510.24432v1",
    "title": "Fill in the Blanks: Accelerating Q-Learning with a Handful of   Demonstrations in Sparse Reward Settings",
    "date": "2025-10-28",
    "tags": [
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Seyed Mahdi Basiri Azad",
      "Joschka Boedecker"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24432v1",
    "abstract": "Reinforcement learning (RL) in sparse-reward environments remains a significant challenge due to the lack of informative feedback. We propose a simple yet effective method that uses a small number of successful demonstrations to initialize the value function of an RL agent. By precomputing value estimates from offline demonstrations and using them as targets for early learning, our approach provides the agent with a useful prior over promising actions. The agent then refines these estimates through standard online interaction. This hybrid offline-to-online paradigm significantly reduces the exploration burden and improves sample efficiency in sparse-reward settings. Experiments on benchmark tasks demonstrate that our method accelerates convergence and outperforms standard baselines, even with minimal or suboptimal demonstration data.",
    "fetched_at": "2025-10-29T10:17:26.629416Z"
  },
  {
    "id": "2510.24515v1",
    "title": "Stochastic Prize-Collecting Games: Strategic Planning in Multi-Robot   Systems",
    "date": "2025-10-28",
    "tags": [
      "cs.RO",
      "RO"
    ],
    "authors": [
      "Malintha Fernando",
      "Petter Ögren",
      "Silun Zhang"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24515v1",
    "abstract": "The Team Orienteering Problem (TOP) generalizes many real-world multi-robot scheduling and routing tasks that occur in autonomous mobility, aerial logistics, and surveillance applications. While many flavors of the TOP exist for planning in multi-robot systems, they assume that all the robots cooperate toward a single objective; thus, they do not extend to settings where the robots compete in reward-scarce environments. We propose Stochastic Prize-Collecting Games (SPCG) as an extension of the TOP to plan in the presence of self-interested robots operating on a graph, under energy constraints and stochastic transitions. A theoretical study on complete and star graphs establishes that there is a unique pure Nash equilibrium in SPCGs that coincides with the optimal routing solution of an equivalent TOP given a rank-based conflict resolution rule. This work proposes two algorithms: Ordinal Rank Search (ORS) to obtain the ''ordinal rank'' --one's effective rank in temporarily-formed local neighborhoods during the games' stages, and Fictitious Ordinal Response Learning (FORL) to obtain best-response policies against one's senior-rank opponents. Empirical evaluations conducted on road networks and synthetic graphs under both dynamic and stationary prize distributions show that 1) the state-aliasing induced by OR-conditioning enables learning policies that scale more efficiently to large team sizes than those trained with the global index, and 2) Policies trained with FORL generalize better to imbalanced prize distributions than those with other multi-agent training methods. Finally, the learned policies in the SPCG achieved between 87% and 95% optimality compared to an equivalent TOP solution obtained by mixed-integer linear programming.",
    "fetched_at": "2025-10-29T10:17:26.629377Z"
  },
  {
    "id": "2510.24628v1",
    "title": "\"Mm, Wat?\" Detecting Other-initiated Repair Requests in Dialogue",
    "date": "2025-10-28",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Anh Ngo",
      "Nicolas Rollet",
      "Catherine Pelachaud",
      "Chloe Clavel"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24628v1",
    "abstract": "Maintaining mutual understanding is a key component in human-human conversation to avoid conversation breakdowns, in which repair, particularly Other-Initiated Repair (OIR, when one speaker signals trouble and prompts the other to resolve), plays a vital role. However, Conversational Agents (CAs) still fail to recognize user repair initiation, leading to breakdowns or disengagement. This work proposes a multimodal model to automatically detect repair initiation in Dutch dialogues by integrating linguistic and prosodic features grounded in Conversation Analysis. The results show that prosodic cues complement linguistic features and significantly improve the results of pretrained text and audio embeddings, offering insights into how different features interact. Future directions include incorporating visual cues, exploring multilingual and cross-context corpora to assess the robustness and generalizability.",
    "fetched_at": "2025-10-29T10:17:26.629300Z"
  },
  {
    "id": "2510.24663v1",
    "title": "OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan   DAGs",
    "date": "2025-10-28",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Yifu Lu",
      "Shengjie Liu",
      "Li Dong"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24663v1",
    "abstract": "Agentic tool use has gained traction with the rise of agentic tool calling, yet most existing work overlooks the complexity of multi-turn tool interactions. We introduce OrchDAG, a synthetic data generation pipeline that models tool execution as directed acyclic graphs (DAGs) with controllable complexity. Using this dataset, we benchmark model performance and propose a graph-based reward to enhance RLVR training. Experiments show that the dataset presents a challenging but solvable benchmark, and the proposed reward is effective when combined with GRPO-style algorithms, highlighting the importance of leveraging topological structure and data complexity in multi-turn tool use.",
    "fetched_at": "2025-10-29T10:17:26.629108Z"
  },
  {
    "id": "2510.24690v1",
    "title": "Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework   for In-Context Planning",
    "date": "2025-10-28",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Shengjie Liu",
      "Li Dong",
      "Zhenyu Zhang"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24690v1",
    "abstract": "We present a framework for uncovering and exploiting dependencies among tools and documents to enhance exemplar artifact generation. Our method begins by constructing a tool knowledge graph from tool schemas,including descriptions, arguments, and output payloads, using a DeepResearch-inspired analysis. In parallel, we derive a complementary knowledge graph from internal documents and SOPs, which is then fused with the tool graph. To generate exemplar plans, we adopt a deep-sparse integration strategy that aligns structural tool dependencies with procedural knowledge. Experiments demonstrate that this unified framework effectively models tool interactions and improves plan generation, underscoring the benefits of linking tool graphs with domain knowledge graphs for tool-augmented reasoning and planning.",
    "fetched_at": "2025-10-29T10:17:26.629066Z"
  },
  {
    "id": "2510.24014v1",
    "title": "TEXT2DB: Integration-Aware Information Extraction with Large Language   Model Agents",
    "date": "2025-10-28",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Yizhu Jiao",
      "Sha Li",
      "Sizhe Zhou",
      "Heng Ji",
      "Jiawei Han"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24014v1",
    "abstract": "The task of information extraction (IE) is to extract structured knowledge from text. However, it is often not straightforward to utilize IE output due to the mismatch between the IE ontology and the downstream application needs. We propose a new formulation of IE TEXT2DB that emphasizes the integration of IE output and the target database (or knowledge base). Given a user instruction, a document set, and a database, our task requires the model to update the database with values from the document set to satisfy the user instruction. This task requires understanding user instructions for what to extract and adapting to the given DB/KB schema for how to extract on the fly. To evaluate this new task, we introduce a new benchmark featuring common demands such as data infilling, row population, and column addition. In addition, we propose an LLM agent framework OPAL (Observe-PlanAnalyze LLM) which includes an Observer component that interacts with the database, the Planner component that generates a code-based plan with calls to IE models, and the Analyzer component that provides feedback regarding code quality before execution. Experiments show that OPAL can successfully adapt to diverse database schemas by generating different code plans and calling the required IE models. We also highlight difficult cases such as dealing with large databases with complex dependencies and extraction hallucination, which we believe deserve further investigation. Source code: https://github.com/yzjiao/Text2DB",
    "fetched_at": "2025-10-29T10:17:25.314328Z"
  },
  {
    "id": "2510.24030v1",
    "title": "Human Machine Social Hybrid Intelligence:A Collaborative Decision Making   Framework for Large Model Agent Groups and Human Experts",
    "date": "2025-10-28",
    "tags": [
      "cs.MA",
      "MA"
    ],
    "authors": [
      "Ahmet Akkaya Melih",
      "Yamuna Singh",
      "Kunal L. Agarwal",
      "Priya Mukherjee",
      "Kiran Pattnaik",
      "Hanuman Bhatia"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24030v1",
    "abstract": "The rapid advancements in large foundation models and multi-agent systems offer unprecedented capabilities, yet current Human-in-the-Loop (HiTL) paradigms inadequately integrate human expertise, often leading to cognitive overload and decision-making bottlenecks in complex, high-stakes environments. We propose the \"Human-Machine Social Hybrid Intelligence\" (HMS-HI) framework, a novel architecture designed for deep, collaborative decision-making between groups of human experts and LLM-powered AI agents. HMS-HI is built upon three core pillars: (1) a \\textbf{Shared Cognitive Space (SCS)} for unified, multi-modal situational awareness and structured world modeling; (2) a \\textbf{Dynamic Role and Task Allocation (DRTA)} module that adaptively assigns tasks to the most suitable agent (human or AI) based on capabilities and workload; and (3) a \\textbf{Cross-Species Trust Calibration (CSTC)} protocol that fosters transparency, accountability, and mutual adaptation through explainable declarations and structured feedback. Validated in a high-fidelity urban emergency response simulation, HMS-HI significantly reduced civilian casualties by 72\\% and cognitive load by 70\\% compared to traditional HiTL approaches, demonstrating superior decision quality, efficiency, and human-AI trust. An ablation study confirms the critical contribution of each module, highlighting that engineered trust and shared context are foundational for scalable, synergistic human-AI collaboration.",
    "fetched_at": "2025-10-29T10:17:25.314275Z"
  },
  {
    "id": "2510.24051v1",
    "title": "Pie: A Programmable Serving System for Emerging LLM Applications",
    "date": "2025-10-28",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "In Gim",
      "Zhiyao Ma",
      "Seung-seob Lee",
      "Lin Zhong"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24051v1",
    "abstract": "Emerging large language model (LLM) applications involve diverse reasoning strategies and agentic workflows, straining the capabilities of existing serving systems built on a monolithic token generation loop. This paper introduces Pie, a programmable LLM serving system designed for flexibility and efficiency. Pie decomposes the traditional generation loop into fine-grained service handlers exposed via an API and delegates control of the generation process to user-provided programs, called inferlets. This enables applications to implement new KV cache strategies, bespoke generation logic, and seamlessly integrate computation and I/O-entirely within the application, without requiring modifications to the serving system. Pie executes inferlets using WebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows Pie matches state-of-the-art performance on standard tasks (3-12% latency overhead) while significantly improving latency and throughput (1.3x-3.4x higher) on agentic workflows by enabling application-specific optimizations.",
    "fetched_at": "2025-10-29T10:17:25.314219Z"
  },
  {
    "id": "2510.24109v1",
    "title": "PFEA: An LLM-based High-Level Natural Language Planning and Feedback   Embodied Agent for Human-Centered AI",
    "date": "2025-10-28",
    "tags": [
      "cs.RO",
      "RO"
    ],
    "authors": [
      "Wenbin Ding",
      "Jun Chen",
      "Mingjia Chen",
      "Fei Xie",
      "Qi Mao",
      "Philip Dames"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24109v1",
    "abstract": "The rapid advancement of Large Language Models (LLMs) has marked a significant breakthrough in Artificial Intelligence (AI), ushering in a new era of Human-centered Artificial Intelligence (HAI). HAI aims to better serve human welfare and needs, thereby placing higher demands on the intelligence level of robots, particularly in aspects such as natural language interaction, complex task planning, and execution. Intelligent agents powered by LLMs have opened up new pathways for realizing HAI. However, existing LLM-based embodied agents often lack the ability to plan and execute complex natural language control tasks online. This paper explores the implementation of intelligent robotic manipulating agents based on Vision-Language Models (VLMs) in the physical world. We propose a novel embodied agent framework for robots, which comprises a human-robot voice interaction module, a vision-language agent module and an action execution module. The vision-language agent itself includes a vision-based task planner, a natural language instruction converter, and a task performance feedback evaluator. Experimental results demonstrate that our agent achieves a 28\\% higher average task success rate in both simulated and real environments compared to approaches relying solely on LLM+CLIP, significantly improving the execution success rate of high-level natural language instruction tasks.",
    "fetched_at": "2025-10-29T10:17:25.314174Z"
  },
  {
    "id": "2510.24126v1",
    "title": "Reinforcement Learning for Long-Horizon Multi-Turn Search Agents",
    "date": "2025-10-28",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Vivek Kalyan",
      "Martin Andrews"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24126v1",
    "abstract": "Large Language Model (LLM) agents can leverage multiple turns and tools to solve complex tasks, with prompt-based approaches achieving strong performance. This work demonstrates that Reinforcement Learning (RL) can push capabilities significantly further by learning from experience. Through experiments on a legal document search benchmark, we show that our RL-trained 14 Billion parameter model outperforms frontier class models (85% vs 78% accuracy). In addition, we explore turn-restricted regimes, during training and at test-time, that show these agents achieve better results if allowed to operate over longer multi-turn horizons.",
    "fetched_at": "2025-10-29T10:17:25.314118Z"
  },
  {
    "id": "2510.24161v1",
    "title": "BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and   Cross-Embodiment Learning",
    "date": "2025-10-28",
    "tags": [
      "cs.AI",
      "AI",
      "cs.MM",
      "MM",
      "cs.RO",
      "RO"
    ],
    "authors": [
      "Wentao Tan",
      "Bowen Wang",
      "Heng Zhi",
      "Chenyu Liu",
      "Zhe Li",
      "Jian Liu",
      "Zengrong Lin",
      "Yukun Dai",
      "Yipeng Chen",
      "Wenjie Yang",
      "Enci Xie",
      "Hao Xue",
      "Baixu Ji",
      "Chen Xu",
      "Zhibin Wang",
      "Tianshi Wang",
      "Lei Zhu",
      "Heng Tao Shen"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.24161v1",
    "abstract": "Multimodal large language models (MLLMs) have advanced vision-language reasoning and are increasingly deployed in embodied agents. However, significant limitations remain: MLLMs generalize poorly across digital-physical spaces and embodiments; vision-language-action models (VLAs) produce low-level actions yet lack robust high-level embodied reasoning; and most embodied large language models (ELLMs) are constrained to digital-space with poor generalization to the physical world. Thus, unified models that operate seamlessly across digital and physical spaces while generalizing across embodiments and tasks remain absent. We introduce the \\textbf{Boundless Large Model (BLM$_1$)}, a multimodal spatial foundation model that preserves instruction following and reasoning, incorporates embodied knowledge, and supports robust cross-embodiment control. BLM$_1$ integrates three key capabilities -- \\textit{cross-space transfer, cross-task learning, and cross-embodiment generalization} -- via a two-stage training paradigm. Stage I injects embodied knowledge into the MLLM through curated digital corpora while maintaining language competence. Stage II trains a policy module through an intent-bridging interface that extracts high-level semantics from the MLLM to guide control, without fine-tuning the MLLM backbone. This process is supported by a self-collected cross-embodiment demonstration suite spanning four robot embodiments and six progressively challenging tasks. Evaluations across digital and physical benchmarks show that a single BLM$_1$ instance outperforms four model families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving $\\sim\\!\\textbf{6%}$ gains in digital tasks and $\\sim\\!\\textbf{3%}$ in physical tasks.",
    "fetched_at": "2025-10-29T10:17:25.314083Z"
  },
  {
    "id": "2510.24168v1",
    "title": "MGA: Memory-Driven GUI Agent for Observation-Centric Interaction",
    "date": "2025-10-28",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Weihua Cheng",
      "Ersheng Ni",
      "Wenlong Wang",
      "Yifei Sun",
      "Junming Liu",
      "Wangyu Shen",
      "Yirong Chen",
      "Botian Shi",
      "Ding Wang"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24168v1",
    "abstract": "The rapid progress of Large Language Models (LLMs) and their multimodal extensions (MLLMs) has enabled agentic systems capable of perceiving and acting across diverse environments. A challenging yet impactful frontier is the development of GUI agents, which must navigate complex desktop and web interfaces while maintaining robustness and generalization. Existing paradigms typically model tasks as long-chain executions, concatenating historical trajectories into the context. While approaches such as Mirage and GTA1 refine planning or introduce multi-branch action selection, they remain constrained by two persistent issues: Dependence on historical trajectories, which amplifies error propagation. And Local exploration bias, where \"decision-first, observation-later\" mechanisms overlook critical interface cues. We introduce the Memory-Driven GUI Agent (MGA), which reframes GUI interaction around the principle of observe first, then decide. MGA models each step as an independent, context-rich environment state represented by a triad: current screenshot, task-agnostic spatial information, and a dynamically updated structured memory. Experiments on OSworld benchmarks, real desktop applications (Chrome, VSCode, VLC), and cross-task transfer demonstrate that MGA achieves substantial gains in robustness, generalization, and efficiency compared to state-of-the-art baselines. The code is publicly available at: {https://anonymous.4open.science/r/MGA-3571}.",
    "fetched_at": "2025-10-29T10:17:25.313985Z"
  },
  {
    "id": "2510.24251v1",
    "title": "GRAPHIA: Harnessing Social Graph Data to Enhance LLM-Based Social   Simulation",
    "date": "2025-10-28",
    "tags": [
      "cs.SI",
      "SI"
    ],
    "authors": [
      "Jiarui Ji",
      "Zehua Zhang",
      "Zhewei Wei",
      "Bin Tong",
      "Guan Wang",
      "Bo Zheng"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24251v1",
    "abstract": "Large language models (LLMs) have shown promise in simulating human-like social behaviors. Social graphs provide high-quality supervision signals that encode both local interactions and global network structure, yet they remain underutilized for LLM training. To address this gap, we propose Graphia, the first general LLM-based social graph simulation framework that leverages graph data as supervision for LLM post-training via reinforcement learning. With GNN-based structural rewards, Graphia trains specialized agents to predict whom to interact with (destination selection) and how to interact (edge generation), followed by designed graph generation pipelines. We evaluate Graphia under two settings: Transductive Dynamic Graph Generation (TDGG), a micro-level task with our proposed node-wise interaction alignment metrics; and Inductive Dynamic Graph Generation (IDGG), a macro-level task with our proposed metrics for aligning emergent network properties. On three real-world networks, Graphia improves micro-level alignment by 6.1% in the composite destination selection score, 12% in edge classification accuracy, and 27.9% in edge content BERTScore over the strongest baseline. For macro-level alignment, it achieves 41.11% higher structural similarity and 32.98% better replication of social phenomena such as power laws and echo chambers. Graphia also supports counterfactual simulation, generating plausible behavioral shifts under platform incentives. Our results show that social graphs can serve as high-quality supervision signals for LLM post-training, closing the gap between agent behaviors and network dynamics for LLM-based simulation. Code is available at https://github.com/Ji-Cather/Graphia.git.",
    "fetched_at": "2025-10-29T10:17:25.313920Z"
  },
  {
    "id": "2510.24259v1",
    "title": "Can LLMs Translate Human Instructions into a Reinforcement Learning   Agent's Internal Emergent Symbolic Representation?",
    "date": "2025-10-28",
    "tags": [
      "cs.CL",
      "CL",
      "cs.RO",
      "RO"
    ],
    "authors": [
      "Ziqi Ma",
      "Sao Mai Nguyen",
      "Philippe Xu"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.24259v1",
    "abstract": "Emergent symbolic representations are critical for enabling developmental learning agents to plan and generalize across tasks. In this work, we investigate whether large language models (LLMs) can translate human natural language instructions into the internal symbolic representations that emerge during hierarchical reinforcement learning. We apply a structured evaluation framework to measure the translation performance of commonly seen LLMs -- GPT, Claude, Deepseek and Grok -- across different internal symbolic partitions generated by a hierarchical reinforcement learning algorithm in the Ant Maze and Ant Fall environments. Our findings reveal that although LLMs demonstrate some ability to translate natural language into a symbolic representation of the environment dynamics, their performance is highly sensitive to partition granularity and task complexity. The results expose limitations in current LLMs capacity for representation alignment, highlighting the need for further research on robust alignment between language and internal agent representations.",
    "fetched_at": "2025-10-29T10:17:25.313863Z"
  },
  {
    "id": "2510.24284v1",
    "title": "MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and   Scaling MCP Tools",
    "date": "2025-10-28",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Wenhao Wang",
      "Peizhi Niu",
      "Zhao Xu",
      "Zhaoyu Chen",
      "Jian Du",
      "Yaxin Du",
      "Xianghe Pang",
      "Keduan Huang",
      "Yanfeng Wang",
      "Qiang Yan",
      "Siheng Chen"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.24284v1",
    "abstract": "Large Language Models (LLMs) increasingly rely on external tools to perform complex, realistic tasks, yet their ability to utilize the rapidly expanding Model Contextual Protocol (MCP) ecosystem remains limited. Existing MCP research covers few servers, depends on costly manual curation, and lacks training support, hindering progress toward real-world deployment. To overcome these limitations, we introduce MCP-Flow, an automated web-agent-driven pipeline for large-scale server discovery, data synthesis, and model training. MCP-Flow collects and filters data from 1166 servers and 11536 tools, producing 68733 high-quality instruction-function call pairs and 6439 trajectories, far exceeding prior work in scale and diversity. Extensive experiments demonstrate MCP-Flow's effectiveness in driving superior MCP tool selection, function-call generation, and enhanced agentic task performance. MCP-Flow thus provides a scalable foundation for advancing LLM agents' proficiency in real-world MCP environments. MCP-Flow is publicly available at \\href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.",
    "fetched_at": "2025-10-29T10:17:25.313816Z"
  },
  {
    "id": "2510.24303v1",
    "title": "Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental   Forecasting",
    "date": "2025-10-28",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Deniz Gorur",
      "Antoni Rago",
      "Francesca Toni"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24303v1",
    "abstract": "Judgmental forecasting is the task of making predictions about future events based on human judgment. This task can be seen as a form of claim verification, where the claim corresponds to a future event and the task is to assess the plausibility of that event. In this paper, we propose a novel multi-agent framework for claim verification, whereby different agents may disagree on claim veracity and bring specific evidence for and against the claims, represented as quantitative bipolar argumentation frameworks (QBAFs). We then instantiate the framework for supporting claim verification, with a variety of agents realised with Large Language Models (LLMs): (1) ArgLLM agents, an existing approach for claim verification that generates and evaluates QBAFs; (2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM) from external sources is used to generate QBAFs; (3) RAG-ArgLLM agents, extending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of arguments from external sources. Finally, we conduct experiments with two standard judgmental forecasting datasets, with instances of our framework with two or three agents, empowered by six different base LLMs. We observe that combining evidence from agents can improve forecasting accuracy, especially in the case of three agents, while providing an explainable combination of evidence for claim verification.",
    "fetched_at": "2025-10-29T10:17:25.313748Z"
  },
  {
    "id": "2510.24317v1",
    "title": "Cybersecurity AI Benchmark (CAIBench): A Meta-Benchmark for Evaluating   Cybersecurity AI Agents",
    "date": "2025-10-28",
    "tags": [
      "cs.CR",
      "CR"
    ],
    "authors": [
      "María Sanz-Gómez",
      "Víctor Mayoral-Vilches",
      "Francesco Balassone",
      "Luis Javier Navarrete-Lozano",
      "Cristóbal R. J. Veas Chavez",
      "Maite del Mundo de Torres"
    ],
    "institution": "Meta, MIT",
    "link": "http://arxiv.org/pdf/2510.24317v1",
    "abstract": "Cybersecurity spans multiple interconnected domains, complicating the development of meaningful, labor-relevant benchmarks. Existing benchmarks assess isolated skills rather than integrated performance. We find that pre-trained knowledge of cybersecurity in LLMs does not imply attack and defense abilities, revealing a gap between knowledge and capability. To address this limitation, we present the Cybersecurity AI Benchmark (CAIBench), a modular meta-benchmark framework that allows evaluating LLM models and agents across offensive and defensive cybersecurity domains, taking a step towards meaningfully measuring their labor-relevance. CAIBench integrates five evaluation categories, covering over 10,000 instances: Jeopardy-style CTFs, Attack and Defense CTFs, Cyber Range exercises, knowledge benchmarks, and privacy assessments. Key novel contributions include systematic simultaneous offensive-defensive evaluation, robotics-focused cybersecurity challenges (RCTF2), and privacy-preserving performance assessment (CyberPII-Bench). Evaluation of state-of-the-art AI models reveals saturation on security knowledge metrics (~70\\% success) but substantial degradation in multi-step adversarial (A\\&D) scenarios (20-40\\% success), or worse in robotic targets (22\\% success). The combination of framework scaffolding and LLM model choice significantly impacts performance; we find that proper matches improve up to 2.6$\\times$ variance in Attack and Defense CTFs. These results demonstrate a pronounced gap between conceptual knowledge and adaptive capability, emphasizing the need for a meta-benchmark.",
    "fetched_at": "2025-10-29T10:17:25.313699Z"
  },
  {
    "id": "2510.24339v1",
    "title": "VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science   Automation",
    "date": "2025-10-28",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Yunxuan Jiang",
      "Silan Hu",
      "Xiaoning Wang",
      "Yuanyuan Zhang",
      "Xiangyu Chang"
    ],
    "institution": "Beijing Baixingkefu Network Technology Co., Ltd., School of Computing, National University of Singapore, School of Data Science and Media Intelligence, Communication University of China, School of Management, Xi'an Jiaotong University",
    "link": "http://arxiv.org/pdf/2510.24339v1",
    "abstract": "Large language models (LLMs) become increasingly integrated into data science workflows for automated system design. However, these LLM-driven data science systems rely solely on the internal reasoning of LLMs, lacking guidance from scientific and theoretical principles. This limits their trustworthiness and robustness, especially when dealing with noisy and complex real-world datasets. This paper provides VDSAgents, a multi-agent system grounded in the Predictability-Computability-Stability (PCS) principles proposed in the Veridical Data Science (VDS) framework. Guided by PCS principles, the system implements a modular workflow for data cleaning, feature engineering, modeling, and evaluation. Each phase is handled by an elegant agent, incorporating perturbation analysis, unit testing, and model validation to ensure both functionality and scientific auditability. We evaluate VDSAgents on nine datasets with diverse characteristics, comparing it with state-of-the-art end-to-end data science systems, such as AutoKaggle and DataInterpreter, using DeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the results of AutoKaggle and DataInterpreter, which validates the feasibility of embedding PCS principles into LLM-driven data science automation.",
    "fetched_at": "2025-10-29T10:17:25.313613Z"
  },
  {
    "id": "2510.24358v1",
    "title": "Automatically Benchmarking LLM Code Agents through Agent-Driven   Annotation and Evaluation",
    "date": "2025-10-28",
    "tags": [
      "cs.SE",
      "SE",
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Lingyue Fu",
      "Bolun Zhang",
      "Hao Guan",
      "Yaoming Zhu",
      "Lin Qiu",
      "Weiwen Liu",
      "Xuezhi Cao",
      "Xunliang Cai",
      "Weinan Zhang",
      "Yong Yu"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.24358v1",
    "abstract": "Recent advances in code agents have enabled automated software development at the project level, supported by large language models (LLMs) and widely adopted tools. However, existing benchmarks for code agent evaluation face two major limitations: high annotation cost and expertise requirements, and rigid evaluation metrics that rely primarily on unit tests. To address these challenges, we propose an agent-driven benchmark construction pipeline that leverages human supervision to efficiently generate diverse and challenging project-level tasks. Based on this approach, we introduce PRDBench, a novel benchmark comprising 50 real-world Python projects across 20 domains, each with structured Product Requirement Document (PRD) requirements, comprehensive evaluation criteria, and reference implementations. PRDBench features rich data sources, high task complexity, and flexible metrics. We further employ an Agent-as-a-Judge paradigm to score agent outputs, enabling the evaluation of various test types beyond unit tests. Extensive experiments on PRDBench demonstrate its effectiveness in assessing the capabilities of both code agents and evaluation agents, providing a scalable and robust framework for annotation and evaluation.",
    "fetched_at": "2025-10-29T10:17:25.313570Z"
  },
  {
    "id": "2510.24390v1",
    "title": "Improving LLM Reasoning via Dependency-Aware Query Decomposition and   Logic-Parallel Content Expansion",
    "date": "2025-10-28",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Xianjun Gao",
      "Jianchun Liu",
      "Hongli Xu",
      "Liusheng Huang"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.24390v1",
    "abstract": "The integration of Large Language Models (LLMs) into real-time Web applications, such as AI-powered search and conversational agents, presents a fundamental Web infrastructure challenge: reconciling the demand for high-quality, complex reasoning with the stringent low-latency and high-throughput requirements of interactive services. Current LLM reasoning, hindered by computationally inefficient sequential generation and rigid reasoning strategies, creates a critical bottleneck for the Web services. Existing approaches typically optimize the LLM reasoning for either efficiency or quality but struggle to achieve both, and thus fail to meet the dual requirements of modern Web platforms. To overcome these limitations, we propose Orion, a novel and efficient reasoning framework that enables dependency-aware query decomposition and logic-parallel content expansion. Concretely, Orion decomposes a single query reasoning process into two synergistic phases: (1) \\textit{key point generation}, which distills logically structured key points through retrieval-augmented few-shot prompting, and (2) \\textit{content parallel expansion}, which concurrently elaborates on these points based on a dependency graph to ensure logical consistency. Furthermore, Orion introduces a pipeline scheduling mechanism that exploits the complementary computational characteristics of the two phases (generation imposes pressure on GPU computing and expansion stresses on GPU memory) across multiple queries, enabling cross-query parallelism and dramatically improving reasoning performance (\\ie, efficiency and quality). Experiments on diverse benchmarks show that Orion not only delivers up to 4.33x higher token generation speed and 3.42x lower answer latency over the baselines but also improves reasoning quality by up to 18.75% through explicitly modeling inter-point dependencies.",
    "fetched_at": "2025-10-29T10:17:25.313503Z"
  },
  {
    "id": "2510.24397v1",
    "title": "APTBench: Benchmarking Agentic Potential of Base LLMs During   Pre-Training",
    "date": "2025-10-28",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Jiarui Qin",
      "Yunjia Xi",
      "Junjie Huang",
      "Renting Rui",
      "Di Yin",
      "Weiwen Liu",
      "Yong Yu",
      "Weinan Zhang",
      "Xing Sun"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24397v1",
    "abstract": "With the rapid development of LLM-based agents, there is a growing trend to incorporate agent-specific data into the pre-training stage of LLMs, aiming to better align LLMs with real-world autonomous task execution. However, current pre-training benchmarks primarily focus on isolated and static skills, e.g., common knowledge or mathematical/code reasoning, and fail to reflect model's agentic capabilities. On the other hand, agent benchmarks are typically designed for post-trained models, requiring multi-turn task execution abilities that base models struggle to support. Thus, there is a compelling need for a benchmark that can evaluate agentic potentials during pre-training and guide the model training more effectively. To address this gap, we propose APTBench, a framework that converts real-world agent tasks and successful trajectories into multiple-choice or text completion questions tailored for base models. It focuses on core agentic abilities, e.g., planning and action, and covers key agent scenarios, software engineering and deep research. Compared to existing general-purpose benchmarks, APTBench offers a more predictive signal of a model's downstream performance as an agent, while remaining significantly more lightweight and cost-effective than full-scale, end-to-end agent evaluations after post-training.",
    "fetched_at": "2025-10-29T10:17:25.313450Z"
  },
  {
    "id": "2510.24411v1",
    "title": "OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid   Validation in Realistic Workflows",
    "date": "2025-10-28",
    "tags": [
      "cs.AI",
      "AI",
      "cs.CL",
      "CL",
      "cs.CV",
      "CV",
      "cs.HC",
      "HC"
    ],
    "authors": [
      "Qiushi Sun",
      "Mukai Li",
      "Zhoumianze Liu",
      "Zhihui Xie",
      "Fangzhi Xu",
      "Zhangyue Yin",
      "Kanzhi Cheng",
      "Zehao Li",
      "Zichen Ding",
      "Qi Liu",
      "Zhiyong Wu",
      "Zhuosheng Zhang",
      "Ben Kao",
      "Lingpeng Kong"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24411v1",
    "abstract": "Computer-using agents powered by Vision-Language Models (VLMs) have demonstrated human-like capabilities in operating digital environments like mobile platforms. While these agents hold great promise for advancing digital automation, their potential for unsafe operations, such as system compromise and privacy leakage, is raising significant concerns. Detecting these safety concerns across the vast and complex operational space of mobile environments presents a formidable challenge that remains critically underexplored. To establish a foundation for mobile agent safety research, we introduce MobileRisk-Live, a dynamic sandbox environment accompanied by a safety detection benchmark comprising realistic trajectories with fine-grained annotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety detection framework that synergistically combines a Formal Verifier for detecting explicit system-level violations with a VLM-based Contextual Judge for assessing contextual risks and agent actions. Experiments show that OS-Sentinel achieves 10%-30% improvements over existing approaches across multiple metrics. Further analysis provides critical insights that foster the development of safer and more reliable autonomous mobile agents.",
    "fetched_at": "2025-10-29T10:17:25.313386Z"
  },
  {
    "id": "2510.24428v1",
    "title": "CodeWiki: Automated Repository-Level Documentation at Scale",
    "date": "2025-10-28",
    "tags": [
      "cs.SE",
      "SE"
    ],
    "authors": [
      "Nguyen Hoang Anh",
      "Minh Le-Anh",
      "Bach Le",
      "Nghi D. Q. Bui"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24428v1",
    "abstract": "Developers spend nearly 58% of their time understanding codebases, yet maintaining comprehensive documentation remains challenging due to complexity and manual effort. While recent Large Language Models (LLMs) show promise for function-level documentation, they fail at the repository level, where capturing architectural patterns and cross-module interactions is essential. We introduce CodeWiki, the first open-source framework for holistic repository-level documentation across seven programming languages. CodeWiki employs three innovations: (i) hierarchical decomposition that preserves architectural context, (ii) recursive agentic processing with dynamic delegation, and (iii) synthesis of textual and visual artifacts including architecture diagrams and data flows. We also present CodeWikiBench, the first repository-level documentation benchmark with multi-level rubrics and agentic assessment. CodeWiki achieves 68.79% quality score with proprietary models and 64.80% with open-source alternatives, outperforming existing closed-source systems and demonstrating scalable, accurate documentation for real-world repositories.",
    "fetched_at": "2025-10-29T10:17:25.313299Z"
  },
  {
    "id": "2510.24438v1",
    "title": "Can LLMs Write Faithfully? An Agent-Based Evaluation of LLM-generated   Islamic Content",
    "date": "2025-10-28",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI",
      "cs.CY",
      "CY",
      "cs.MA",
      "MA"
    ],
    "authors": [
      "Abdullah Mushtaq",
      "Rafay Naeem",
      "Ezieddin Elmahjub",
      "Ibrahim Ghaznavi",
      "Shawqi Al-Maliki",
      "Mohamed Abdallah",
      "Ala Al-Fuqaha",
      "Junaid Qadir"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24438v1",
    "abstract": "Large language models are increasingly used for Islamic guidance, but risk misquoting texts, misapplying jurisprudence, or producing culturally inconsistent responses. We pilot an evaluation of GPT-4o, Ansari AI, and Fanar on prompts from authentic Islamic blogs. Our dual-agent framework uses a quantitative agent for citation verification and six-dimensional scoring (e.g., Structure, Islamic Consistency, Citations) and a qualitative agent for five-dimensional side-by-side comparison (e.g., Tone, Depth, Originality). GPT-4o scored highest in Islamic Accuracy (3.93) and Citation (3.38), Ansari AI followed (3.68, 3.32), and Fanar lagged (2.76, 1.82). Despite relatively strong performance, models still fall short in reliably producing accurate Islamic content and citations -- a paramount requirement in faith-sensitive writing. GPT-4o had the highest mean quantitative score (3.90/5), while Ansari AI led qualitative pairwise wins (116/200). Fanar, though trailing, introduces innovations for Islamic and Arabic contexts. This study underscores the need for community-driven benchmarks centering Muslim perspectives, offering an early step toward more reliable AI in Islamic knowledge and other high-stakes domains such as medicine, law, and journalism.",
    "fetched_at": "2025-10-29T10:17:25.313253Z"
  },
  {
    "id": "2510.24442v1",
    "title": "Law in Silico: Simulating Legal Society with LLM-Based Agents",
    "date": "2025-10-28",
    "tags": [
      "cs.AI",
      "AI",
      "cs.CL",
      "CL",
      "cs.CY",
      "CY",
      "cs.MA",
      "MA"
    ],
    "authors": [
      "Yiding Wang",
      "Yuxuan Chen",
      "Fanxu Meng",
      "Xifan Chen",
      "Xiaolei Yang",
      "Muhan Zhang"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24442v1",
    "abstract": "Since real-world legal experiments are often costly or infeasible, simulating legal societies with Artificial Intelligence (AI) systems provides an effective alternative for verifying and developing legal theory, as well as supporting legal administration. Large Language Models (LLMs), with their world knowledge and role-playing capabilities, are strong candidates to serve as the foundation for legal society simulation. However, the application of LLMs to simulate legal systems remains underexplored. In this work, we introduce Law in Silico, an LLM-based agent framework for simulating legal scenarios with individual decision-making and institutional mechanisms of legislation, adjudication, and enforcement. Our experiments, which compare simulated crime rates with real-world data, demonstrate that LLM-based agents can largely reproduce macro-level crime trends and provide insights that align with real-world observations. At the same time, micro-level simulations reveal that a well-functioning, transparent, and adaptive legal system offers better protection of the rights of vulnerable individuals.",
    "fetched_at": "2025-10-29T10:17:25.313182Z"
  },
  {
    "id": "2510.24476v1",
    "title": "Mitigating Hallucination in Large Language Models (LLMs): An   Application-Oriented Survey on RAG, Reasoning, and Agentic Systems",
    "date": "2025-10-28",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Yihan Li",
      "Xiyuan Fu",
      "Ghanshyam Verma",
      "Paul Buitelaar",
      "Mingming Liu"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.24476v1",
    "abstract": "Hallucination remains one of the key obstacles to the reliable deployment of large language models (LLMs), particularly in real-world applications. Among various mitigation strategies, Retrieval-Augmented Generation (RAG) and reasoning enhancement have emerged as two of the most effective and widely adopted approaches, marking a shift from merely suppressing hallucinations to balancing creativity and reliability. However, their synergistic potential and underlying mechanisms for hallucination mitigation have not yet been systematically examined. This survey adopts an application-oriented perspective of capability enhancement to analyze how RAG, reasoning enhancement, and their integration in Agentic Systems mitigate hallucinations. We propose a taxonomy distinguishing knowledge-based and logic-based hallucinations, systematically examine how RAG and reasoning address each, and present a unified framework supported by real-world applications, evaluations, and benchmarks.",
    "fetched_at": "2025-10-29T10:17:25.313127Z"
  },
  {
    "id": "2510.24551v1",
    "title": "Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives",
    "date": "2025-10-28",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Gang Chen",
      "Changshuo Liu",
      "Gene Anne Ooi",
      "Marcus Tan",
      "Zhongle Xie",
      "Jianwei Yin",
      "James Wei Luen Yip",
      "Wenqiao Zhang",
      "Jiaqi Zhu",
      "Beng Chin Ooi"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24551v1",
    "abstract": "Generative Artificial Intelligence (GenAI) is taking the world by storm. It promises transformative opportunities for advancing and disrupting existing practices, including healthcare. From large language models (LLMs) for clinical note synthesis and conversational assistance to multimodal systems that integrate medical imaging, electronic health records, and genomic data for decision support, GenAI is transforming the practice of medicine and the delivery of healthcare, such as diagnosis and personalized treatments, with great potential in reducing the cognitive burden on clinicians, thereby improving overall healthcare delivery. However, GenAI deployment in healthcare requires an in-depth understanding of healthcare tasks and what can and cannot be achieved. In this paper, we propose a data-centric paradigm in the design and deployment of GenAI systems for healthcare. Specifically, we reposition the data life cycle by making the medical data ecosystem as the foundational substrate for generative healthcare systems. This ecosystem is designed to sustainably support the integration, representation, and retrieval of diverse medical data and knowledge. With effective and efficient data processing pipelines, such as semantic vector search and contextual querying, it enables GenAI-powered operations for upstream model components and downstream clinical applications. Ultimately, it not only supplies foundation models with high-quality, multimodal data for large-scale pretraining and domain-specific fine-tuning, but also serves as a knowledge retrieval backend to support task-specific inference via the agentic layer. The ecosystem enables the deployment of GenAI for high-quality and effective healthcare delivery.",
    "fetched_at": "2025-10-29T10:17:25.313076Z"
  },
  {
    "id": "2510.24591v1",
    "title": "ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?",
    "date": "2025-10-28",
    "tags": [
      "cs.CL",
      "CL",
      "astro-ph.IM",
      "IM"
    ],
    "authors": [
      "Christine Ye",
      "Sihan Yuan",
      "Suchetha Cooray",
      "Steven Dillmann",
      "Ian L. V. Roque",
      "Dalya Baron",
      "Philipp Frank",
      "Sergio Martin-Alvarez",
      "Nolan Koblischke",
      "Frank J Qu",
      "Diyi Yang",
      "Risa Wechsler",
      "Ioana Ciuca"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24591v1",
    "abstract": "Frontier AI agents show increasing promise as scientific research assistants, and may eventually be useful for extended, open-ended research workflows. However, in order to use agents for novel research, we must first assess the underlying faithfulness and correctness of their work. To evaluate agents as research assistants, we introduce ReplicationBench, an evaluation framework that tests whether agents can replicate entire research papers drawn from the astrophysics literature. Astrophysics, where research relies heavily on archival data and computational study while requiring little real-world experimentation, is a particularly useful testbed for AI agents in scientific research. We split each paper into tasks which require agents to replicate the paper's core contributions, including the experimental setup, derivations, data analysis, and codebase. Each task is co-developed with the original paper authors and targets a key scientific result, enabling objective evaluation of both faithfulness (adherence to original methods) and correctness (technical accuracy of results). ReplicationBench is extremely challenging for current frontier language models: even the best-performing language models score under 20%. We analyze ReplicationBench trajectories in collaboration with domain experts and find a rich, diverse set of failure modes for agents in scientific research. ReplicationBench establishes the first benchmark of paper-scale, expert-validated astrophysics research tasks, reveals insights about agent performance generalizable to other domains of data-driven science, and provides a scalable framework for measuring AI agents' reliability in scientific research.",
    "fetched_at": "2025-10-29T10:17:25.313005Z"
  },
  {
    "id": "2510.24636v1",
    "title": "OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement   Learning",
    "date": "2025-10-28",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Ziyou Hu",
      "Zhengliang Shi",
      "Minghang Zhu",
      "Haitao Li",
      "Teng Sun",
      "Pengjie Ren",
      "Suzan Verberne",
      "Zhaochun Ren"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.24636v1",
    "abstract": "Reward models (RMs) have become essential for aligning large language models (LLMs), serving as scalable proxies for human evaluation in both training and inference. However, existing RMs struggle on knowledge-intensive and long-form tasks, where evaluating correctness requires grounding beyond the model's internal knowledge. This limitation hinders them from reliably discriminating subtle quality differences, especially when external evidence is necessary. To address this, we introduce OpenRM, a tool-augmented long-form reward model that systematically judges open-ended responses by invoking external tools to gather relevant evidence. We train OpenRM with Group Relative Policy Optimization (GRPO) on over 27K synthesized pairwise examples generated through a controllable data synthesis framework. The training objective jointly supervises intermediate tool usage and final outcome accuracy, incentivizing our reward model to learn effective evidence-based judgment strategies. Extensive experiments on three newly-collected datasets and two widely-used benchmarks demonstrate that OpenRM substantially outperforms existing reward modeling approaches. As a further step, we integrate OpenRM into both inference-time response selection and training-time data selection. This yields consistent gains in downstream LLM alignment tasks, highlighting the potential of tool-augmented reward models for scaling reliable long-form evaluation.",
    "fetched_at": "2025-10-29T10:17:25.312921Z"
  },
  {
    "id": "2510.24645v1",
    "title": "FunReason-MT Technical Report: Overcoming the Complexity Barrier in   Multi-Turn Function Calling",
    "date": "2025-10-28",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Zengzhuang Xu",
      "Bingguang Hao",
      "Zechuan Wang",
      "Yuntao Wen",
      "Maolin Wang",
      "Yang Liu",
      "Long Chen",
      "Dong Wang",
      "Yicheng Chen",
      "Cunyin Peng",
      "Chenyi Zhuang",
      "Jinjie Gu",
      "Leilei Gan",
      "Xiangyu Zhao",
      "Shi Gu"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24645v1",
    "abstract": "Function calling (FC) empowers large language models (LLMs) and autonomous agents to interface with external tools, a critical capability for solving complex, real-world problems. As this ability becomes increasingly central to advanced AI systems, the need for high-quality, multi-turn training data to develop and refine it cannot be overstated. Existing data synthesis methods, such as random environment sampling or multi-agent role-playing, are not powerful enough to generate high-quality data in real-world environments. Practical challenges come in three folds: targeted model training, isolation of tool architecture, and multi-turn logical dependency. To address these structural deficiencies, we present FunReason-MT, a novel data synthesis framework for real-world multi-turn tool use. FunReason-MT resolves the complexity barrier in multi-turn FC data by employing 1) Environment-API Graph Interactions to gather varied high-quality trajectories, 2) Advanced Tool-Query Synthesis to simplify hard query construction, and 3) Guided Iterative Chain for sophisticated CoT generation. Evaluations on Berkeley Function-Calling Leaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built upon FunReason-MT generated data achieves state-of-the-art performance among comparable-sized models, outperforming most close-source models. Further performance improvements on BFCLv4 confirm that FunReason-MT provides a reliable and robust source for agentic learning.",
    "fetched_at": "2025-10-29T10:17:25.312855Z"
  },
  {
    "id": "2510.24654v1",
    "title": "Evolving Diagnostic Agents in a Virtual Clinical Environment",
    "date": "2025-10-28",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Pengcheng Qiu",
      "Chaoyi Wu",
      "Junwei Liu",
      "Qiaoyu Zheng",
      "Yusheng Liao",
      "Haowen Wang",
      "Yun Yue",
      "Qianrui Fan",
      "Shuai Zhen",
      "Jian Wang",
      "Jinjie Gu",
      "Yanfeng Wang",
      "Ya Zhang",
      "Weidi Xie"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.24654v1",
    "abstract": "In this paper, we present a framework for training large language models (LLMs) as diagnostic agents with reinforcement learning, enabling them to manage multi-turn diagnostic processes, adaptively select examinations, and commit to final diagnoses. Unlike instruction-tuned models trained on static case summaries, our method acquires diagnostic strategies through interactive exploration and outcome-based feedback. Our contributions are fourfold: (i) We present DiagGym, a diagnostics world model trained with electronic health records that emits examination outcomes conditioned on patient history and recommended examination, serving as a virtual clinical environment for realistic diagnosis training and evaluation; (ii) We train DiagAgent via end-to-end, multi-turn reinforcement learning to learn diagnostic policies that optimize both information yield and diagnostic accuracy; (iii) We introduce DiagBench, a diagnostic benchmark comprising 750 cases with physician-validated examination recommendations and 99 cases annotated with 973 physician-written rubrics on diagnosis process; (iv) we demonstrate superior performance across diverse diagnostic settings. DiagAgent significantly outperforms 10 state-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two prompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34% higher diagnostic accuracy and 44.03% improvement in examination recommendation hit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic accuracy and 23.09% boost in examination recommendation F1 score. In rubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by 7.1% in weighted rubric score. These findings indicate that learning policies in interactive clinical environments confers dynamic and clinically meaningful diagnostic management abilities unattainable through passive training alone.",
    "fetched_at": "2025-10-29T10:17:25.312768Z"
  },
  {
    "id": "2510.24694v1",
    "title": "Repurposing Synthetic Data for Fine-grained Search Agent Supervision",
    "date": "2025-10-28",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Yida Zhao",
      "Kuan Li",
      "Xixi Wu",
      "Liwen Zhang",
      "Dingchu Zhang",
      "Baixuan Li",
      "Maojia Song",
      "Zhuo Chen",
      "Chenxi Wang",
      "Xinyu Wang",
      "Kewei Tu",
      "Pengjun Xie",
      "Jingren Zhou",
      "Yong Jiang"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.24694v1",
    "abstract": "LLM-based search agents are increasingly trained on entity-centric synthetic data to solve complex, knowledge-intensive tasks. However, prevailing training methods like Group Relative Policy Optimization (GRPO) discard this rich entity information, relying instead on sparse, outcome-based rewards. This critical limitation renders them unable to distinguish informative \"near-miss\" samples-those with substantially correct reasoning but a flawed final answer-from complete failures, thus discarding valuable learning signals. We address this by leveraging the very entities discarded during training. Our empirical analysis reveals a strong positive correlation between the number of ground-truth entities identified during an agent's reasoning process and final answer accuracy. Building on this insight, we introduce Entity-aware Group Relative Policy Optimization (E-GRPO), a novel framework that formulates a dense entity-aware reward function. E-GRPO assigns partial rewards to incorrect samples proportional to their entity match rate, enabling the model to effectively learn from these \"near-misses\". Experiments on diverse question-answering (QA) and deep research benchmarks show that E-GRPO consistently and significantly outperforms the GRPO baseline. Furthermore, our analysis reveals that E-GRPO not only achieves superior accuracy but also induces more efficient reasoning policies that require fewer tool calls, demonstrating a more effective and sample-efficient approach to aligning search agents.",
    "fetched_at": "2025-10-29T10:17:25.312681Z"
  },
  {
    "id": "2510.24695v1",
    "title": "AgentFrontier: Expanding the Capability Frontier of LLM Agents with   ZPD-Guided Data Synthesis",
    "date": "2025-10-28",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Xuanzhong Chen",
      "Zile Qiao",
      "Guoxin Chen",
      "Liangcai Su",
      "Zhen Zhang",
      "Xinyu Wang",
      "Pengjun Xie",
      "Fei Huang",
      "Jingren Zhou",
      "Yong Jiang"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24695v1",
    "abstract": "Training large language model agents on tasks at the frontier of their capabilities is key to unlocking advanced reasoning. We introduce a data synthesis approach inspired by the educational theory of the Zone of Proximal Development (ZPD), which defines this frontier as tasks an LLM cannot solve alone but can master with guidance. To operationalize this, we present the AgentFrontier Engine, an automated pipeline that synthesizes high-quality, multidisciplinary data situated precisely within the LLM's ZPD. This engine supports both continued pre-training with knowledge-intensive data and targeted post-training on complex reasoning tasks. From the same framework, we derive the ZPD Exam, a dynamic and automated benchmark designed to evaluate agent capabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on our synthesized data, which achieves state-of-the-art results on demanding benchmarks like Humanity's Last Exam, even surpassing some leading proprietary agents. Our work demonstrates that a ZPD-guided approach to data synthesis offers a scalable and effective path toward building more capable LLM agents.",
    "fetched_at": "2025-10-29T10:17:25.312579Z"
  },
  {
    "id": "2510.24697v1",
    "title": "WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling   Info-Rich Seeking",
    "date": "2025-10-28",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Zhengwei Tao",
      "Haiyang Shen",
      "Baixuan Li",
      "Wenbiao Yin",
      "Jialong Wu",
      "Kuan Li",
      "Zhongwang Zhang",
      "Huifeng Yin",
      "Rui Ye",
      "Liwen Zhang",
      "Xinyu Wang",
      "Pengjun Xie",
      "Jingren Zhou",
      "Yong Jiang"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.24697v1",
    "abstract": "Large Language Model (LLM)-based agents have emerged as a transformative approach for open-ended problem solving, with information seeking (IS) being a core capability that enables autonomous reasoning and decision-making. While prior research has largely focused on improving retrieval depth, we observe that current IS agents often suffer from low search efficiency, which in turn constrains overall performance. A key factor underlying this inefficiency is the sparsity of target entities in training tasks, which limits opportunities for agents to learn and generalize efficient search behaviors. To address these challenges, we propose WebLeaper, a framework for constructing high-coverage IS tasks and generating efficient solution trajectories. We formulate IS as a tree-structured reasoning problem, enabling a substantially larger set of target entities to be embedded within a constrained context. Leveraging curated Wikipedia tables, we propose three variants for synthesizing IS tasks, Basic, Union, and Reverse-Union, to systematically increase both IS efficiency and efficacy. Finally, we curate training trajectories by retaining only those that are simultaneously accurate and efficient, ensuring that the model is optimized for both correctness and search performance. Extensive experiments on both basic and comprehensive settings, conducted on five IS benchmarks, BrowserComp, GAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method consistently achieves improvements in both effectiveness and efficiency over strong baselines.",
    "fetched_at": "2025-10-29T10:17:25.312511Z"
  },
  {
    "id": "2510.24699v1",
    "title": "AgentFold: Long-Horizon Web Agents with Proactive Context Management",
    "date": "2025-10-28",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Rui Ye",
      "Zhongwang Zhang",
      "Kuan Li",
      "Huifeng Yin",
      "Zhengwei Tao",
      "Yida Zhao",
      "Liangcai Su",
      "Liwen Zhang",
      "Zile Qiao",
      "Xinyu Wang",
      "Pengjun Xie",
      "Fei Huang",
      "Siheng Chen",
      "Jingren Zhou",
      "Yong Jiang"
    ],
    "institution": "OpenAI",
    "link": "http://arxiv.org/pdf/2510.24699v1",
    "abstract": "LLM-based web agents show immense promise for information seeking, yet their effectiveness on long-horizon tasks is hindered by a fundamental trade-off in context management. Prevailing ReAct-based agents suffer from context saturation as they accumulate noisy, raw histories, while methods that fixedly summarize the full history at each step risk the irreversible loss of critical details. Addressing these, we introduce AgentFold, a novel agent paradigm centered on proactive context management, inspired by the human cognitive process of retrospective consolidation. AgentFold treats its context as a dynamic cognitive workspace to be actively sculpted, rather than a passive log to be filled. At each step, it learns to execute a `folding' operation, which manages its historical trajectory at multiple scales: it can perform granular condensations to preserve vital, fine-grained details, or deep consolidations to abstract away entire multi-step sub-tasks. The results on prominent benchmarks are striking: with simple supervised fine-tuning (without continual pre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp and 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or matches open-source models of a dramatically larger scale, such as the DeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like OpenAI's o4-mini.",
    "fetched_at": "2025-10-29T10:17:25.312426Z"
  },
  {
    "id": "2510.24701v1",
    "title": "Tongyi DeepResearch Technical Report",
    "date": "2025-10-28",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI",
      "cs.IR",
      "IR",
      "cs.LG",
      "LG",
      "cs.MA",
      "MA"
    ],
    "authors": [
      "Tongyi DeepResearch Team",
      "Baixuan Li",
      "Bo Zhang",
      "Dingchu Zhang",
      "Fei Huang",
      "Guangyu Li",
      "Guoxin Chen",
      "Huifeng Yin",
      "Jialong Wu",
      "Jingren Zhou",
      "Kuan Li",
      "Liangcai Su",
      "Litu Ou",
      "Liwen Zhang",
      "Pengjun Xie",
      "Rui Ye",
      "Wenbiao Yin",
      "Xinmiao Yu",
      "Xinyu Wang",
      "Xixi Wu",
      "Xuanzhong Chen",
      "Yida Zhao",
      "Zhen Zhang",
      "Zhengwei Tao",
      "Zhongwang Zhang",
      "Zile Qiao",
      "Chenxi Wang",
      "Donglei Yu",
      "Gang Fu",
      "Haiyang Shen",
      "Jiayin Yang",
      "Jun Lin",
      "Junkai Zhang",
      "Kui Zeng",
      "Li Yang",
      "Hailong Yin",
      "Maojia Song",
      "Ming Yan",
      "Peng Xia",
      "Qian Xiao",
      "Rui Min",
      "Ruixue Ding",
      "Runnan Fang",
      "Shaowei Chen",
      "Shen Huang",
      "Shihang Wang",
      "Shihao Cai",
      "Weizhou Shen",
      "Xiaobin Wang",
      "Xin Guan",
      "Xinyu Geng",
      "Yingcheng Shi",
      "Yuning Wu",
      "Zhuo Chen",
      "Zijian Li",
      "Yong Jiang"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24701v1",
    "abstract": "We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community.",
    "fetched_at": "2025-10-29T10:17:25.312334Z"
  },
  {
    "id": "2510.24702v1",
    "title": "Agent Data Protocol: Unifying Datasets for Diverse, Effective   Fine-tuning of LLM Agents",
    "date": "2025-10-28",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Yueqi Song",
      "Ketan Ramaneti",
      "Zaid Sheikh",
      "Ziru Chen",
      "Boyu Gou",
      "Tianbao Xie",
      "Yiheng Xu",
      "Danyang Zhang",
      "Apurva Gandhi",
      "Fan Yang",
      "Joseph Liu",
      "Tianyue Ou",
      "Zhihao Yuan",
      "Frank Xu",
      "Shuyan Zhou",
      "Xingyao Wang",
      "Xiang Yue",
      "Tao Yu",
      "Huan Sun",
      "Yu Su",
      "Graham Neubig"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.24702v1",
    "abstract": "Public research results on large-scale supervised finetuning of AI agents remain relatively rare, since the collection of agent training data presents unique challenges. In this work, we argue that the bottleneck is not a lack of underlying data sources, but that a large variety of data is fragmented across heterogeneous formats, tools, and interfaces. To this end, we introduce the agent data protocol (ADP), a light-weight representation language that serves as an \"interlingua\" between agent datasets in diverse formats and unified agent training pipelines downstream. The design of ADP is expressive enough to capture a large variety of tasks, including API/tool use, browsing, coding, software engineering, and general agentic workflows, while remaining simple to parse and train on without engineering at a per-dataset level. In experiments, we unified a broad collection of 13 existing agent training datasets into ADP format, and converted the standardized ADP data into training-ready formats for multiple agent frameworks. We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. All code and data are released publicly, in the hope that ADP could help lower the barrier to standardized, scalable, and reproducible agent training.",
    "fetched_at": "2025-10-29T10:17:25.312095Z"
  },
  {
    "id": "2510.23524v1",
    "title": "Toward Carbon-Neutral Human AI: Rethinking Data, Computation, and   Learning Paradigms for Sustainable Intelligence",
    "date": "2025-10-27",
    "tags": [
      "cs.AI",
      "AI",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "KC Santosh",
      "Rodrigue Rizk",
      "Longwei Wang"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23524v1",
    "abstract": "The rapid advancement of Artificial Intelligence (AI) has led to unprecedented computational demands, raising significant environmental and ethical concerns. This paper critiques the prevailing reliance on large-scale, static datasets and monolithic training paradigms, advocating for a shift toward human-inspired, sustainable AI solutions. We introduce a novel framework, Human AI (HAI), which emphasizes incremental learning, carbon-aware optimization, and human-in-the-loop collaboration to enhance adaptability, efficiency, and accountability. By drawing parallels with biological cognition and leveraging dynamic architectures, HAI seeks to balance performance with ecological responsibility. We detail the theoretical foundations, system design, and operational principles that enable AI to learn continuously and contextually while minimizing carbon footprints and human annotation costs. Our approach addresses pressing challenges in active learning, continual adaptation, and energy-efficient model deployment, offering a pathway toward responsible, human-centered artificial intelligence.",
    "fetched_at": "2025-10-28T05:41:46.837102Z"
  },
  {
    "id": "2510.23530v1",
    "title": "Learning Linearity in Audio Consistency Autoencoders via Implicit   Regularization",
    "date": "2025-10-27",
    "tags": [
      "cs.SD",
      "SD",
      "cs.AI",
      "AI",
      "cs.LG",
      "LG",
      "eess.AS",
      "AS"
    ],
    "authors": [
      "Bernardo Torres",
      "Manuel Moussallam",
      "Gabriel Meseguer-Brocal"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23530v1",
    "abstract": "Audio autoencoders learn useful, compressed audio representations, but their non-linear latent spaces prevent intuitive algebraic manipulation such as mixing or scaling. We introduce a simple training methodology to induce linearity in a high-compression Consistency Autoencoder (CAE) by using data augmentation, thereby inducing homogeneity (equivariance to scalar gain) and additivity (the decoder preserves addition) without altering the model's architecture or loss function. When trained with our method, the CAE exhibits linear behavior in both the encoder and decoder while preserving reconstruction fidelity. We test the practical utility of our learned space on music source composition and separation via simple latent arithmetic. This work presents a straightforward technique for constructing structured latent spaces, enabling more intuitive and efficient audio processing.",
    "fetched_at": "2025-10-28T05:41:46.837057Z"
  },
  {
    "id": "2510.23532v1",
    "title": "When No Paths Lead to Rome: Benchmarking Systematic Neural Relational   Reasoning",
    "date": "2025-10-27",
    "tags": [
      "cs.AI",
      "AI",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Anirban Das",
      "Irtaza Khalid",
      "Rafael Peñaloza",
      "Steven Schockaert"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23532v1",
    "abstract": "Designing models that can learn to reason in a systematic way is an important and long-standing challenge. In recent years, a wide range of solutions have been proposed for the specific case of systematic relational reasoning, including Neuro-Symbolic approaches, variants of the Transformer architecture, and specialised Graph Neural Networks. However, existing benchmarks for systematic relational reasoning focus on an overly simplified setting, based on the assumption that reasoning can be reduced to composing relational paths. In fact, this assumption is hard-baked into the architecture of several recent models, leading to approaches that can perform well on existing benchmarks but are difficult to generalise to other settings. To support further progress in the field of systematic relational reasoning with neural networks, we introduce NoRA, a new benchmark which adds several levels of difficulty and requires models to go beyond path-based reasoning.",
    "fetched_at": "2025-10-28T05:41:46.837014Z"
  },
  {
    "id": "2510.23534v1",
    "title": "Direct Debiased Machine Learning via Bregman Divergence Minimization",
    "date": "2025-10-27",
    "tags": [
      "econ.EM",
      "EM",
      "cs.LG",
      "LG",
      "math.ST",
      "ST",
      "stat.ME",
      "ME",
      "stat.ML",
      "ML",
      "stat.TH",
      "TH"
    ],
    "authors": [
      "Masahiro Kato"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23534v1",
    "abstract": "We develop a direct debiased machine learning framework comprising Neyman targeted estimation and generalized Riesz regression. Our framework unifies Riesz regression for automatic debiased machine learning, covariate balancing, targeted maximum likelihood estimation (TMLE), and density-ratio estimation. In many problems involving causal effects or structural models, the parameters of interest depend on regression functions. Plugging regression functions estimated by machine learning methods into the identifying equations can yield poor performance because of first-stage bias. To reduce such bias, debiased machine learning employs Neyman orthogonal estimating equations. Debiased machine learning typically requires estimation of the Riesz representer and the regression function. For this problem, we develop a direct debiased machine learning framework with an end-to-end algorithm. We formulate estimation of the nuisance parameters, the regression function and the Riesz representer, as minimizing the discrepancy between Neyman orthogonal scores computed with known and unknown nuisance parameters, which we refer to as Neyman targeted estimation. Neyman targeted estimation includes Riesz representer estimation, and we measure discrepancies using the Bregman divergence. The Bregman divergence encompasses various loss functions as special cases, where the squared loss yields Riesz regression and the Kullback-Leibler divergence yields entropy balancing. We refer to this Riesz representer estimation as generalized Riesz regression. Neyman targeted estimation also yields TMLE as a special case for regression function estimation. Furthermore, for specific pairs of models and Riesz representer estimation methods, we can automatically obtain the covariate balancing property without explicitly solving the covariate balancing objective.",
    "fetched_at": "2025-10-28T05:41:46.836967Z"
  },
  {
    "id": "2510.23536v1",
    "title": "IPQA: A Benchmark for Core Intent Identification in Personalized   Question Answering",
    "date": "2025-10-27",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Jieyong Kim",
      "Maryam Amirizaniani",
      "Soojin Yoon",
      "Dongha Lee"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23536v1",
    "abstract": "Intent identification serves as the foundation for generating appropriate responses in personalized question answering (PQA). However, existing benchmarks evaluate only response quality or retrieval performance without directly measuring intent identification capabilities. This gap is critical because without understanding which intents users prioritize, systems cannot generate responses satisfying individual information needs. To address this, we introduce the concept of core intents: intents users prioritize when selecting answers to satisfy their information needs. To evaluate these core intents, we propose IPQA, a benchmark for core Intent identification in Personalized Question Answering. Since users do not explicitly state their prioritized intents, we derive core intents from observable behavior patterns in answer selection, grounded in satisficing theory where users choose answers meeting their acceptance thresholds. We construct a dataset with various domains through systematic filtering, LLM-based annotation, and rigorous quality control combining automated verification with human validation. Experimental evaluations across state-of-the-art language models reveal that current systems struggle with core intent identification in personalized contexts. Models fail to identify core intents from user histories, with performance degrading as question complexity increases. The code and dataset will be made publicly available to facilitate future research in this direction.",
    "fetched_at": "2025-10-28T05:41:46.836858Z"
  },
  {
    "id": "2510.23538v1",
    "title": "JanusCoder: Towards a Foundational Visual-Programmatic Interface for   Code Intelligence",
    "date": "2025-10-27",
    "tags": [
      "cs.AI",
      "AI",
      "cs.CL",
      "CL",
      "cs.CV",
      "CV",
      "cs.SE",
      "SE"
    ],
    "authors": [
      "Qiushi Sun",
      "Jingyang Gong",
      "Yang Liu",
      "Qiaosheng Chen",
      "Lei Li",
      "Kai Chen",
      "Qipeng Guo",
      "Ben Kao",
      "Fei Yuan"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23538v1",
    "abstract": "The scope of neural code intelligence is rapidly expanding beyond text-based source code to encompass the rich visual outputs that programs generate. This visual dimension is critical for advanced applications like flexible content generation and precise, program-driven editing of visualizations. However, progress has been impeded by the scarcity of high-quality multimodal code data, a bottleneck stemming from challenges in synthesis and quality assessment. To address these challenges, we make contributions from both a data and modeling perspective. We first introduce a complete synthesis toolkit that leverages reciprocal synergies between data modalities to efficiently produce a large-scale, high-quality corpus spanning from standard charts to complex interactive web UIs and code-driven animations. Leveraging this toolkit, we construct JanusCode-800K, the largest multimodal code corpus to date. This powers the training of our models, JanusCoder and JanusCoderV, which establish a visual-programmatic interface for generating code from textual instructions, visual inputs, or a combination of both. Our unified model is a departure from existing approaches that build specialized models for isolated tasks. Extensive experiments on both text-centric and vision-centric coding tasks demonstrate the superior performance of the JanusCoder series, with our 7B to 14B scale models approaching or even exceeding the performance of commercial models. Furthermore, extensive analysis provides key insights into harmonizing programmatic logic with its visual expression. Our code and checkpoints will are available at https://github.com/InternLM/JanusCoder.",
    "fetched_at": "2025-10-28T05:41:46.836808Z"
  },
  {
    "id": "2510.23544v1",
    "title": "LimRank: Less is More for Reasoning-Intensive Information Reranking",
    "date": "2025-10-27",
    "tags": [
      "cs.CL",
      "CL",
      "cs.IR",
      "IR"
    ],
    "authors": [
      "Tingyu Song",
      "Yilun Zhao",
      "Siyue Zhang",
      "Chen Zhao",
      "Arman Cohan"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23544v1",
    "abstract": "Existing approaches typically rely on large-scale fine-tuning to adapt LLMs for information reranking tasks, which is computationally expensive. In this work, we demonstrate that modern LLMs can be effectively adapted using only minimal, high-quality supervision. To enable this, we design LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating diverse, challenging, and realistic reranking examples. Using this synthetic data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and FollowIR for instruction-following retrieval. Our experiments demonstrate that LIMRANK achieves competitive performance, while being trained on less than 5% of the data typically used in prior work. Further ablation studies demonstrate the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization capabilities of LIMRANK across downstream tasks, including scientific literature search and retrieval-augmented generation for knowledge-intensive problem solving.",
    "fetched_at": "2025-10-28T05:41:46.836738Z"
  },
  {
    "id": "2510.23553v1",
    "title": "OntoPret: An Ontology for the Interpretation of Human Behavior",
    "date": "2025-10-27",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Alexis Ellis",
      "Stacie Severyn",
      "Fjollë Novakazi",
      "Hadi Banaee",
      "Cogan Shimizu"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23553v1",
    "abstract": "As human machine teaming becomes central to paradigms like Industry 5.0, a critical need arises for machines to safely and effectively interpret complex human behaviors. A research gap currently exists between techno centric robotic frameworks, which often lack nuanced models of human behavior, and descriptive behavioral ontologies, which are not designed for real time, collaborative interpretation. This paper addresses this gap by presenting OntoPret, an ontology for the interpretation of human behavior. Grounded in cognitive science and a modular engineering methodology, OntoPret provides a formal, machine processable framework for classifying behaviors, including task deviations and deceptive actions. We demonstrate its adaptability across two distinct use cases manufacturing and gameplay and establish the semantic foundations necessary for advanced reasoning about human intentions.",
    "fetched_at": "2025-10-28T05:41:46.836689Z"
  },
  {
    "id": "2510.23554v1",
    "title": "A U-Net and Transformer Pipeline for Multilingual Image Translation",
    "date": "2025-10-27",
    "tags": [
      "cs.LG",
      "LG",
      "cs.CL",
      "CL",
      "cs.CV",
      "CV"
    ],
    "authors": [
      "Siddharth Sahay",
      "Radhika Agarwal"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23554v1",
    "abstract": "This paper presents an end-to-end multilingual translation pipeline that integrates a custom U-Net for text detection, the Tesseract engine for text recognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for Neural Machine Translation (NMT). Our approach first utilizes a U-Net model, trained on a synthetic dataset , to accurately segment and detect text regions from an image. These detected regions are then processed by Tesseract to extract the source text. This extracted text is fed into a custom Transformer model trained from scratch on a multilingual parallel corpus spanning 5 languages. Unlike systems reliant on monolithic pre-trained models, our architecture emphasizes full customization and adaptability. The system is evaluated on its text detection accuracy, text recognition quality, and translation performance via BLEU scores. The complete pipeline demonstrates promising results, validating the viability of a custom-built system for translating text directly from images.",
    "fetched_at": "2025-10-28T05:41:46.836642Z"
  },
  {
    "id": "2510.23558v1",
    "title": "ISA-Bench: Benchmarking Instruction Sensitivity for Large Audio Language   Models",
    "date": "2025-10-27",
    "tags": [
      "cs.SD",
      "SD",
      "cs.CL",
      "CL",
      "eess.AS",
      "AS"
    ],
    "authors": [
      "Bohan Li",
      "Wenbin Huang",
      "Yuhang Qiu",
      "Yiwei Guo",
      "Hankun Wang",
      "Zhihan Li",
      "Jing Peng",
      "Ziyang Ma",
      "Xie Chen",
      "Kai Yu"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23558v1",
    "abstract": "Large Audio Language Models (LALMs), which couple acoustic perception with large language models (LLMs) to extract and understand diverse information from audio, have attracted intense interest from both academic and industrial communities. However, existing LALMs are highly sensitive to how instructions are phrased, affecting both (i) instruction-following rates and (ii) task performance. Yet, no existing benchmarks offer a systematic and comprehensive evaluation of this sensitivity. We introduce ISA-Bench, a dynamic benchmark evaluating instruction sensitivity for LALMs along three axes: instruction description, output format, and task composition. We assess recent open-source and proprietary LALMs using ISA-Bench, profiling both compliance and accuracy under controlled instruction variations. Experimental results reveal that even state-of-the-art LALMs suffer significant instruction sensitivity, leading to degraded performance on fundamental audio understanding tasks. To mitigate this issue, we fine-tune Qwen2-Audio on a specifically constructed complex instruction-variant dataset, achieving a marked improvement in instruction-following performance. However, this also induces nontrivial catastrophic forgetting: the model loses some previously mastered task capabilities when exposed to new instruction styles. Our benchmark provides a standardized basis for assessing and improving instruction sensitivity in LALMs, underscoring the need for instruction-robust audio understanding in real-world pipelines.",
    "fetched_at": "2025-10-28T05:41:46.836539Z"
  },
  {
    "id": "2510.23576v1",
    "title": "UrbanVLA: A Vision-Language-Action Model for Urban Micromobility",
    "date": "2025-10-27",
    "tags": [
      "cs.RO",
      "RO",
      "cs.AI",
      "AI",
      "cs.CV",
      "CV"
    ],
    "authors": [
      "Anqi Li",
      "Zhiyong Wang",
      "Jiazhao Zhang",
      "Minghan Li",
      "Yunpeng Qi",
      "Zhibo Chen",
      "Zhizheng Zhang",
      "He Wang"
    ],
    "institution": "Meta",
    "link": "http://arxiv.org/pdf/2510.23576v1",
    "abstract": "Urban micromobility applications, such as delivery robots, demand reliable navigation across large-scale urban environments while following long-horizon route instructions. This task is particularly challenging due to the dynamic and unstructured nature of real-world city areas, yet most existing navigation methods remain tailored to short-scale and controllable scenarios. Effective urban micromobility requires two complementary levels of navigation skills: low-level capabilities such as point-goal reaching and obstacle avoidance, and high-level capabilities, such as route-visual alignment. To this end, we propose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework designed for scalable urban navigation. Our method explicitly aligns noisy route waypoints with visual observations during execution, and subsequently plans trajectories to drive the robot. To enable UrbanVLA to master both levels of navigation, we employ a two-stage training pipeline. The process begins with Supervised Fine-Tuning (SFT) using simulated environments and trajectories parsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on a mixture of simulation and real-world data, which enhances the model's safety and adaptability in real-world settings. Experiments demonstrate that UrbanVLA surpasses strong baselines by more than 55% in the SocialNav task on MetaUrban. Furthermore, UrbanVLA achieves reliable real-world navigation, showcasing both scalability to large-scale urban environments and robustness against real-world uncertainties.",
    "fetched_at": "2025-10-28T05:41:46.836315Z"
  },
  {
    "id": "2510.23578v1",
    "title": "Reduced AI Acceptance After the Generative AI Boom: Evidence From a   Two-Wave Survey Study",
    "date": "2025-10-27",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Joachim Baumann",
      "Aleksandra Urman",
      "Ulrich Leicht-Deobald",
      "Zachary J. Roman",
      "Anikó Hannák",
      "Markus Christen"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23578v1",
    "abstract": "The rapid adoption of generative artificial intelligence (GenAI) technologies has led many organizations to integrate AI into their products and services, often without considering user preferences. Yet, public attitudes toward AI use, especially in impactful decision-making scenarios, are underexplored. Using a large-scale two-wave survey study (n_wave1=1514, n_wave2=1488) representative of the Swiss population, we examine shifts in public attitudes toward AI before and after the launch of ChatGPT. We find that the GenAI boom is significantly associated with reduced public acceptance of AI (see Figure 1) and increased demand for human oversight in various decision-making contexts. The proportion of respondents finding AI \"not acceptable at all\" increased from 23% to 30%, while support for human-only decision-making rose from 18% to 26%. These shifts have amplified existing social inequalities in terms of widened educational, linguistic, and gender gaps post-boom. Our findings challenge industry assumptions about public readiness for AI deployment and highlight the critical importance of aligning technological development with evolving public preferences.",
    "fetched_at": "2025-10-28T05:41:46.836244Z"
  },
  {
    "id": "2510.23581v1",
    "title": "Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human   Animation",
    "date": "2025-10-27",
    "tags": [
      "cs.CV",
      "CV",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Junyoung Seo",
      "Rodrigo Mira",
      "Alexandros Haliassos",
      "Stella Bounareli",
      "Honglie Chen",
      "Linh Tran",
      "Seungryong Kim",
      "Zoe Landgraf",
      "Jie Shen"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23581v1",
    "abstract": "Audio-driven human animation models often suffer from identity drift during temporal autoregressive generation, where characters gradually lose their identity over time. One solution is to generate keyframes as intermediate temporal anchors that prevent degradation, but this requires an additional keyframe generation stage and can restrict natural motion dynamics. To address this, we propose Lookahead Anchoring, which leverages keyframes from future timesteps ahead of the current generation window, rather than within it. This transforms keyframes from fixed boundaries into directional beacons: the model continuously pursues these future anchors while responding to immediate audio cues, maintaining consistent identity through persistent guidance. This also enables self-keyframing, where the reference image serves as the lookahead target, eliminating the need for keyframe generation entirely. We find that the temporal lookahead distance naturally controls the balance between expressivity and consistency: larger distances allow for greater motion freedom, while smaller ones strengthen identity adherence. When applied to three recent human animation models, Lookahead Anchoring achieves superior lip synchronization, identity preservation, and visual quality, demonstrating improved temporal conditioning across several different architectures. Video results are available at the following link: https://lookahead-anchoring.github.io.",
    "fetched_at": "2025-10-28T05:41:46.836188Z"
  },
  {
    "id": "2510.23585v1",
    "title": "Hope Speech Detection in Social Media English Corpora: Performance of   Traditional and Transformer Models",
    "date": "2025-10-27",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Luis Ramos",
      "Hiram Calvo",
      "Olga Kolesnikova"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23585v1",
    "abstract": "The identification of hope speech has become a promised NLP task, considering the need to detect motivational expressions of agency and goal-directed behaviour on social media platforms. This proposal evaluates traditional machine learning models and fine-tuned transformers for a previously split hope speech dataset as train, development and test set. On development test, a linear-kernel SVM and logistic regression both reached a macro-F1 of 0.78; SVM with RBF kernel reached 0.77, and Na\\\"ive Bayes hit 0.75. Transformer models delivered better results, the best model achieved weighted precision of 0.82, weighted recall of 0.80, weighted F1 of 0.79, macro F1 of 0.79, and 0.80 accuracy. These results suggest that while optimally configured traditional machine learning models remain agile, transformer architectures detect some subtle semantics of hope to achieve higher precision and recall in hope speech detection, suggesting that larges transformers and LLMs could perform better in small datasets.",
    "fetched_at": "2025-10-28T05:41:46.836121Z"
  },
  {
    "id": "2510.23590v1",
    "title": "Lightweight Robust Direct Preference Optimization",
    "date": "2025-10-27",
    "tags": [
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Cheol Woo Kim",
      "Shresth Verma",
      "Mauricio Tec",
      "Milind Tambe"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23590v1",
    "abstract": "Direct Preference Optimization (DPO) has become a popular method for fine-tuning large language models (LLMs) due to its stability and simplicity. However, it is also known to be sensitive to noise in the data and prone to overfitting. Recent works have proposed using distributionally robust optimization (DRO) to address potential noise and distributional shift in the data. However, these methods often suffer from excessive conservatism and high computational cost. We propose DPO-PRO (DPO with Preference Robustness), a robust fine-tuning algorithm based on DPO which accounts for uncertainty in the preference distribution through a lightweight DRO formulation. Unlike prior DRO-based variants, DPO-PRO focuses solely on uncertainty in preferences, avoiding unnecessary conservatism and incurring negligible computational overhead. We further show that DPO-PRO is equivalent to a regularized DPO objective that penalizes model overconfidence under weak preference signals. We evaluate DPO-PRO on standard alignment benchmarks and a real-world public health task. Experimental results show that our method consistently improves robustness to noisy preference signals compared to existing DPO variants.",
    "fetched_at": "2025-10-28T05:41:46.835954Z"
  },
  {
    "id": "2510.23596v1",
    "title": "Think Twice: Branch-and-Rethink Reasoning Reward Model",
    "date": "2025-10-27",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Yizhu Jiao",
      "Jiaqi Zeng",
      "Julien Veron Vialard",
      "Oleksii Kuchaiev",
      "Jiawei Han",
      "Olivier Delalleau"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23596v1",
    "abstract": "Large language models (LLMs) increasingly rely on thinking models that externalize intermediate steps and allocate extra test-time compute, with think-twice strategies showing that a deliberate second pass can elicit stronger reasoning. In contrast, most reward models (RMs) still compress many quality dimensions into a single scalar in one shot, a design that induces judgment diffusion: attention spreads across evaluation criteria, yielding diluted focus and shallow analysis. We introduce branch-and-rethink (BR-RM), a two-turn RM that transfers the think-twice principle to reward modeling. Turn 1 performs adaptive branching, selecting a small set of instance-critical dimensions (such as factuality and safety) and sketching concise, evidence-seeking hypotheses. Turn 2 executes branch-conditioned rethinking, a targeted reread that tests those hypotheses and scrutinizes only what matters most. We train with GRPO-style reinforcement learning over structured two-turn traces using a simple binary outcome reward with strict format checks, making the approach compatible with standard RLHF pipelines. By converting all-at-oncescoringintofocused, second-lookreasoning, BR-RMreducesjudgmentdiffusionandimproves sensitivity to subtle yet consequential errors while remaining practical and scalable. Experimental results demonstrate that our model achieves state-of-the-art performance on three challenging reward modeling benchmarks across diverse domains. The code and the model will be released soon.",
    "fetched_at": "2025-10-28T05:41:46.835843Z"
  },
  {
    "id": "2510.23605v1",
    "title": "Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with   Progressive Texture Infilling",
    "date": "2025-10-27",
    "tags": [
      "cs.CV",
      "CV",
      "cs.AI",
      "AI",
      "cs.GR",
      "GR",
      "cs.LG",
      "LG",
      "cs.RO",
      "RO"
    ],
    "authors": [
      "Shuhong Zheng",
      "Ashkan Mirzaei",
      "Igor Gilitschenski"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23605v1",
    "abstract": "Current 3D/4D generation methods are usually optimized for photorealism, efficiency, and aesthetics. However, they often fail to preserve the semantic identity of the subject across different viewpoints. Adapting generation methods with one or few images of a specific subject (also known as Personalization or Subject-driven generation) allows generating visual content that align with the identity of the subject. However, personalized 3D/4D generation is still largely underexplored. In this work, we introduce TIRE (Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation. It takes an initial 3D asset produced by an existing 3D generative model as input and uses video tracking to identify the regions that need to be modified. Then, we adopt a subject-driven 2D inpainting model for progressively infilling the identified regions. Finally, we resplat the modified 2D multi-view observations back to 3D while still maintaining consistency. Extensive experiments demonstrate that our approach significantly improves identity preservation in 3D/4D generation compared to state-of-the-art methods. Our project website is available at https://zsh2000.github.io/track-inpaint-resplat.github.io/.",
    "fetched_at": "2025-10-28T05:41:46.835705Z"
  },
  {
    "id": "2510.23606v1",
    "title": "Variational Masked Diffusion Models",
    "date": "2025-10-27",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI",
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Yichi Zhang",
      "Alex Schwing",
      "Zhizhen Zhao"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23606v1",
    "abstract": "Masked diffusion models have recently emerged as a flexible framework for discrete generative modeling. However, a key limitation of standard masked diffusion is its inability to effectively capture dependencies among tokens that are predicted concurrently, leading to degraded generation quality when dependencies among tokens are important. To explicitly model dependencies among tokens, we propose Variational Masked Diffusion (VMD), a framework that introduces latent variables into the masked diffusion process. Through controlled experiments on synthetic datasets, we demonstrate that VMD successfully learns dependencies that conventional masked diffusion fails to capture. We further validate the effectiveness of our approach on Sudoku puzzles and text datasets, where learning of dependencies among tokens improves global consistency. Across these domains, VMD enhances both generation quality and dependency awareness, highlighting the value of integrating variational inference into masked diffusion. Our code is available at: https://riccizz.github.io/VMD.",
    "fetched_at": "2025-10-28T05:41:46.835640Z"
  },
  {
    "id": "2510.22963v1",
    "title": "CompressionAttack: Exploiting Prompt Compression as a New Attack Surface   in LLM-Powered Agents",
    "date": "2025-10-27",
    "tags": [
      "cs.CR",
      "CR",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Zesen Liu",
      "Zhixiang Zhang",
      "Yuchong Xie",
      "Dongdong She"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.22963v1",
    "abstract": "LLM-powered agents often use prompt compression to reduce inference costs, but this introduces a new security risk. Compression modules, which are optimized for efficiency rather than safety, can be manipulated by adversarial inputs, causing semantic drift and altering LLM behavior. This work identifies prompt compression as a novel attack surface and presents CompressionAttack, the first framework to exploit it. CompressionAttack includes two strategies: HardCom, which uses discrete adversarial edits for hard compression, and SoftCom, which performs latent-space perturbations for soft compression. Experiments on multiple LLMs show up to 80% attack success and 98% preference flips, while remaining highly stealthy and transferable. Case studies in VSCode Cline and Ollama confirm real-world impact, and current defenses prove ineffective, highlighting the need for stronger protections.",
    "fetched_at": "2025-10-28T05:41:45.732016Z"
  },
  {
    "id": "2510.22967v1",
    "title": "MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality   Evaluation in LLMs",
    "date": "2025-10-27",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Yucheng Ning",
      "Xixun Lin",
      "Fang Fang",
      "Yanan Cao"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.22967v1",
    "abstract": "The widespread adoption of Large Language Models (LLMs) raises critical concerns about the factual accuracy of their outputs, especially in high-risk domains such as biomedicine, law, and education. Existing evaluation methods for short texts often fail on long-form content due to complex reasoning chains, intertwined perspectives, and cumulative information. To address this, we propose a systematic approach integrating large-scale long-form datasets, multi-agent verification mechanisms, and weighted evaluation metrics. We construct LongHalluQA, a Chinese long-form factuality dataset; and develop MAD-Fact, a debate-based multi-agent verification system. We introduce a fact importance hierarchy to capture the varying significance of claims in long-form texts. Experiments on two benchmarks show that larger LLMs generally maintain higher factual consistency, while domestic models excel on Chinese content. Our work provides a structured framework for evaluating and enhancing factual reliability in long-form LLM outputs, guiding their safe deployment in sensitive domains.",
    "fetched_at": "2025-10-28T05:41:45.731972Z"
  },
  {
    "id": "2510.22977v1",
    "title": "The Reasoning Trap: How Enhancing LLM Reasoning Amplifies Tool   Hallucination",
    "date": "2025-10-27",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI",
      "I.2",
      "2"
    ],
    "authors": [
      "Chenlong Yin",
      "Zeyang Sha",
      "Shiwen Cui",
      "Changhua Meng"
    ],
    "institution": "OpenAI, MIT",
    "link": "http://arxiv.org/pdf/2510.22977v1",
    "abstract": "Enhancing the reasoning capabilities of Large Language Models (LLMs) is a key strategy for building Agents that \"think then act.\" However, recent observations, like OpenAI's o3, suggest a paradox: stronger reasoning often coincides with increased hallucination, yet no prior work has systematically examined whether reasoning enhancement itself causes tool hallucination. To address this gap, we pose the central question: Does strengthening reasoning increase tool hallucination? To answer this, we introduce SimpleToolHalluBench, a diagnostic benchmark measuring tool hallucination in two failure modes: (i) no tool available, and (ii) only distractor tools available. Through controlled experiments, we establish three key findings. First, we demonstrate a causal relationship: progressively enhancing reasoning through RL increases tool hallucination proportionally with task performance gains. Second, this effect transcends overfitting - training on non-tool tasks (e.g., mathematics) still amplifies subsequent tool hallucination. Third, the effect is method-agnostic, appearing when reasoning is instilled via supervised fine-tuning and when it is merely elicited at inference by switching from direct answers to step-by-step thinking. We also evaluate mitigation strategies including Prompt Engineering and Direct Preference Optimization (DPO), revealing a fundamental reliability-capability trade-off: reducing hallucination consistently degrades utility. Mechanistically, Reasoning RL disproportionately collapses tool-reliability-related representations, and hallucinations surface as amplified divergences concentrated in late-layer residual streams. These findings reveal that current reasoning enhancement methods inherently amplify tool hallucination, highlighting the need for new training objectives that jointly optimize for capability and reliability.",
    "fetched_at": "2025-10-28T05:41:45.731925Z"
  },
  {
    "id": "2510.23011v1",
    "title": "LangLingual: A Personalised, Exercise-oriented English Language Learning   Tool Leveraging Large Language Models",
    "date": "2025-10-27",
    "tags": [
      "cs.CL",
      "CL",
      "cs.CY",
      "CY"
    ],
    "authors": [
      "Sammriddh Gupta",
      "Sonit Singh",
      "Aditya Joshi",
      "Mira Kim"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23011v1",
    "abstract": "Language educators strive to create a rich experience for learners, while they may be restricted in the extend of feedback and practice they can provide. We present the design and development of LangLingual, a conversational agent built using the LangChain framework and powered by Large Language Models. The system is specifically designed to provide real-time, grammar-focused feedback, generate context-aware language exercises and track learner proficiency over time. The paper discusses the architecture, implementation and evaluation of LangLingual in detail. The results indicate strong usability, positive learning outcomes and encouraging learner engagement.",
    "fetched_at": "2025-10-28T05:41:45.731773Z"
  },
  {
    "id": "2510.23032v1",
    "title": "P1GPT: a multi-agent LLM workflow module for multi-modal financial   information analysis",
    "date": "2025-10-27",
    "tags": [
      "cs.CE",
      "CE"
    ],
    "authors": [
      "Chen-Che Lu",
      "Yun-Cheng Chou",
      "Teng-Ruei Chen"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23032v1",
    "abstract": "Recent advances in large language models (LLMs) have enabled multi-agent reasoning systems capable of collaborative decision-making. However, in financial analysis, most frameworks remain narrowly focused on either isolated single-agent predictors or loosely connected analyst ensembles, and they lack a coherent reasoning workflow that unifies diverse data modalities. We introduce P1GPT, a layered multi-agent LLM framework for multi-modal financial information analysis and interpretable trading decision support. Unlike prior systems that emulate trading teams through role simulation, P1GPT implements a structured reasoning pipeline that systematically fuses technical, fundamental, and news-based insights through coordinated agent communication and integration-time synthesis. Backtesting on multi-modal datasets across major U.S. equities demonstrates that P1GPT achieves superior cumulative and risk-adjusted returns, maintains low drawdowns, and provides transparent causal rationales. These findings suggest that structured reasoning workflows, rather than agent role imitation, offer a scalable path toward explainable and trustworthy financial AI systems.",
    "fetched_at": "2025-10-28T05:41:45.731731Z"
  },
  {
    "id": "2510.23127v1",
    "title": "Lost in Tokenization: Context as the Key to Unlocking Biomolecular   Understanding in Scientific LLMs",
    "date": "2025-10-27",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Kai Zhuang",
      "Jiawei Zhang",
      "Yumou Liu",
      "Hanqun Cao",
      "Chunbin Gu",
      "Mengdi Liu",
      "Zhangyang Gao",
      "Zitong Jerry Wang",
      "Xuanhe Zhou",
      "Pheng-Ann Heng",
      "Lijun Wu",
      "Conghui He",
      "Cheng Tan"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23127v1",
    "abstract": "Scientific Large Language Models (Sci-LLMs) have emerged as a promising frontier for accelerating biological discovery. However, these models face a fundamental challenge when processing raw biomolecular sequences: the tokenization dilemma. Whether treating sequences as a specialized language, risking the loss of functional motif information, or as a separate modality, introducing formidable alignment challenges, current strategies fundamentally limit their reasoning capacity. We challenge this sequence-centric paradigm by positing that a more effective strategy is to provide Sci-LLMs with high-level structured context derived from established bioinformatics tools, thereby bypassing the need to interpret low-level noisy sequence data directly. Through a systematic comparison of leading Sci-LLMs on biological reasoning tasks, we tested three input modes: sequence-only, context-only, and a combination of both. Our findings are striking: the context-only approach consistently and substantially outperforms all other modes. Even more revealing, the inclusion of the raw sequence alongside its high-level context consistently degrades performance, indicating that raw sequences act as informational noise, even for models with specialized tokenization schemes. These results suggest that the primary strength of existing Sci-LLMs lies not in their nascent ability to interpret biomolecular syntax from scratch, but in their profound capacity for reasoning over structured, human-readable knowledge. Therefore, we argue for reframing Sci-LLMs not as sequence decoders, but as powerful reasoning engines over expert knowledge. This work lays the foundation for a new class of hybrid scientific AI agents, repositioning the developmental focus from direct sequence interpretation towards high-level knowledge synthesis. The code is available at github.com/opendatalab-raise-dev/CoKE.",
    "fetched_at": "2025-10-28T05:41:45.731627Z"
  },
  {
    "id": "2510.23245v1",
    "title": "Multi-Stakeholder Alignment in LLM-Powered Collaborative AI Systems: A   Multi-Agent Framework for Intelligent Tutoring",
    "date": "2025-10-27",
    "tags": [
      "cs.HC",
      "HC",
      "cs.MA",
      "MA"
    ],
    "authors": [
      "Alexandre P Uchoa",
      "Carlo E T Oliveira",
      "Claudia L R Motta",
      "Daniel Schneider"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23245v1",
    "abstract": "The integration of Large Language Models into Intelligent Tutoring Systems pre-sents significant challenges in aligning with diverse and often conflicting values from students, parents, teachers, and institutions. Existing architectures lack for-mal mechanisms for negotiating these multi-stakeholder tensions, creating risks in accountability and bias. This paper introduces the Advisory Governance Layer (AGL), a non-intrusive, multi-agent framework designed to enable distributed stakeholder participation in AI governance. The AGL employs specialized agents representing stakeholder groups to evaluate pedagogical actions against their spe-cific policies in a privacy-preserving manner, anticipating future advances in per-sonal assistant technology that will enhance stakeholder value expression. Through a novel policy taxonomy and conflict-resolution protocols, the frame-work provides structured, auditable governance advice to the ITS without altering its core pedagogical decision-making. This work contributes a reference architec-ture and technical specifications for aligning educational AI with multi-stakeholder values, bridging the gap between high-level ethical principles and practical implementation.",
    "fetched_at": "2025-10-28T05:41:45.731433Z"
  },
  {
    "id": "2510.23408v1",
    "title": "AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream   Processing Pipelines",
    "date": "2025-10-27",
    "tags": [
      "cs.AI",
      "AI",
      "cs.DC",
      "DC",
      "cs.ET",
      "ET",
      "cs.LG",
      "LG",
      "cs.MA",
      "MA"
    ],
    "authors": [
      "Abolfazl Younesi",
      "Zahra Najafabadi Samani",
      "Thomas Fahringer"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23408v1",
    "abstract": "Data pipelines are essential in stream processing as they enable the efficient collection, processing, and delivery of real-time data, supporting rapid data analysis. In this paper, we present AutoStreamPipe, a novel framework that employs Large Language Models (LLMs) to automate the design, generation, and deployment of stream processing pipelines. AutoStreamPipe bridges the semantic gap between high-level user intent and platform-specific implementations across distributed stream processing systems for structured multi-agent reasoning by integrating a Hypergraph of Thoughts (HGoT) as an extended version of GoT. AutoStreamPipe combines resilient execution strategies, advanced query analysis, and HGoT to deliver pipelines with good accuracy. Experimental evaluations on diverse pipelines demonstrate that AutoStreamPipe significantly reduces development time (x6.3) and error rates (x5.19), as measured by a novel Error-Free Score (EFS), compared to LLM code-generation methods.",
    "fetched_at": "2025-10-28T05:41:45.731229Z"
  },
  {
    "id": "2510.22907v1",
    "title": "Language Server CLI Empowers Language Agents with Process Rewards",
    "date": "2025-10-27",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI",
      "cs.PL",
      "PL",
      "cs.SE",
      "SE"
    ],
    "authors": [
      "Yifan Zhang",
      "Lanser Contributors"
    ],
    "institution": "Meta",
    "link": "http://arxiv.org/pdf/2510.22907v1",
    "abstract": "Large language models routinely hallucinate APIs and mislocalize edits, while language servers compute verified, IDE-grade facts about real code. We present Lanser-CLI, a CLI-first orchestration layer that pins and mediates a Language Server Protocol (LSP) server for coding agents and CI, exposing deterministic, replayable workflows. Our position is that language servers provide not only structural information (definitions, references, types, diagnostics) but also an actionable process reward: machine-checked, step-wise signals that align an agent's planning loop with program reality. In this work, Lanser-CLI contributes: (i) a robust addressing scheme beyond brittle \"file:line:col\" via a Selector DSL (symbolic, AST-path, and content-anchored selectors) with a principled relocation algorithm; (ii) deterministic Analysis Bundles that normalize Language Server responses and capture environment/capability metadata with stable content hashes; (iii) a safety envelope for mutating operations (rename, code actions) with preview, workspace jails, and Git-aware, transactional apply; and (iv) a process-reward functional derived from Language Server facts (diagnostic deltas, disambiguation confidence, and safe-apply checks) that is computable online and replayable offline. We formalize determinism under frozen snapshots and establish a monotonicity property for the process reward, making it suitable for process supervision and counterfactual analysis. Project Page: https://github.com/yifanzhang-pro/lanser-cli",
    "fetched_at": "2025-10-28T05:41:44.624048Z"
  },
  {
    "id": "2510.22940v1",
    "title": "RL-AUX: Reinforcement Learning for Auxiliary Task Generation",
    "date": "2025-10-27",
    "tags": [
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Judah Goldfeder",
      "Matthew So",
      "Hod Lipson"
    ],
    "institution": "Meta",
    "link": "http://arxiv.org/pdf/2510.22940v1",
    "abstract": "Auxiliary Learning (AL) is a special case of Multi-task Learning (MTL) in which a network trains on auxiliary tasks to improve performance on its main task. This technique is used to improve generalization and, ultimately, performance on the network's main task. AL has been demonstrated to improve performance across multiple domains, including navigation, image classification, and natural language processing. One weakness of AL is the need for labeled auxiliary tasks, which can require human effort and domain expertise to generate. Meta Learning techniques have been used to solve this issue by learning an additional auxiliary task generation network that can create helpful tasks for the primary network. The most prominent techniques rely on Bi-Level Optimization, which incurs computational cost and increased code complexity. To avoid the need for Bi-Level Optimization, we present an RL-based approach to dynamically create auxiliary tasks. In this framework, an RL agent is tasked with selecting auxiliary labels for every data point in a training set. The agent is rewarded when their selection improves the performance on the primary task. We also experiment with learning optimal strategies for weighing the auxiliary loss per data point. On the 20-Superclass CIFAR100 problem, our RL approach outperforms human-labeled auxiliary tasks and performs as well as a prominent Bi-Level Optimization technique. Our weight learning approaches significantly outperform all of these benchmarks. For example, a Weight-Aware RL-based approach helps the VGG16 architecture achieve 80.9% test accuracy while the human-labeled auxiliary task setup achieved 75.53%. The goal of this work is to (1) prove that RL is a viable approach to dynamically generate auxiliary tasks and (2) demonstrate that per-sample auxiliary task weights can be learned alongside the auxiliary task labels and can achieve strong results.",
    "fetched_at": "2025-10-28T05:41:44.624004Z"
  },
  {
    "id": "2510.22969v1",
    "title": "Multi-Agent Conditional Diffusion Model with Mean Field Communication as   Wireless Resource Allocation Planner",
    "date": "2025-10-27",
    "tags": [
      "cs.AI",
      "AI",
      "cs.MA",
      "MA"
    ],
    "authors": [
      "Kechen Meng",
      "Sinuo Zhang",
      "Rongpeng Li",
      "Xiangming Meng",
      "Chan Wang",
      "Ming Lei",
      "Zhifeng Zhao"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.22969v1",
    "abstract": "In wireless communication systems, efficient and adaptive resource allocation plays a crucial role in enhancing overall Quality of Service (QoS). While centralized Multi-Agent Reinforcement Learning (MARL) frameworks rely on a central coordinator for policy training and resource scheduling, they suffer from scalability issues and privacy risks. In contrast, the Distributed Training with Decentralized Execution (DTDE) paradigm enables distributed learning and decision-making, but it struggles with non-stationarity and limited inter-agent cooperation, which can severely degrade system performance. To overcome these challenges, we propose the Multi-Agent Conditional Diffusion Model Planner (MA-CDMP) for decentralized communication resource management. Built upon the Model-Based Reinforcement Learning (MBRL) paradigm, MA-CDMP employs Diffusion Models (DMs) to capture environment dynamics and plan future trajectories, while an inverse dynamics model guides action generation, thereby alleviating the sample inefficiency and slow convergence of conventional DTDE methods. Moreover, to approximate large-scale agent interactions, a Mean-Field (MF) mechanism is introduced as an assistance to the classifier in DMs. This design mitigates inter-agent non-stationarity and enhances cooperation with minimal communication overhead in distributed settings. We further theoretically establish an upper bound on the distributional approximation error introduced by the MF-based diffusion generation, guaranteeing convergence stability and reliable modeling of multi-agent stochastic dynamics. Extensive experiments demonstrate that MA-CDMP consistently outperforms existing MARL baselines in terms of average reward and QoS metrics, showcasing its scalability and practicality for real-world wireless network optimization.",
    "fetched_at": "2025-10-28T05:41:44.623956Z"
  },
  {
    "id": "2510.22986v1",
    "title": "CodeAD: Synthesize Code of Rules for Log-based Anomaly Detection with   LLMs",
    "date": "2025-10-27",
    "tags": [
      "cs.SE",
      "SE",
      "cs.DC",
      "DC",
      "cs.MA",
      "MA"
    ],
    "authors": [
      "Junjie Huang",
      "Minghua He",
      "Jinyang Liu",
      "Yintong Huo",
      "Domenico Bianculli",
      "Michael R. Lyu"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.22986v1",
    "abstract": "Log-based anomaly detection (LogAD) is critical for maintaining the reliability and availability of large-scale online service systems. While machine learning, deep learning, and large language models (LLMs)-based methods have advanced the LogAD, they often suffer from limited interpretability, high inference costs, and extensive preprocessing requirements, limiting their practicality for real-time, high-volume log analysis. In contrast, rule-based systems offer efficiency and transparency, but require significant manual effort and are difficult to scale across diverse and evolving environments. In this paper, We present CodeAD, a novel framework that automatically synthesizes lightweight Python rule functions for LogAD using LLMs. CodeAD introduces a hierarchical clustering and anchor-grounded sampling strategy to construct representative contrastive log windows, enabling LLMs to discern discriminative anomaly patterns. To ensure robustness and generalizability, CodeAD employs an agentic workflow that iteratively generates, tests, repairs, and refines the rules until it meets correctness and abstraction requirements. The synthesized rules are interpretable, lightweight, and directly executable on raw logs, supporting efficient and transparent online anomaly detection. Our comprehensive experiments on three public datasets (BGL, Hadoop, Thunderbird) demonstrate that CodeAD achieves an average absolute improvement of 3.6% F1 score over the state-of-the-art baselines, while processing large datasets up to 4x faster and at a fraction of the cost (total LLM invocation cost under 4 USD per dataset). These results highlight CodeAD as a practical and scalable solution for online monitoring systems, enabling interpretable, efficient, and automated LogAD in real-world environment.",
    "fetched_at": "2025-10-28T05:41:44.623894Z"
  },
  {
    "id": "2510.23010v1",
    "title": "TALM: Dynamic Tree-Structured Multi-Agent Framework with Long-Term   Memory for Scalable Code Generation",
    "date": "2025-10-27",
    "tags": [
      "cs.SE",
      "SE"
    ],
    "authors": [
      "Ming-Tung Shen",
      "Yuh-Jzer Joung"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23010v1",
    "abstract": "Agentic code generation requires large language models (LLMs) capable of complex context management and multi-step reasoning. Prior multi-agent frameworks attempt to address these challenges through collaboration, yet they often suffer from rigid workflows and high reasoning recovery costs. To overcome these limitations, we propose TALM (Tree-Structured Multi-Agent Framework with Long-Term Memory), a dynamic framework that integrates structured task decomposition, localized re-reasoning, and long-term memory mechanisms. TALM employs an extensible tree-based collaboration structure. The parent-child relationships, when combined with a divide-and-conquer strategy, enhance reasoning flexibility and enable efficient error correction across diverse task scopes. Furthermore, a long-term memory module enables semantic querying and integration of prior knowledge, supporting implicit self-improvement through experience reuse. Experimental results on HumanEval, BigCodeBench, and ClassEval benchmarks demonstrate that TALM consistently delivers strong reasoning performance and high token efficiency, highlighting its robustness and practical utility in complex code generation tasks.",
    "fetched_at": "2025-10-28T05:41:44.623835Z"
  },
  {
    "id": "2510.23038v1",
    "title": "Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated   Reinforcement Learning",
    "date": "2025-10-27",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Ran Xu",
      "Jingjing Chen",
      "Jiayu Ye",
      "Yu Wu",
      "Jun Yan",
      "Carl Yang",
      "Hongkun Yu"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23038v1",
    "abstract": "Large Language Models (LLMs) are widely used as judges to evaluate response quality, providing a scalable alternative to human evaluation. However, most LLM judges operate solely on intrinsic text-based reasoning, limiting their ability to verify complex constraints or perform accurate computation. Motivated by the success of tool-integrated reasoning (TIR) in numerous tasks, we propose TIR-Judge, an end-to-end RL framework for training LLM judges that integrates a code executor for precise evaluation. TIR-Judge is built on three principles: (i) diverse training across verifiable and non-verifiable domains, (ii) flexible judgment formats (pointwise, pairwise, listwise), and (iii) iterative RL that bootstraps directly from the initial model without distillation. On seven public benchmarks, TIR-Judge surpasses strong reasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and achieves listwise performance comparable to Claude-Opus-4 despite having only 8B parameters. Remarkably, TIR-Judge-Zero - trained entirely without distilled judge trajectories, matches the performance of distilled variants, demonstrating that tool-augmented judges can self-evolve through iterative reinforcement learning.",
    "fetched_at": "2025-10-28T05:41:44.623795Z"
  },
  {
    "id": "2510.23053v1",
    "title": "AirFed: Federated Graph-Enhanced Multi-Agent Reinforcement Learning for   Multi-UAV Cooperative Mobile Edge Computing",
    "date": "2025-10-27",
    "tags": [
      "cs.LG",
      "LG",
      "cs.DC",
      "DC"
    ],
    "authors": [
      "Zhiyu Wang",
      "Suman Raj",
      "Rajkumar Buyya"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23053v1",
    "abstract": "Multiple Unmanned Aerial Vehicles (UAVs) cooperative Mobile Edge Computing (MEC) systems face critical challenges in coordinating trajectory planning, task offloading, and resource allocation while ensuring Quality of Service (QoS) under dynamic and uncertain environments. Existing approaches suffer from limited scalability, slow convergence, and inefficient knowledge sharing among UAVs, particularly when handling large-scale IoT device deployments with stringent deadline constraints. This paper proposes AirFed, a novel federated graph-enhanced multi-agent reinforcement learning framework that addresses these challenges through three key innovations. First, we design dual-layer dynamic Graph Attention Networks (GATs) that explicitly model spatial-temporal dependencies among UAVs and IoT devices, capturing both service relationships and collaborative interactions within the network topology. Second, we develop a dual-Actor single-Critic architecture that jointly optimizes continuous trajectory control and discrete task offloading decisions. Third, we propose a reputation-based decentralized federated learning mechanism with gradient-sensitive adaptive quantization, enabling efficient and robust knowledge sharing across heterogeneous UAVs. Extensive experiments demonstrate that AirFed achieves 42.9% reduction in weighted cost compared to state-of-the-art baselines, attains over 99% deadline satisfaction and 94.2% IoT device coverage rate, and reduces communication overhead by 54.5%. Scalability analysis confirms robust performance across varying UAV numbers, IoT device densities, and system scales, validating AirFed's practical applicability for large-scale UAV-MEC deployments.",
    "fetched_at": "2025-10-28T05:41:44.623738Z"
  },
  {
    "id": "2510.23076v1",
    "title": "Periodic event-triggered impulsive control for fully heterogeneous   stochastic multi-agent systems with a time-varying topology",
    "date": "2025-10-27",
    "tags": [
      "math.DS",
      "DS"
    ],
    "authors": [
      "Xuetao Yang",
      "Ruilu An",
      "Quanxin Zhu"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23076v1",
    "abstract": "In this paper, we focus on a periodic event-triggered impulsive control (PETIC) for fully heterogeneous stochastic multi-agent systems (MASs) with a time-varying topology. Firstly, a novel time-varying topology is established by incorporating the energy consumption of each agent. This topology enables active adjustment of the information interaction intensity between agents. Secondly, to address the difficulties that agents with different dimensions cannot communicate in fully heterogeneous stochastic MASs, a virtual state space is designed. According to the above framework, novel PETICs with/without actuation delays are presented to achieve the mean-square exponential consensus of fully heterogeneous stochastic MASs. Finally, the effectiveness of the proposed methods is verified through a numerical simulation of unmanned aerial vehicles and unmanned ground vehicles.",
    "fetched_at": "2025-10-28T05:41:44.623689Z"
  },
  {
    "id": "2510.23148v1",
    "title": "Adapting Interleaved Encoders with PPO for Language-Guided Reinforcement   Learning in BabyAI",
    "date": "2025-10-27",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI",
      "eess.IV",
      "IV",
      "I.2.6; I.2.9; I.5.4",
      "4"
    ],
    "authors": [
      "Aryan Mathur",
      "Asaduddin Ahmed"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23148v1",
    "abstract": "Deep reinforcement learning agents often struggle when tasks require understanding both vision and language. Conventional architectures typically isolate perception (for example, CNN-based visual encoders) from decision-making (policy networks). This separation can be inefficient, since the policy's failures do not directly help the perception module learn what is important. To address this, we implement the Perception-Decision Interleaving Transformer (PDiT) architecture introduced by Mao et al. (2023), a model that alternates between perception and decision layers within a single transformer. This interleaving allows feedback from decision-making to refine perceptual features dynamically. In addition, we integrate a contrastive loss inspired by CLIP to align textual mission embeddings with visual scene features. We evaluate the PDiT encoders on the BabyAI GoToLocal environment and find that the approach achieves more stable rewards and stronger alignment compared to a standard PPO baseline. The results suggest that interleaved transformer encoders are a promising direction for developing more integrated autonomous agents.",
    "fetched_at": "2025-10-28T05:41:44.623644Z"
  },
  {
    "id": "2510.23182v1",
    "title": "SI-Bench: Benchmarking Social Intelligence of Large Language Models in   Human-to-Human Conversations",
    "date": "2025-10-27",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Shuai Huang",
      "Wenxuan Zhao",
      "Jun Gao"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23182v1",
    "abstract": "As large language models (LLMs) develop anthropomorphic abilities, they are increasingly being deployed as autonomous agents to interact with humans. However, evaluating their performance in realistic and complex social interactions remains a significant challenge. Most previous research built datasets through simulated agent-to-agent interactions, which fails to capture the authentic linguistic styles and relational dynamics found in real human conversations. To address this gap, we introduce SI-Bench, a novel benchmark designed to evaluate aspects of social intelligence in LLMs. Grounded in broad social science theories, SI-Bench contains 2,221 authentic multi-turn dialogues collected from a social networking application. We further selected a subset of 312 dialogues for manual annotation across 8 major models. The experiments show that SOTA models have surpassed the human expert in process reasoning under complex social situations, yet they still fall behind humans in reply quality. Moreover, introducing Chain-of-Thought (CoT) reasoning may degrade the performance of LLMs in social dialogue tasks. All datasets are openly available at https://github.com/SI-Bench/SI-Bench.git.",
    "fetched_at": "2025-10-28T05:41:44.623581Z"
  },
  {
    "id": "2510.23190v1",
    "title": "Evaluation of Vision-LLMs in Surveillance Video",
    "date": "2025-10-27",
    "tags": [
      "cs.CV",
      "CV"
    ],
    "authors": [
      "Pascal Benschop",
      "Cristian Meo",
      "Justin Dauwels",
      "Jelte P. Mense"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23190v1",
    "abstract": "The widespread use of cameras in our society has created an overwhelming amount of video data, far exceeding the capacity for human monitoring. This presents a critical challenge for public safety and security, as the timely detection of anomalous or criminal events is crucial for effective response and prevention. The ability for an embodied agent to recognize unexpected events is fundamentally tied to its capacity for spatial reasoning. This paper investigates the spatial reasoning of vision-language models (VLMs) by framing anomalous action recognition as a zero-shot, language-grounded task, addressing the embodied perception challenge of interpreting dynamic 3D scenes from sparse 2D video. Specifically, we investigate whether small, pre-trained vision--LLMs can act as spatially-grounded, zero-shot anomaly detectors by converting video into text descriptions and scoring labels via textual entailment. We evaluate four open models on UCF-Crime and RWF-2000 under prompting and privacy-preserving conditions. Few-shot exemplars can improve accuracy for some models, but may increase false positives, and privacy filters -- especially full-body GAN transforms -- introduce inconsistencies that degrade accuracy. These results chart where current vision--LLMs succeed (simple, spatially salient events) and where they falter (noisy spatial cues, identity obfuscation). Looking forward, we outline concrete paths to strengthen spatial grounding without task-specific training: structure-aware prompts, lightweight spatial memory across clips, scene-graph or 3D-pose priors during description, and privacy methods that preserve action-relevant geometry. This positions zero-shot, language-grounded pipelines as adaptable building blocks for embodied, real-world video understanding. Our implementation for evaluating VLMs is publicly available at: https://github.com/pascalbenschopTU/VLLM_AnomalyRecognition",
    "fetched_at": "2025-10-28T05:41:44.623537Z"
  },
  {
    "id": "2510.23216v1",
    "title": "Human-Like Goalkeeping in a Realistic Football Simulation: a   Sample-Efficient Reinforcement Learning Approach",
    "date": "2025-10-27",
    "tags": [
      "cs.AI",
      "AI",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Alessandro Sestini",
      "Joakim Bergdahl",
      "Jean-Philippe Barrette-LaPierre",
      "Florian Fuchs",
      "Brady Chen",
      "Micheal Jones",
      "Linus Gisslén"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23216v1",
    "abstract": "While several high profile video games have served as testbeds for Deep Reinforcement Learning (DRL), this technique has rarely been employed by the game industry for crafting authentic AI behaviors. Previous research focuses on training super-human agents with large models, which is impractical for game studios with limited resources aiming for human-like agents. This paper proposes a sample-efficient DRL method tailored for training and fine-tuning agents in industrial settings such as the video game industry. Our method improves sample efficiency of value-based DRL by leveraging pre-collected data and increasing network plasticity. We evaluate our method training a goalkeeper agent in EA SPORTS FC 25, one of the best-selling football simulations today. Our agent outperforms the game's built-in AI by 10% in ball saving rate. Ablation studies show that our method trains agents 50% faster compared to standard DRL methods. Finally, qualitative evaluation from domain experts indicates that our approach creates more human-like gameplay compared to hand-crafted agents. As a testimony of the impact of the approach, the method is intended to replace the hand-crafted counterpart in next iterations of the series.",
    "fetched_at": "2025-10-28T05:41:44.623484Z"
  },
  {
    "id": "2510.23272v1",
    "title": "Code Aesthetics with Agentic Reward Feedback",
    "date": "2025-10-27",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Bang Xiao",
      "Lingjie Jiang",
      "Shaohan Huang",
      "Tengchao Lv",
      "Yupan Huang",
      "Xun Wu",
      "Lei Cui",
      "Furu Wei"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23272v1",
    "abstract": "Large Language Models (LLMs) have become valuable assistants for developers in code-related tasks. While LLMs excel at traditional programming tasks such as code generation and bug fixing, they struggle with visually-oriented coding tasks, often producing suboptimal aesthetics. In this paper, we introduce a new pipeline to enhance the aesthetic quality of LLM-generated code. We first construct AesCode-358K, a large-scale instruction-tuning dataset focused on code aesthetics. Next, we propose agentic reward feedback, a multi-agent system that evaluates executability, static aesthetics, and interactive aesthetics. Building on this, we develop GRPO-AR, which integrates these signals into the GRPO algorithm for joint optimization of functionality and code aesthetics. Finally, we develop OpenDesign, a benchmark for assessing code aesthetics. Experimental results show that combining supervised fine-tuning on AesCode-358K with reinforcement learning using agentic reward feedback significantly improves performance on OpenDesign and also enhances results on existing benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o and GPT-4.1, and achieves performance comparable to large open-source models with 480B-685B parameters, underscoring the effectiveness of our approach.",
    "fetched_at": "2025-10-28T05:41:44.623426Z"
  },
  {
    "id": "2510.23304v1",
    "title": "CNOT Minimal Circuit Synthesis: A Reinforcement Learning Approach",
    "date": "2025-10-27",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Riccardo Romanello",
      "Daniele Lizzio Bosco",
      "Jacopo Cossio",
      "Dusan Sutulovic",
      "Giuseppe Serra",
      "Carla Piazza",
      "Paolo Burelli"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23304v1",
    "abstract": "CNOT gates are fundamental to quantum computing, as they facilitate entanglement, a crucial resource for quantum algorithms. Certain classes of quantum circuits are constructed exclusively from CNOT gates. Given their widespread use, it is imperative to minimise the number of CNOT gates employed. This problem, known as CNOT minimisation, remains an open challenge, with its computational complexity yet to be fully characterised. In this work, we introduce a novel reinforcement learning approach to address this task. Instead of training multiple reinforcement learning agents for different circuit sizes, we use a single agent up to a fixed size $m$. Matrices of sizes different from m are preprocessed using either embedding or Gaussian striping. To assess the efficacy of our approach, we trained an agent with m = 8, and evaluated it on matrices of size n that range from 3 to 15. The results we obtained show that our method overperforms the state-of-the-art algorithm as the value of n increases.",
    "fetched_at": "2025-10-28T05:41:44.623358Z"
  },
  {
    "id": "2510.23340v1",
    "title": "Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by   Projecting User Awareness across Future Timesteps",
    "date": "2025-10-27",
    "tags": [
      "cs.AI",
      "AI",
      "cs.CL",
      "CL",
      "cs.HC",
      "HC"
    ],
    "authors": [
      "Anwesha Das",
      "John Duff",
      "Jörg Hoffmann",
      "Vera Demberg"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23340v1",
    "abstract": "Adaptive agent design offers a way to improve human-AI collaboration on time-sensitive tasks in rapidly changing environments. In such cases, to ensure the human maintains an accurate understanding of critical task elements, an assistive agent must not only identify the highest priority information but also estimate how and when this information can be communicated most effectively, given that human attention represents a zero-sum cognitive resource where focus on one message diminishes awareness of other or upcoming information. We introduce a theoretical framework for adaptive signalling which meets these challenges by using principles of rational communication, formalised as Bayesian reference resolution using the Rational Speech Act (RSA) modelling framework, to plan a sequence of messages which optimise timely alignment between user belief and a dynamic environment. The agent adapts message specificity and timing to the particulars of a user and scenario based on projections of how prior-guided interpretation of messages will influence attention to the interface and subsequent belief update, across several timesteps out to a fixed horizon. In a comparison to baseline methods, we show that this effectiveness depends crucially on combining multi-step planning with a realistic model of user awareness. As the first application of RSA for communication in a dynamic environment, and for human-AI interaction in general, we establish theoretical foundations for pragmatic communication in human-agent teams, highlighting how insights from cognitive science can be capitalised to inform the design of assistive agents.",
    "fetched_at": "2025-10-28T05:41:44.623298Z"
  },
  {
    "id": "2510.23397v1",
    "title": "VideoTG-R1: Boosting Video Temporal Grounding via Curriculum   Reinforcement Learning on Reflected Boundary Annotations",
    "date": "2025-10-27",
    "tags": [
      "cs.CV",
      "CV"
    ],
    "authors": [
      "Lu Dong",
      "Haiyu Zhang",
      "Han Lin",
      "Ziang Yan",
      "Xiangyu Zeng",
      "Hongjie Zhang",
      "Yifei Huang",
      "Yi Wang",
      "Zhen-Hua Ling",
      "Limin Wang",
      "Yali Wang"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23397v1",
    "abstract": "Video temporal grounding (VTG) aims to locate precise segments in videos based on language queries, which is a fundamental challenge in video understanding. While recent Multimodal Large Language Models (MLLMs) have shown promise in tackling VTG through reinforcement learning (RL), they overlook the challenges arising from both the quality and difficulty of training samples. (1) Partially annotated samples. Many samples contain relevant segments beyond the annotated interval, introducing ambiguous supervision. (2) Hard-to-ground samples. Samples with poor zero-shot performance produce consistently low and indistinguishable rewards during RL training, exhibiting no clear preference among multiple outputs and thus hindering learning efficiency. To address these challenges, we propose VideoTG-R1, a novel curriculum RL framework with reflected boundary annotations, enabling data-efficient training. Specifically, we propose a Boundary Reflection Agent that utilizes MLLMs to predict query-relevant timestamps outside the annotated intervals, allowing us to identify and filter out partially annotated samples, thereby reducing ambiguity. Furthermore, we introduce a Difficulty Estimation Agent to assess the training difficulty of each sample and design a curriculum RL strategy that dynamically masks the videos of hard-to-ground samples according to the training steps, easing the training difficulty and providing clearer preference. Experiments on the VTG and grounded VideoQA tasks demonstrate the effectiveness of our method. Remarkably, with only 10% of the training samples and 21% of the computational budget, VideoTG-R1 outperforms full-data counterparts under both group relative policy optimization (GRPO) and supervised fine-tuning (SFT). The code is available at https://github.com/ldong1111/VideoTG-R1.",
    "fetched_at": "2025-10-28T05:41:44.623189Z"
  },
  {
    "id": "2510.23424v1",
    "title": "Causal Deep Q Network",
    "date": "2025-10-27",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Elouanes Khelifi",
      "Amir Saki",
      "Usef Faghihi"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23424v1",
    "abstract": "Deep Q Networks (DQN) have shown remarkable success in various reinforcement learning tasks. However, their reliance on associative learning often leads to the acquisition of spurious correlations, hindering their problem-solving capabilities. In this paper, we introduce a novel approach to integrate causal principles into DQNs, leveraging the PEACE (Probabilistic Easy vAriational Causal Effect) formula for estimating causal effects. By incorporating causal reasoning during training, our proposed framework enhances the DQN's understanding of the underlying causal structure of the environment, thereby mitigating the influence of confounding factors and spurious correlations. We demonstrate that integrating DQNs with causal capabilities significantly enhances their problem-solving capabilities without compromising performance. Experimental results on standard benchmark environments showcase that our approach outperforms conventional DQNs, highlighting the effectiveness of causal reasoning in reinforcement learning. Overall, our work presents a promising avenue for advancing the capabilities of deep reinforcement learning agents through principled causal inference.",
    "fetched_at": "2025-10-28T05:41:44.623110Z"
  },
  {
    "id": "2510.23476v1",
    "title": "Human-AI Collaborative Uncertainty Quantification",
    "date": "2025-10-27",
    "tags": [
      "cs.AI",
      "AI",
      "cs.HC",
      "HC",
      "stat.ML",
      "ML"
    ],
    "authors": [
      "Sima Noorani",
      "Shayan Kiyani",
      "George Pappas",
      "Hamed Hassani"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23476v1",
    "abstract": "AI predictive systems are increasingly embedded in decision making pipelines, shaping high stakes choices once made solely by humans. Yet robust decisions under uncertainty still rely on capabilities that current AI lacks: domain knowledge not captured by data, long horizon context, and reasoning grounded in the physical world. This gap has motivated growing efforts to design collaborative frameworks that combine the complementary strengths of humans and AI. This work advances this vision by identifying the fundamental principles of Human AI collaboration within uncertainty quantification, a key component of reliable decision making. We introduce Human AI Collaborative Uncertainty Quantification, a framework that formalizes how an AI model can refine a human expert's proposed prediction set with two goals: avoiding counterfactual harm, ensuring the AI does not degrade correct human judgments, and complementarity, enabling recovery of correct outcomes the human missed. At the population level, we show that the optimal collaborative prediction set follows an intuitive two threshold structure over a single score function, extending a classical result in conformal prediction. Building on this insight, we develop practical offline and online calibration algorithms with provable distribution free finite sample guarantees. The online method adapts to distribution shifts, including human behavior evolving through interaction with AI, a phenomenon we call Human to AI Adaptation. Experiments across image classification, regression, and text based medical decision making show that collaborative prediction sets consistently outperform either agent alone, achieving higher coverage and smaller set sizes across various conditions.",
    "fetched_at": "2025-10-28T05:41:44.623042Z"
  },
  {
    "id": "2510.23535v1",
    "title": "Sequential Multi-Agent Dynamic Algorithm Configuration",
    "date": "2025-10-27",
    "tags": [
      "cs.LG",
      "LG",
      "cs.NE",
      "NE"
    ],
    "authors": [
      "Chen Lu",
      "Ke Xue",
      "Lei Yuan",
      "Yao Wang",
      "Yaoyuan Wang",
      "Sheng Fu",
      "Chao Qian"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23535v1",
    "abstract": "Dynamic algorithm configuration (DAC) is a recent trend in automated machine learning, which can dynamically adjust the algorithm's configuration during the execution process and relieve users from tedious trial-and-error tuning tasks. Recently, multi-agent reinforcement learning (MARL) approaches have improved the configuration of multiple heterogeneous hyperparameters, making various parameter configurations for complex algorithms possible. However, many complex algorithms have inherent inter-dependencies among multiple parameters (e.g., determining the operator type first and then the operator's parameter), which are, however, not considered in previous approaches, thus leading to sub-optimal results. In this paper, we propose the sequential multi-agent DAC (Seq-MADAC) framework to address this issue by considering the inherent inter-dependencies of multiple parameters. Specifically, we propose a sequential advantage decomposition network, which can leverage action-order information through sequential advantage decomposition. Experiments from synthetic functions to the configuration of multi-objective optimization algorithms demonstrate Seq-MADAC's superior performance over state-of-the-art MARL methods and show strong generalization across problem classes. Seq-MADAC establishes a new paradigm for the widespread dependency-aware automated algorithm configuration. Our code is available at https://github.com/lamda-bbo/seq-madac.",
    "fetched_at": "2025-10-28T05:41:44.622882Z"
  },
  {
    "id": "2510.23458v1",
    "title": "BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents",
    "date": "2025-10-27",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Litu Ou",
      "Kuan Li",
      "Huifeng Yin",
      "Liwen Zhang",
      "Zhongwang Zhang",
      "Xixi Wu",
      "Rui Ye",
      "Zile Qiao",
      "Yong Jiang",
      "Pengjun Xie",
      "Fei Huang",
      "Jingren Zhou"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23458v1",
    "abstract": "Confidence in LLMs is a useful indicator of model uncertainty and answer reliability. Existing work mainly focused on single-turn scenarios, while research on confidence in complex multi-turn interactions is limited. In this paper, we investigate whether LLM-based search agents have the ability to communicate their own confidence through verbalized confidence scores after long sequences of actions, a significantly more challenging task compared to outputting confidence in a single interaction. Experimenting on open-source agentic models, we first find that models exhibit much higher task accuracy at high confidence while having near-zero accuracy when confidence is low. Based on this observation, we propose Test-Time Scaling (TTS) methods that use confidence scores to determine answer quality, encourage the model to try again until reaching a satisfactory confidence level. Results show that our proposed methods significantly reduce token consumption while demonstrating competitive performance compared to baseline fixed budget TTS methods.",
    "fetched_at": "2025-10-28T05:41:43.501730Z"
  },
  {
    "id": "2510.23487v1",
    "title": "Are Agents Just Automata? On the Formal Equivalence Between Agentic AI   and the Chomsky Hierarchy",
    "date": "2025-10-27",
    "tags": [
      "cs.AI",
      "AI",
      "cs.FL",
      "FL"
    ],
    "authors": [
      "Roham Koohestani",
      "Ziyou Li",
      "Anton Podkopaev",
      "Maliheh Izadi"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23487v1",
    "abstract": "This paper establishes a formal equivalence between the architectural classes of modern agentic AI systems and the abstract machines of the Chomsky hierarchy. We posit that the memory architecture of an AI agent is the definitive feature determining its computational power and that it directly maps it to a corresponding class of automaton. Specifically, we demonstrate that simple reflex agents are equivalent to Finite Automata, hierarchical task-decomposition agents are equivalent to Pushdown Automata, and agents employing readable/writable memory for reflection are equivalent to TMs. This Automata-Agent Framework provides a principled methodology for right-sizing agent architectures to optimize computational efficiency and cost. More critically, it creates a direct pathway to formal verification, enables the application of mature techniques from automata theory to guarantee agent safety and predictability. By classifying agents, we can formally delineate the boundary between verifiable systems and those whose behavior is fundamentally undecidable. We address the inherent probabilistic nature of LLM-based agents by extending the framework to probabilistic automata that allow quantitative risk analysis. The paper concludes by outlining an agenda for developing static analysis tools and grammars for agentic frameworks.",
    "fetched_at": "2025-10-28T05:41:43.501653Z"
  },
  {
    "id": "2510.23509v1",
    "title": "Deductive Chain-of-Thought Augmented Socially-aware Robot Navigation   World Model",
    "date": "2025-10-27",
    "tags": [
      "cs.RO",
      "RO"
    ],
    "authors": [
      "Weizheng Wang",
      "Obi Ike",
      "Soyun Choi",
      "Sungeun Hong",
      "Byung-Cheol Min"
    ],
    "institution": "Google, MIT",
    "link": "http://arxiv.org/pdf/2510.23509v1",
    "abstract": "Social robot navigation increasingly relies on large language models for reasoning, path planning, and enabling movement in dynamic human spaces. However, relying solely on LLMs for planning often leads to unpredictable and unsafe behaviors, especially in dynamic human spaces, due to limited physical grounding and weak logical consistency. In this work, we introduce NaviWM, a socially-aware robot Navigation World Model that augments LLM reasoning with a structured world model and a logic-driven chain-of-thought process. NaviWM consists of two main components: (1) a spatial-temporal world model that captures the positions, velocities, and activities of agents in the environment, and (2) a deductive reasoning module that guides LLMs through a multi-step, logic-based inference process. This integration enables the robot to generate navigation decisions that are both socially compliant and physically safe, under well-defined constraints such as personal space, collision avoidance, and timing. Unlike previous methods based on prompting or fine-tuning, NaviWM encodes social norms as first-order logic, enabling interpretable and verifiable reasoning. Experiments show that NaviWM improves success rates and reduces social violations, particularly in crowded environments. These results demonstrate the benefit of combining formal reasoning with LLMs for robust social navigation. Additional experimental details and demo videos for this work can be found at: https://sites.google.com/view/NaviWM.",
    "fetched_at": "2025-10-28T05:41:43.501571Z"
  },
  {
    "id": "2510.23557v1",
    "title": "Minimizing Human Intervention in Online Classification",
    "date": "2025-10-27",
    "tags": [
      "stat.ML",
      "ML",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "William Réveillard",
      "Vasileios Saketos",
      "Alexandre Proutiere",
      "Richard Combes"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23557v1",
    "abstract": "We introduce and study an online problem arising in question answering systems. In this problem, an agent must sequentially classify user-submitted queries represented by $d$-dimensional embeddings drawn i.i.d. from an unknown distribution. The agent may consult a costly human expert for the correct label, or guess on her own without receiving feedback. The goal is to minimize regret against an oracle with free expert access. When the time horizon $T$ is at least exponential in the embedding dimension $d$, one can learn the geometry of the class regions: in this regime, we propose the Conservative Hull-based Classifier (CHC), which maintains convex hulls of expert-labeled queries and calls the expert as soon as a query lands outside all known hulls. CHC attains $\\mathcal{O}(\\log^d T)$ regret in $T$ and is minimax optimal for $d=1$. Otherwise, the geometry cannot be reliably learned without additional distributional assumptions. We show that when the queries are drawn from a subgaussian mixture, for $T \\le e^d$, a Center-based Classifier (CC) achieves regret proportional to $N\\log{N}$ where $N$ is the number of labels. To bridge these regimes, we introduce the Generalized Hull-based Classifier (GHC), a practical extension of CHC that allows for more aggressive guessing via a tunable threshold parameter. Our approach is validated with experiments, notably on real-world question-answering datasets using embeddings derived from state-of-the-art large language models.",
    "fetched_at": "2025-10-28T05:41:43.501516Z"
  },
  {
    "id": "2510.23564v1",
    "title": "ReCode: Unify Plan and Action for Universal Granularity Control",
    "date": "2025-10-27",
    "tags": [
      "cs.AI",
      "AI",
      "cs.CL",
      "CL",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Zhaoyang Yu",
      "Jiayi Zhang",
      "Huixue Su",
      "Yufan Zhao",
      "Yifan Wu",
      "Mingyi Deng",
      "Jinyu Xiang",
      "Yizhang Lin",
      "Lingxiao Tang",
      "Yingchao Li",
      "Yuyu Luo",
      "Bang Liu",
      "Chenglin Wu"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23564v1",
    "abstract": "Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode.",
    "fetched_at": "2025-10-28T05:41:43.501465Z"
  },
  {
    "id": "2510.23569v1",
    "title": "EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT",
    "date": "2025-10-27",
    "tags": [
      "cs.CV",
      "CV"
    ],
    "authors": [
      "Baoqi Pei",
      "Yifei Huang",
      "Jilan Xu",
      "Yuping He",
      "Guo Chen",
      "Fei Wu",
      "Yu Qiao",
      "Jiangmiao Pang"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23569v1",
    "abstract": "Egocentric video reasoning centers on an unobservable agent behind the camera who dynamically shapes the environment, requiring inference of hidden intentions and recognition of fine-grained interactions. This core challenge limits current multimodal large language models MLLMs, which excel at visible event reasoning but lack embodied, first-person understanding. To bridge this gap, we introduce EgoThinker, a novel framework that endows MLLMs with robust egocentric reasoning capabilities through spatio-temporal chain-of-thought supervision and a two-stage learning curriculum. First, we introduce EgoRe-5M, a large-scale egocentric QA dataset constructed from 13M diverse egocentric video clips. This dataset features multi-minute segments annotated with detailed CoT rationales and dense hand-object grounding. Second, we employ SFT on EgoRe-5M to instill reasoning skills, followed by reinforcement fine-tuning RFT to further enhance spatio-temporal localization. Experimental results show that EgoThinker outperforms existing methods across multiple egocentric benchmarks, while achieving substantial improvements in fine-grained spatio-temporal localization tasks. Full code and data are released at https://github.com/InternRobotics/EgoThinker.",
    "fetched_at": "2025-10-28T05:41:43.501381Z"
  },
  {
    "id": "2510.23571v1",
    "title": "RobotArena $\\infty$: Scalable Robot Benchmarking via Real-to-Sim   Translation",
    "date": "2025-10-27",
    "tags": [
      "cs.RO",
      "RO",
      "cs.AI",
      "AI",
      "cs.CV",
      "CV",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Yash Jangir",
      "Yidi Zhang",
      "Kashu Yamazaki",
      "Chenyu Zhang",
      "Kuan-Hsun Tu",
      "Tsung-Wei Ke",
      "Lei Ke",
      "Yonatan Bisk",
      "Katerina Fragkiadaki"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23571v1",
    "abstract": "The pursuit of robot generalists - instructable agents capable of performing diverse tasks across diverse environments - demands rigorous and scalable evaluation. Yet real-world testing of robot policies remains fundamentally constrained: it is labor-intensive, slow, unsafe at scale, and difficult to reproduce. Existing simulation benchmarks are similarly limited, as they train and test policies within the same synthetic domains and cannot assess models trained from real-world demonstrations or alternative simulation environments. As policies expand in scope and complexity, these barriers only intensify, since defining \"success\" in robotics often hinges on nuanced human judgments of execution quality. In this paper, we introduce a new benchmarking framework that overcomes these challenges by shifting VLA evaluation into large-scale simulated environments augmented with online human feedback. Leveraging advances in vision-language models, 2D-to-3D generative modeling, and differentiable rendering, our approach automatically converts video demonstrations from widely used robot datasets into simulated counterparts. Within these digital twins, we assess VLA policies using both automated VLM-guided scoring and scalable human preference judgments collected from crowdworkers, transforming human involvement from tedious scene setup, resetting, and safety supervision into lightweight preference comparisons. To measure robustness, we systematically perturb simulated environments along multiple axes, such as textures and object placements, stress-testing policy generalization under controlled variation. The result is a continuously evolving, reproducible, and scalable benchmark for real-world trained robot manipulation policies, addressing a critical missing capability in today's robotics landscape.",
    "fetched_at": "2025-10-28T05:41:43.501318Z"
  },
  {
    "id": "2510.23587v1",
    "title": "A Survey of Data Agents: Emerging Paradigm or Overstated Hype?",
    "date": "2025-10-27",
    "tags": [
      "cs.DB",
      "DB",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Yizhang Zhu",
      "Liangwei Wang",
      "Chenyu Yang",
      "Xiaotian Lin",
      "Boyan Li",
      "Wei Zhou",
      "Xinyu Liu",
      "Zhangyang Peng",
      "Tianqi Luo",
      "Yu Li",
      "Chengliang Chai",
      "Chong Chen",
      "Shimin Di",
      "Ju Fan",
      "Ji Sun",
      "Nan Tang",
      "Fugee Tsung",
      "Jiannan Wang",
      "Chenglin Wu",
      "Yanwei Xu",
      "Shaolei Zhang",
      "Yong Zhang",
      "Xuanhe Zhou",
      "Guoliang Li",
      "Yuyu Luo"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23587v1",
    "abstract": "The rapid advancement of large language models (LLMs) has spurred the emergence of data agents--autonomous systems designed to orchestrate Data + AI ecosystems for tackling complex data-related tasks. However, the term \"data agent\" currently suffers from terminological ambiguity and inconsistent adoption, conflating simple query responders with sophisticated autonomous architectures. This terminological ambiguity fosters mismatched user expectations, accountability challenges, and barriers to industry growth. Inspired by the SAE J3016 standard for driving automation, this survey introduces the first systematic hierarchical taxonomy for data agents, comprising six levels that delineate and trace progressive shifts in autonomy, from manual operations (L0) to a vision of generative, fully autonomous data agents (L5), thereby clarifying capability boundaries and responsibility allocation. Through this lens, we offer a structured review of existing research arranged by increasing autonomy, encompassing specialized data agents for data management, preparation, and analysis, alongside emerging efforts toward versatile, comprehensive systems with enhanced autonomy. We further analyze critical evolutionary leaps and technical gaps for advancing data agents, especially the ongoing L2-to-L3 transition, where data agents evolve from procedural execution to autonomous orchestration. Finally, we conclude with a forward-looking roadmap, envisioning the advent of proactive, generative data agents.",
    "fetched_at": "2025-10-28T05:41:43.501241Z"
  },
  {
    "id": "2510.23595v1",
    "title": "Multi-Agent Evolve: LLM Self-Improve through Co-evolution",
    "date": "2025-10-27",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Yixing Chen",
      "Yiding Wang",
      "Siqi Zhu",
      "Haofei Yu",
      "Tao Feng",
      "Muhan Zhan",
      "Mostofa Patwary",
      "Jiaxuan You"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23595v1",
    "abstract": "Reinforcement Learning (RL) has demonstrated significant potential in enhancing the reasoning capabilities of large language models (LLMs). However, the success of RL for LLMs heavily relies on human-curated datasets and verifiable rewards, which limit their scalability and generality. Recent Self-Play RL methods, inspired by the success of the paradigm in games and Go, aim to enhance LLM reasoning capabilities without human-annotated data. However, their methods primarily depend on a grounded environment for feedback (e.g., a Python interpreter or a game engine); extending them to general domains remains challenging. To address these challenges, we propose Multi-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in solving diverse tasks, including mathematics, reasoning, and general knowledge Q&A. The core design of MAE is based on a triplet of interacting agents (Proposer, Solver, Judge) that are instantiated from a single LLM, and applies reinforcement learning to optimize their behaviors. The Proposer generates questions, the Solver attempts solutions, and the Judge evaluates both while co-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves an average improvement of 4.54% on multiple benchmarks. These results highlight MAE as a scalable, data-efficient method for enhancing the general reasoning abilities of LLMs with minimal reliance on human-curated supervision.",
    "fetched_at": "2025-10-28T05:41:43.501113Z"
  },
  {
    "id": "2510.23601v1",
    "title": "Alita-G: Self-Evolving Generative Agent for Agent Generation",
    "date": "2025-10-27",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Jiahao Qiu",
      "Xuan Qi",
      "Hongru Wang",
      "Xinzhe Juan",
      "Yimin Wang",
      "Zelin Zhao",
      "Jiayi Geng",
      "Jiacheng Guo",
      "Peihang Li",
      "Jingzhe Shi",
      "Shilong Liu",
      "Mengdi Wang"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23601v1",
    "abstract": "Large language models (LLMs) have been shown to perform better when scaffolded into agents with memory, tools, and feedback. Beyond this, self-evolving agents have emerged, but current work largely limits adaptation to prompt rewriting or failure retries. Therefore, we present ALITA-G, a self-evolution framework that transforms a general-purpose agent into a domain expert by systematically generating, abstracting, and curating Model Context Protocol (MCP) tools. In this framework, a generalist agent executes a curated suite of target-domain tasks and synthesizes candidate MCPs from successful trajectories. These are then abstracted to parameterized primitives and consolidated into an MCP Box. At inference time, ALITA-G performs retrieval-augmented MCP selection with the help of each tool's descriptions and use cases, before executing an agent equipped with the MCP Executor. Across several benchmarks GAIA, PathVQA, and Humanity's Last Exam, ALITA-G attains strong gains while reducing computation costs. On GAIA validation, it achieves 83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result while reducing mean tokens per example by approximately 15% relative to a strong baseline agent. ALITA-G thus provides a principled pathway from generalist capability to reusable, domain-specific competence, improving both accuracy and efficiency on complex reasoning tasks.",
    "fetched_at": "2025-10-28T05:41:43.501026Z"
  },
  {
    "id": "2510.23449v1",
    "title": "Schrodinger Neural Network and Uncertainty Quantification: Quantum   Machine",
    "date": "2025-10-27",
    "tags": [
      "cs.LG",
      "LG"
    ],
    "authors": [
      "M. M. Hammad"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23449v1",
    "abstract": "We introduce the Schrodinger Neural Network (SNN), a principled architecture for conditional density estimation and uncertainty quantification inspired by quantum mechanics. The SNN maps each input to a normalized wave function on the output domain and computes predictive probabilities via the Born rule. The SNN departs from standard parametric likelihood heads by learning complex coefficients of a spectral expansion (e . g ., Chebyshev polynomials) whose squared modulus yields the conditional density $p(y|x)=\\left| \\psi _x(y)\\right| {}^2$ with analytic normalization. This representation confers three practical advantages: positivity and exact normalization by construction, native multimodality through interference among basis modes without explicit mixture bookkeeping, and yields closed-form (or efficiently computable) functionals$-$such as moments and several calibration diagnostics$-$as quadratic forms in coefficient space. We develop the statistical and computational foundations of the SNN, including (i) training by exact maximum-likelihood with unit-sphere coefficient parameterization, (ii) physics-inspired quadratic regularizers (kinetic and potential energies) motivated by uncertainty relations between localization and spectral complexity, (iii) scalable low-rank and separable extensions for multivariate outputs, (iv) operator-based extensions that represent observables, constraints, and weak labels as self-adjoint matrices acting on the amplitude space, and (v) a comprehensive framework for evaluating multimodal predictions. The SNN provides a coherent, tractable framework to elevate probabilistic prediction from point estimates to physically inspired amplitude-based distributions.",
    "fetched_at": "2025-10-28T03:51:07.300853Z"
  },
  {
    "id": "2510.23451v1",
    "title": "Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with   Free-Form Preferences",
    "date": "2025-10-27",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI",
      "cs.CV",
      "CV"
    ],
    "authors": [
      "Zhuoran Jin",
      "Hongbang Yuan",
      "Kejian Zhu",
      "Jiachun Li",
      "Pengfei Cao",
      "Yubo Chen",
      "Kang Liu",
      "Jun Zhao"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23451v1",
    "abstract": "Reward models (RMs) play a critical role in aligning AI behaviors with human preferences, yet they face two fundamental challenges: (1) Modality Imbalance, where most RMs are mainly focused on text and image modalities, offering limited support for video, audio, and other modalities; and (2) Preference Rigidity, where training on fixed binary preference pairs fails to capture the complexity and diversity of personalized preferences. To address the above challenges, we propose Omni-Reward, a step toward generalist omni-modal reward modeling with support for free-form preferences, consisting of: (1) Evaluation: We introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form preferences, covering nine tasks across five modalities including text, image, video, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal preference dataset comprising 248K general preference pairs and 69K instruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We propose Omni-RewardModel, which includes both discriminative and generative RMs, and achieves strong performance on Omni-RewardBench as well as other widely used reward modeling benchmarks.",
    "fetched_at": "2025-10-28T03:51:07.300802Z"
  },
  {
    "id": "2510.23453v1",
    "title": "What are the odds? Risk and uncertainty about AI existential risk",
    "date": "2025-10-27",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Marco Grossi"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23453v1",
    "abstract": "This work is a commentary of the article \\href{https://doi.org/10.18716/ojs/phai/2025.2801}{AI Survival Stories: a Taxonomic Analysis of AI Existential Risk} by Cappelen, Goldstein, and Hawthorne. It is not just a commentary though, but a useful reminder of the philosophical limitations of \\say{linear} models of risk. The article will focus on the model employed by the authors: first, I discuss some differences between standard Swiss Cheese models and this one. I then argue that in a situation of epistemic indifference the probability of P(D) is higher than what one might first suggest, given the structural relationships between layers. I then distinguish between risk and uncertainty, and argue that any estimation of P(D) is structurally affected by two kinds of uncertainty: option uncertainty and state-space uncertainty. Incorporating these dimensions of uncertainty into our qualitative discussion on AI existential risk can provide a better understanding of the likeliness of P(D).",
    "fetched_at": "2025-10-28T03:51:07.300728Z"
  },
  {
    "id": "2510.23455v1",
    "title": "SGFusion: Stochastic Geographic Gradient Fusion in Federated Learning",
    "date": "2025-10-27",
    "tags": [
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Khoa Nguyen",
      "Khang Tran",
      "NhatHai Phan",
      "Cristian Borcea",
      "Rouming Jin",
      "Issa Khalil"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23455v1",
    "abstract": "This paper proposes Stochastic Geographic Gradient Fusion (SGFusion), a novel training algorithm to leverage the geographic information of mobile users in Federated Learning (FL). SGFusion maps the data collected by mobile devices onto geographical zones and trains one FL model per zone, which adapts well to the data and behaviors of users in that zone. SGFusion models the local data-based correlation among geographical zones as a hierarchical random graph (HRG) optimized by Markov Chain Monte Carlo sampling. At each training step, every zone fuses its local gradient with gradients derived from a small set of other zones sampled from the HRG. This approach enables knowledge fusion and sharing among geographical zones in a probabilistic and stochastic gradient fusion process with self-attention weights, such that \"more similar\" zones have \"higher probabilities\" of sharing gradients with \"larger attention weights.\" SGFusion remarkably improves model utility without introducing undue computational cost. Extensive theoretical and empirical results using a heart-rate prediction dataset collected across 6 countries show that models trained with SGFusion converge with upper-bounded expected errors and significantly improve utility in all countries compared to existing approaches without notable cost in system scalability.",
    "fetched_at": "2025-10-28T03:51:07.300692Z"
  },
  {
    "id": "2510.23463v1",
    "title": "Differential Privacy as a Perk: Federated Learning over Multiple-Access   Fading Channels with a Multi-Antenna Base Station",
    "date": "2025-10-27",
    "tags": [
      "cs.LG",
      "LG",
      "cs.CR",
      "CR",
      "stat.ML",
      "ML"
    ],
    "authors": [
      "Hao Liang",
      "Haifeng Wen",
      "Kaishun Wu",
      "Hong Xing"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23463v1",
    "abstract": "Federated Learning (FL) is a distributed learning paradigm that preserves privacy by eliminating the need to exchange raw data during training. In its prototypical edge instantiation with underlying wireless transmissions enabled by analog over-the-air computing (AirComp), referred to as \\emph{over-the-air FL (AirFL)}, the inherent channel noise plays a unique role of \\emph{frenemy} in the sense that it degrades training due to noisy global aggregation while providing a natural source of randomness for privacy-preserving mechanisms, formally quantified by \\emph{differential privacy (DP)}. It remains, nevertheless, challenging to effectively harness such channel impairments, as prior arts, under assumptions of either simple channel models or restricted types of loss functions, mostly considering (local) DP enhancement with a single-round or non-convergent bound on privacy loss. In this paper, we study AirFL over multiple-access fading channels with a multi-antenna base station (BS) subject to user-level DP requirements. Despite a recent study, which claimed in similar settings that artificial noise (AN) must be injected to ensure DP in general, we demonstrate, on the contrary, that DP can be gained as a \\emph{perk} even \\emph{without} employing any AN. Specifically, we derive a novel bound on DP that converges under general bounded-domain assumptions on model parameters, along with a convergence bound with general smooth and non-convex loss functions. Next, we optimize over receive beamforming and power allocations to characterize the optimal convergence-privacy trade-offs, which also reveal explicit conditions in which DP is achievable without compromising training. Finally, our theoretical findings are validated by extensive numerical results.",
    "fetched_at": "2025-10-28T03:51:07.300558Z"
  },
  {
    "id": "2510.23464v1",
    "title": "Evaluating Large Language Models for Stance Detection on Financial   Targets from SEC Filing Reports and Earnings Call Transcripts",
    "date": "2025-10-27",
    "tags": [
      "cs.CL",
      "CL",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Nikesh Gyawali",
      "Doina Caragea",
      "Alex Vasenkov",
      "Cornelia Caragea"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23464v1",
    "abstract": "Financial narratives from U.S. Securities and Exchange Commission (SEC) filing reports and quarterly earnings call transcripts (ECTs) are very important for investors, auditors, and regulators. However, their length, financial jargon, and nuanced language make fine-grained analysis difficult. Prior sentiment analysis in the financial domain required a large, expensive labeled dataset, making the sentence-level stance towards specific financial targets challenging. In this work, we introduce a sentence-level corpus for stance detection focused on three core financial metrics: debt, earnings per share (EPS), and sales. The sentences were extracted from Form 10-K annual reports and ECTs, and labeled for stance (positive, negative, neutral) using the advanced ChatGPT-o3-pro model under rigorous human validation. Using this corpus, we conduct a systematic evaluation of modern large language models (LLMs) using zero-shot, few-shot, and Chain-of-Thought (CoT) prompting strategies. Our results show that few-shot with CoT prompting performs best compared to supervised baselines, and LLMs' performance varies across the SEC and ECT datasets. Our findings highlight the practical viability of leveraging LLMs for target-specific stance in the financial domain without requiring extensive labeled data.",
    "fetched_at": "2025-10-28T03:51:07.300493Z"
  },
  {
    "id": "2510.23469v1",
    "title": "Adaptive Dual Prompting: Hierarchical Debiasing for Fairness-aware Graph   Neural Networks",
    "date": "2025-10-27",
    "tags": [
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Yuhan Yang",
      "Xingbo Fu",
      "Jundong Li"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23469v1",
    "abstract": "In recent years, pre-training Graph Neural Networks (GNNs) through self-supervised learning on unlabeled graph data has emerged as a widely adopted paradigm in graph learning. Although the paradigm is effective for pre-training powerful GNN models, the objective gap often exists between pre-training and downstream tasks. To bridge this gap, graph prompting adapts pre-trained GNN models to specific downstream tasks with extra learnable prompts while keeping the pre-trained GNN models frozen. As recent graph prompting methods largely focus on enhancing model utility on downstream tasks, they often overlook fairness concerns when designing prompts for adaptation. In fact, pre-trained GNN models will produce discriminative node representations across demographic subgroups, as downstream graph data inherently contains biases in both node attributes and graph structures. To address this issue, we propose an Adaptive Dual Prompting (ADPrompt) framework that enhances fairness for adapting pre-trained GNN models to downstream tasks. To mitigate attribute bias, we design an Adaptive Feature Rectification module that learns customized attribute prompts to suppress sensitive information at the input layer, reducing bias at the source. Afterward, we propose an Adaptive Message Calibration module that generates structure prompts at each layer, which adjust the message from neighboring nodes to enable dynamic and soft calibration of the information flow. Finally, ADPrompt jointly optimizes the two prompting modules to adapt the pre-trained GNN while enhancing fairness. We conduct extensive experiments on four datasets with four pre-training strategies to evaluate the performance of ADPrompt. The results demonstrate that our proposed ADPrompt outperforms seven baseline methods on node classification tasks.",
    "fetched_at": "2025-10-28T03:51:07.300433Z"
  },
  {
    "id": "2510.23471v1",
    "title": "Robust Decision Making with Partially Calibrated Forecasts",
    "date": "2025-10-27",
    "tags": [
      "stat.ML",
      "ML",
      "cs.AI",
      "AI",
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Shayan Kiyani",
      "Hamed Hassani",
      "George Pappas",
      "Aaron Roth"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23471v1",
    "abstract": "Calibration has emerged as a foundational goal in ``trustworthy machine learning'', in part because of its strong decision theoretic semantics. Independent of the underlying distribution, and independent of the decision maker's utility function, calibration promises that amongst all policies mapping predictions to actions, the uniformly best policy is the one that ``trusts the predictions'' and acts as if they were correct. But this is true only of \\emph{fully calibrated} forecasts, which are tractable to guarantee only for very low dimensional prediction problems. For higher dimensional prediction problems (e.g. when outcomes are multiclass), weaker forms of calibration have been studied that lack these decision theoretic properties. In this paper we study how a conservative decision maker should map predictions endowed with these weaker (``partial'') calibration guarantees to actions, in a way that is robust in a minimax sense: i.e. to maximize their expected utility in the worst case over distributions consistent with the calibration guarantees. We characterize their minimax optimal decision rule via a duality argument, and show that surprisingly, ``trusting the predictions and acting accordingly'' is recovered in this minimax sense by \\emph{decision calibration} (and any strictly stronger notion of calibration), a substantially weaker and more tractable condition than full calibration. For calibration guarantees that fall short of decision calibration, the minimax optimal decision rule is still efficiently computable, and we provide an empirical evaluation of a natural one that applies to any regression model solved to optimize squared error.",
    "fetched_at": "2025-10-28T03:51:07.300365Z"
  },
  {
    "id": "2510.23472v1",
    "title": "BBOPlace-Bench: Benchmarking Black-Box Optimization for Chip Placement",
    "date": "2025-10-27",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI",
      "cs.AR",
      "AR",
      "cs.NE",
      "NE"
    ],
    "authors": [
      "Ke Xue",
      "Ruo-Tong Chen",
      "Rong-Xi Tan",
      "Xi Lin",
      "Yunqi Shi",
      "Siyuan Xu",
      "Mingxuan Yuan",
      "Chao Qian"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23472v1",
    "abstract": "Chip placement is a vital stage in modern chip design as it has a substantial impact on the subsequent processes and the overall quality of the final chip. The use of black-box optimization (BBO) for chip placement has a history of several decades. However, early efforts were limited by immature problem formulations and inefficient algorithm designs. Recent progress has shown the effectiveness and efficiency of BBO for chip placement, proving its potential to achieve state-of-the-art results. Despite these advancements, the field lacks a unified, BBO-specific benchmark for thoroughly assessing various problem formulations and BBO algorithms. To fill this gap, we propose BBOPlace-Bench, the first benchmark designed specifically for evaluating and developing BBO algorithms for chip placement tasks. It integrates three problem formulations of BBO for chip placement, and offers a modular, decoupled, and flexible framework that enables users to seamlessly implement, test, and compare their own algorithms. BBOPlace-Bench integrates a wide variety of existing BBO algorithms, including simulated annealing (SA), evolutionary algorithms (EAs), and Bayesian optimization (BO). Experimental results show that the problem formulations of mask-guided optimization and hyperparameter optimization exhibit superior performance than the sequence pair problem formulation, while EAs demonstrate better overall performance than SA and BO, especially in high-dimensional search spaces, and also achieve state-of-the-art performance compared to the mainstream chip placement methods. BBOPlace-Bench not only facilitates the development of efficient BBO-driven solutions for chip placement but also broadens the practical application scenarios (which are urgently needed) for the BBO community. The code of BBOPlace-Bench is available at https://github.com/lamda-bbo/BBOPlace-Bench.",
    "fetched_at": "2025-10-28T03:51:07.300296Z"
  },
  {
    "id": "2510.23474v1",
    "title": "Policy-Aware Generative AI for Safe, Auditable Data Access Governance",
    "date": "2025-10-27",
    "tags": [
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Shames Al Mandalawi",
      "Muzakkiruddin Ahmed Mohammed",
      "Hendrika Maclean",
      "Mert Can Cakmak",
      "John R. Talburt"
    ],
    "institution": "Google, Meta",
    "link": "http://arxiv.org/pdf/2510.23474v1",
    "abstract": "Enterprises need access decisions that satisfy least privilege, comply with regulations, and remain auditable. We present a policy aware controller that uses a large language model (LLM) to interpret natural language requests against written policies and metadata, not raw data. The system, implemented with Google Gemini~2.0 Flash, executes a six-stage reasoning framework (context interpretation, user validation, data classification, business purpose test, compliance mapping, and risk synthesis) with early hard policy gates and deny by default. It returns APPROVE, DENY, CONDITIONAL together with cited controls and a machine readable rationale. We evaluate on fourteen canonical cases across seven scenario families using a privacy preserving benchmark. Results show Exact Decision Match improving from 10/14 to 13/14 (92.9\\%) after applying policy gates, DENY recall rising to 1.00, False Approval Rate on must-deny families dropping to 0, and Functional Appropriateness and Compliance Adherence at 14/14. Expert ratings of rationale quality are high, and median latency is under one minute. These findings indicate that policy constrained LLM reasoning, combined with explicit gates and audit trails, can translate human readable policies into safe, compliant, and traceable machine decisions.",
    "fetched_at": "2025-10-28T03:51:07.300223Z"
  },
  {
    "id": "2510.23477v1",
    "title": "MMTutorBench: The First Multimodal Benchmark for AI Math Tutoring",
    "date": "2025-10-27",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Tengchao Yang",
      "Sichen Guo",
      "Mengzhao Jia",
      "Jiaming Su",
      "Yuanyang Liu",
      "Zhihan Zhang",
      "Meng Jiang"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23477v1",
    "abstract": "Effective math tutoring requires not only solving problems but also diagnosing students' difficulties and guiding them step by step. While multimodal large language models (MLLMs) show promise, existing benchmarks largely overlook these tutoring skills. We introduce MMTutorBench, the first benchmark for AI math tutoring, consisting of 685 problems built around pedagogically significant key-steps. Each problem is paired with problem-specific rubrics that enable fine-grained evaluation across six dimensions, and structured into three tasks-Insight Discovery, Operation Formulation, and Operation Execution. We evaluate 12 leading MLLMs and find clear performance gaps between proprietary and open-source systems, substantial room compared to human tutors, and consistent trends across input variants: OCR pipelines degrade tutoring quality, few-shot prompting yields limited gains, and our rubric-based LLM-as-a-Judge proves highly reliable. These results highlight both the difficulty and diagnostic value of MMTutorBench for advancing AI tutoring.",
    "fetched_at": "2025-10-28T03:51:07.300115Z"
  },
  {
    "id": "2510.23482v1",
    "title": "On the Faithfulness of Visual Thinking: Measurement and Enhancement",
    "date": "2025-10-27",
    "tags": [
      "cs.CV",
      "CV",
      "cs.AI",
      "AI"
    ],
    "authors": [
      "Zujing Liu",
      "Junwen Pan",
      "Qi She",
      "Yuan Gao",
      "Guisong Xia"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23482v1",
    "abstract": "Recent large vision-language models (LVLMs) can generate vision-text multimodal chain-of-thought (MCoT) traces after reinforcement fine-tuning (RFT). However, we observe that the visual information incorporated in MCoT is often inaccurate, though still yield correct answers, indicating a lack of faithfulness in the MCoT reasoning process. We attribute this unfaithfulness to the RL reward in RFT, which solely incentivizes the format of interleaved vision-text cues, ie, it encourages the model to incorporate visual information into its text reasoning steps without considering the correctness of the visual information. In this paper, we first probe the faithfulness of MCoT by measuring how much the prediction changes when its visual and textual thoughts are intervened. Surprisingly, the model's predictions remain nearly unchanged under visual intervention but change significantly under textual intervention, indicating that the visual evidence is largely ignored. To further analyze visual information, we introduce an automated LVLM-based evaluation metric that quantifies the faithfulness of visual cues from two perspectives: reliability and sufficiency. Our evaluation reveals that the visual information in current MCoT traces is simultaneously unreliable and insufficient. To address this issue, we propose a novel MCoT learning strategy termed Sufficient-Component Cause Model (SCCM) learning. This approach encourages the MCoT to generate sufficient yet minimal visual components that are independently capable of leading to correct answers. We note that the proposed SCCM is annotation-free and compatible with various RFT for MCoT in a plug-and-play manner. Empirical results demonstrate that SCCM consistently improves the visual faithfulness across a suite of fine-grained perception and reasoning benchmarks. Code is available at https://github.com/EugeneLiu01/Faithful_Thinking_with_Image.",
    "fetched_at": "2025-10-28T03:51:07.300053Z"
  },
  {
    "id": "2510.23484v1",
    "title": "T-REGS: Minimum Spanning Tree Regularization for Self-Supervised   Learning",
    "date": "2025-10-27",
    "tags": [
      "cs.LG",
      "LG",
      "cs.CG",
      "CG",
      "cs.CV",
      "CV"
    ],
    "authors": [
      "Julie Mordacq",
      "David Loiseaux",
      "Vicky Kalogeiton",
      "Steve Oudot"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23484v1",
    "abstract": "Self-supervised learning (SSL) has emerged as a powerful paradigm for learning representations without labeled data, often by enforcing invariance to input transformations such as rotations or blurring. Recent studies have highlighted two pivotal properties for effective representations: (i) avoiding dimensional collapse-where the learned features occupy only a low-dimensional subspace, and (ii) enhancing uniformity of the induced distribution. In this work, we introduce T-REGS, a simple regularization framework for SSL based on the length of the Minimum Spanning Tree (MST) over the learned representation. We provide theoretical analysis demonstrating that T-REGS simultaneously mitigates dimensional collapse and promotes distribution uniformity on arbitrary compact Riemannian manifolds. Several experiments on synthetic data and on classical SSL benchmarks validate the effectiveness of our approach at enhancing representation quality.",
    "fetched_at": "2025-10-28T03:51:07.299979Z"
  },
  {
    "id": "2510.23485v1",
    "title": "Tighter CMI-Based Generalization Bounds via Stochastic Projection and   Quantization",
    "date": "2025-10-27",
    "tags": [
      "stat.ML",
      "ML",
      "cs.IT",
      "IT",
      "cs.LG",
      "LG",
      "math.IT"
    ],
    "authors": [
      "Milad Sefidgaran",
      "Kimia Nadjahi",
      "Abdellatif Zaidi"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23485v1",
    "abstract": "In this paper, we leverage stochastic projection and lossy compression to establish new conditional mutual information (CMI) bounds on the generalization error of statistical learning algorithms. It is shown that these bounds are generally tighter than the existing ones. In particular, we prove that for certain problem instances for which existing MI and CMI bounds were recently shown in Attias et al. [2024] and Livni [2023] to become vacuous or fail to describe the right generalization behavior, our bounds yield suitable generalization guarantees of the order of $\\mathcal{O}(1/\\sqrt{n})$, where $n$ is the size of the training dataset. Furthermore, we use our bounds to investigate the problem of data \"memorization\" raised in those works, and which asserts that there are learning problem instances for which any learning algorithm that has good prediction there exist distributions under which the algorithm must \"memorize\" a big fraction of the training dataset. We show that for every learning algorithm, there exists an auxiliary algorithm that does not memorize and which yields comparable generalization error for any data distribution. In part, this shows that memorization is not necessary for good generalization.",
    "fetched_at": "2025-10-28T03:51:07.299917Z"
  },
  {
    "id": "2510.23486v1",
    "title": "Learning to Reason Efficiently with Discounted Reinforcement Learning",
    "date": "2025-10-27",
    "tags": [
      "cs.LG",
      "LG"
    ],
    "authors": [
      "Alex Ayoub",
      "Kavosh Asadi",
      "Dale Schuurmans",
      "Csaba Szepesvári",
      "Karim Bouyarmane"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23486v1",
    "abstract": "Large reasoning models (LRMs) often consume excessive tokens, inflating computational cost and latency. We challenge the assumption that longer responses improve accuracy. By penalizing reasoning tokens using a discounted reinforcement learning setup (interpretable as a small token cost) and analyzing Blackwell optimality in restricted policy classes, we encourage concise yet accurate reasoning. Experiments confirm our theoretical results that this approach shortens chains of thought while preserving accuracy.",
    "fetched_at": "2025-10-28T03:51:07.299862Z"
  },
  {
    "id": "2510.23489v1",
    "title": "Quantum Phase Classification of Rydberg Atom Systems Using   Resource-Efficient Variational Quantum Circuits and Classical Shadows",
    "date": "2025-10-27",
    "tags": [
      "quant-ph",
      "cs.LG",
      "LG",
      "81P68"
    ],
    "authors": [
      "Hemish Ahuja",
      "Samradh Bhardwaj",
      "Kirti Dhir",
      "Roman Bagdasarian",
      "Ziwoong Jang"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23489v1",
    "abstract": "Quantum phase transitions in Rydberg atom arrays present significant opportunities for studying many-body physics, yet distinguishing between different ordered phases without explicit order parameters remains challenging. We present a resource-efficient quantum machine learning approach combining classical shadow tomography with variational quantum circuits (VQCs) for binary phase classification of Z2 and Z3 ordered phases. Our pipeline processes 500 randomized measurements per 51-atom chain state, reconstructs shadow operators, performs PCA dimensionality reduction (514 features), and encodes features using angle embedding onto a 2-qubit parameterized circuit. The circuit employs RY-RZ angle encoding, strong entanglement via all-to-all CZ gates, and a minimal 2-parameter ansatz achieving depth 7. Training via simultaneous perturbation stochastic approximation (SPSA) with hinge loss converged in 120 iterations. The model achieved 100% test accuracy with perfect precision, recall, and F1 scores, demonstrating that minimal quantum resources suffice for high-accuracy phase classification. This work establishes pathways for quantum-enhanced condensed matter physics on near-term quantum devices.",
    "fetched_at": "2025-10-28T03:51:07.299763Z"
  },
  {
    "id": "2510.23498v1",
    "title": "Mixed Precision Training of Neural ODEs",
    "date": "2025-10-27",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI",
      "cs.NA",
      "NA",
      "math.NA",
      "68T07, 65L06, 65G50",
      "I.2; G.1",
      "1"
    ],
    "authors": [
      "Elena Celledoni",
      "Brynjulf Owren",
      "Lars Ruthotto",
      "Tianjiao Nicole Yang"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23498v1",
    "abstract": "Exploiting low-precision computations has become a standard strategy in deep learning to address the growing computational costs imposed by ever larger models and datasets. However, naively performing all computations in low precision can lead to roundoff errors and instabilities. Therefore, mixed precision training schemes usually store the weights in high precision and use low-precision computations only for whitelisted operations. Despite their success, these principles are currently not reliable for training continuous-time architectures such as neural ordinary differential equations (Neural ODEs). This paper presents a mixed precision training framework for neural ODEs, combining explicit ODE solvers with a custom backpropagation scheme, and demonstrates its effectiveness across a range of learning tasks. Our scheme uses low-precision computations for evaluating the velocity, parameterized by the neural network, and for storing intermediate states, while stability is provided by a custom dynamic adjoint scaling and by accumulating the solution and gradients in higher precision. These contributions address two key challenges in training neural ODE: the computational cost of repeated network evaluations and the growth of memory requirements with the number of time steps or layers. Along with the paper, we publish our extendable, open-source PyTorch package rampde, whose syntax resembles that of leading packages to provide a drop-in replacement in existing codes. We demonstrate the reliability and effectiveness of our scheme using challenging test cases and on neural ODE applications in image classification and generative models, achieving approximately 50% memory reduction and up to 2x speedup while maintaining accuracy comparable to single-precision training.",
    "fetched_at": "2025-10-28T03:51:07.299695Z"
  },
  {
    "id": "2510.23501v1",
    "title": "Towards Deep Physics-Informed Kolmogorov-Arnold Networks",
    "date": "2025-10-27",
    "tags": [
      "cs.LG",
      "LG",
      "physics.comp-ph",
      "comp-ph"
    ],
    "authors": [
      "Spyros Rigas",
      "Fotios Anagnostopoulos",
      "Michalis Papachristou",
      "Georgios Alexandridis"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23501v1",
    "abstract": "Since their introduction, Kolmogorov-Arnold Networks (KANs) have been successfully applied across several domains, with physics-informed machine learning (PIML) emerging as one of the areas where they have thrived. In the PIML setting, Chebyshev-based physics-informed KANs (cPIKANs) have become the standard due to their computational efficiency. However, like their multilayer perceptron-based counterparts, cPIKANs face significant challenges when scaled to depth, leading to training instabilities that limit their applicability to several PDE problems. To address this, we propose a basis-agnostic, Glorot-like initialization scheme that preserves activation variance and yields substantial improvements in stability and accuracy over the default initialization of cPIKANs. Inspired by the PirateNet architecture, we further introduce Residual-Gated Adaptive KANs (RGA KANs), designed to mitigate divergence in deep cPIKANs where initialization alone is not sufficient. Through empirical tests and information bottleneck analysis, we show that RGA KANs successfully traverse all training phases, unlike baseline cPIKANs, which stagnate in the diffusion phase in specific PDE settings. Evaluations on seven standard forward PDE benchmarks under a fixed training pipeline with adaptive components demonstrate that RGA KANs consistently outperform parameter-matched cPIKANs and PirateNets - often by several orders of magnitude - while remaining stable in settings where the others diverge.",
    "fetched_at": "2025-10-28T03:51:07.299622Z"
  },
  {
    "id": "2510.23503v1",
    "title": "Bayes-Split-Edge: Bayesian Optimization for Constrained Collaborative   Inference in Wireless Edge Systems",
    "date": "2025-10-27",
    "tags": [
      "cs.DC",
      "DC",
      "cs.LG",
      "LG",
      "eess.SP",
      "SP"
    ],
    "authors": [
      "Fatemeh Zahra Safaeipour",
      "Jacob Chakareski",
      "Morteza Hashemi"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23503v1",
    "abstract": "Mobile edge devices (e.g., AR/VR headsets) typically need to complete timely inference tasks while operating with limited on-board computing and energy resources. In this paper, we investigate the problem of collaborative inference in wireless edge networks, where energy-constrained edge devices aim to complete inference tasks within given deadlines. These tasks are carried out using neural networks, and the edge device seeks to optimize inference performance under energy and delay constraints. The inference process can be split between the edge device and an edge server, thereby achieving collaborative inference over wireless networks. We formulate an inference utility optimization problem subject to energy and delay constraints, and propose a novel solution called Bayes-Split-Edge, which leverages Bayesian optimization for collaborative split inference over wireless edge networks. Our solution jointly optimizes the transmission power and the neural network split point. The Bayes-Split-Edge framework incorporates a novel hybrid acquisition function that balances inference task utility, sample efficiency, and constraint violation penalties. We evaluate our approach using the VGG19 model on the ImageNet-Mini dataset, and Resnet101 on Tiny-ImageNet, and real-world mMobile wireless channel datasets. Numerical results demonstrate that Bayes-Split-Edge achieves up to 2.4x reduction in evaluation cost compared to standard Bayesian optimization and achieves near-linear convergence. It also outperforms several baselines, including CMA-ES, DIRECT, exhaustive search, and Proximal Policy Optimization (PPO), while matching exhaustive search performance under tight constraints. These results confirm that the proposed framework provides a sample-efficient solution requiring maximum 20 function evaluations and constraint-aware optimization for wireless split inference in edge computing systems.",
    "fetched_at": "2025-10-28T03:51:07.299559Z"
  },
  {
    "id": "2510.23506v1",
    "title": "Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale   Verifier",
    "date": "2025-10-27",
    "tags": [
      "cs.AI",
      "AI",
      "cs.HC",
      "HC"
    ],
    "authors": [
      "Hyeongseop Rha",
      "Jeong Hun Yeo",
      "Yeonju Kim",
      "Yong Man Ro"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.23506v1",
    "abstract": "The recent advancement of Multimodal Large Language Models (MLLMs) is transforming human-computer interaction (HCI) from surface-level exchanges into more nuanced and emotionally intelligent communication. To realize this shift, emotion understanding becomes essential allowing systems to capture subtle cues underlying user intent. Furthermore, providing faithful explanations for predicted emotions is crucial to ensure interpretability and build user trust. However, current MLLM-based methods often generate emotion explanations that diverge from the target labels and sometimes even contradict their own predicted emotions. This inconsistency poses a critical risk for misunderstanding and erodes reliability in interactive settings. To address this, we propose a novel approach: the Emotional Rationale Verifier (ERV) and an Explanation Reward. Our method guides the model to produce reasoning that is explicitly consistent with the target emotion during multimodal emotion recognition without modifying the model architecture or requiring additional paired video-description annotations. Our method significantly improves faithful explanation-prediction consistency and explanation emotion accuracy on the MAFW and DFEW datasets. Through extensive experiments and human evaluations, we show that our approach not only enhances alignment between explanation and prediction but also empowers MLLMs to deliver emotionally coherent, trustworthy interactions, marking a key step toward truly human-like HCI systems.",
    "fetched_at": "2025-10-28T03:51:07.299496Z"
  },
  {
    "id": "2510.23507v1",
    "title": "A Deep Latent Factor Graph Clustering with Fairness-Utility Trade-off   Perspective",
    "date": "2025-10-27",
    "tags": [
      "cs.LG",
      "LG",
      "cs.AI",
      "AI",
      "cs.IT",
      "IT",
      "math.IT"
    ],
    "authors": [
      "Siamak Ghodsi",
      "Amjad Seyedi",
      "Tai Le Quy",
      "Fariba Karimi",
      "Eirini Ntoutsi"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23507v1",
    "abstract": "Fair graph clustering seeks partitions that respect network structure while maintaining proportional representation across sensitive groups, with applications spanning community detection, team formation, resource allocation, and social network analysis. Many existing approaches enforce rigid constraints or rely on multi-stage pipelines (e.g., spectral embedding followed by $k$-means), limiting trade-off control, interpretability, and scalability. We introduce \\emph{DFNMF}, an end-to-end deep nonnegative tri-factorization tailored to graphs that directly optimizes cluster assignments with a soft statistical-parity regularizer. A single parameter $\\lambda$ tunes the fairness--utility balance, while nonnegativity yields parts-based factors and transparent soft memberships. The optimization uses sparse-friendly alternating updates and scales near-linearly with the number of edges. Across synthetic and real networks, DFNMF achieves substantially higher group balance at comparable modularity, often dominating state-of-the-art baselines on the Pareto front. The code is available at https://github.com/SiamakGhodsi/DFNMF.git.",
    "fetched_at": "2025-10-28T03:51:07.299434Z"
  },
  {
    "id": "2510.23508v1",
    "title": "M4FC: a Multimodal, Multilingual, Multicultural, Multitask Real-World   Fact-Checking Dataset",
    "date": "2025-10-27",
    "tags": [
      "cs.CL",
      "CL"
    ],
    "authors": [
      "Jiahui Geng",
      "Jonathan Tonglet",
      "Iryna Gurevych"
    ],
    "institution": "MIT",
    "link": "http://arxiv.org/pdf/2510.23508v1",
    "abstract": "Existing real-world datasets for multimodal automated fact-checking have multiple limitations: they contain few instances, focus on only one or two languages and tasks, suffer from evidence leakage, or depend on external sets of news articles for sourcing true claims. To address these shortcomings, we introduce M4FC, a new real-world dataset comprising 4,982 images paired with 6,980 claims. The images, verified by professional fact-checkers from 22 organizations, represent diverse cultural and geographic contexts. Each claim is available in one or two out of ten languages. M4FC spans six multimodal fact-checking tasks: visual claim extraction, claimant intent prediction, fake detection, image contextualization, location verification, and verdict prediction. We provide baseline results for all tasks and analyze how combining intermediate tasks influence downstream verdict prediction performance. We make our dataset and code available.",
    "fetched_at": "2025-10-28T03:51:07.299358Z"
  },
  {
    "id": "2510.22888v1",
    "title": "MGFRec: Towards Reinforced Reasoning Recommendation with Multiple   Groundings and Feedback",
    "date": "2025-10-27",
    "tags": [
      "cs.IR",
      "IR"
    ],
    "authors": [
      "Shihao Cai",
      "Chongming Gao",
      "Haoyan Liu",
      "Wentao Shi",
      "Jianshan Sun",
      "Ruiming Tang",
      "Fuli Feng"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.22888v1",
    "abstract": "The powerful reasoning and generative capabilities of large language models (LLMs) have inspired researchers to apply them to reasoning-based recommendation tasks, which require in-depth reasoning about user interests and the generation of recommended items. However, previous reasoning-based recommendation methods have typically performed inference within the language space alone, without incorporating the actual item space. This has led to over-interpreting user interests and deviating from real items. Towards this research gap, we propose performing multiple rounds of grounding during inference to help the LLM better understand the actual item space, which could ensure that its reasoning remains aligned with real items. Furthermore, we introduce a user agent that provides feedback during each grounding step, enabling the LLM to better recognize and adapt to user interests. Comprehensive experiments conducted on three Amazon review datasets demonstrate the effectiveness of incorporating multiple groundings and feedback. These findings underscore the critical importance of reasoning within the actual item space, rather than being confined to the language space, for recommendation tasks.",
    "fetched_at": "2025-10-28T03:50:59.657280Z"
  },
  {
    "id": "2510.22898v1",
    "title": "On Generalization in Agentic Tool Calling: CoreThink Agentic Reasoner   and MAVEN Dataset",
    "date": "2025-10-27",
    "tags": [
      "cs.AI",
      "AI",
      "cs.SE",
      "SE"
    ],
    "authors": [
      "Vishvesh Bhat",
      "Omkar Ghugarkar",
      "Julian McAuley"
    ],
    "institution": "",
    "link": "http://arxiv.org/pdf/2510.22898v1",
    "abstract": "Generalization across Agentic tool-calling environments remains a key unsolved challenge in developing reliable agentic reasoning systems. While large language models (LLMs) demonstrate strong performance on isolated benchmarks, their ability to transfer reasoning strategies and co-ordinate tools across diverse domains is poorly understood. In this work, we conduct a large-scale evaluation of state-of-the-art LLMs on multiple tool-calling benchmarksBFCL v3, TauBench, Tau2Bench, and AceBenchand introduce MAVEN (Math & Physics Adversarial Verification & Evaluation Network), a new out of distribution (OOD) benchmark designed to stress-test multi-step reasoning through explicit verification and adversarial task composition. Our results show that most current models achieve below 50% accuracy on MAVEN, revealing a significant generalization gap across tool-use settings.   To address this, we present the CoreThink Agentic Reasoner, a framework that augments LLMs with a lightweight symbolic reasoning layer for structured decomposition and adaptive tool orchestration. Without additional training, it generalizes across all benchmarks, achieving state-of-the-art performance with 530% improvements over existing baselines at roughly one-tenth the computational cost.",
    "fetched_at": "2025-10-28T03:50:59.657202Z"
  }
]